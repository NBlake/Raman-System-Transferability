{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 216300,
     "status": "ok",
     "timestamp": 1680521861573,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "P8aaAvzZH1BY",
    "outputId": "9ccfd482-2632-43e3-ed50-5a46347307e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((560819, 891), (560819,), (560819,), (560819,), (560819,), (891,), (560819,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load all centre raw data\n",
    "\n",
    "import numpy as np\n",
    "SMART_x = np.loadtxt('/content/drive/MyDrive/Thesis_code/SMARTData/Uncorrected_MedianFiltered_SizeMatched_SMART_x.csv', delimiter=',')\n",
    "SMART_y = np.loadtxt('/content/drive/MyDrive/Thesis_code/SMARTData/Uncorrected_MedianFiltered_SizeMatched_SMART_y.csv', delimiter=',')\n",
    "SMART_sample = np.loadtxt('/content/drive/MyDrive/Thesis_code/SMARTData/Uncorrected_MedianFiltered_SizeMatched_SMART_sampleID.csv', delimiter=',')\n",
    "SMART_centre = np.loadtxt('/content/drive/MyDrive/Thesis_code/SMARTData/Uncorrected_MedianFiltered_SizeMatched_SMART_centre.csv', delimiter=',')\n",
    "SMART_patient = np.loadtxt('/content/drive/MyDrive/Thesis_code/SMARTData/Uncorrected_MedianFiltered_SizeMatched_SMART_patientID.csv', delimiter=',')\n",
    "SMART_wn = np.loadtxt('/content/drive/MyDrive/Thesis_code/SMARTData/Uncorrected_MedianFiltered_SizeMatched_SMART_wn.csv', delimiter=',')\n",
    "SMART_uniquemapID = np.loadtxt('/content/drive/MyDrive/Thesis_code/SMARTData/Uncorrected_MedianFiltered_SizeMatched_SMART_mapID.csv', delimiter=',')\n",
    "\n",
    "Raw_labels = {\n",
    "  \"NSQ\": 0,\n",
    "  \"IM\": 1,\n",
    "  \"LGD\": 2,\n",
    "  \"HGD\": 3,\n",
    "  \"AC\": 4\n",
    "}\n",
    "\n",
    "Centre = {\n",
    "  \"GRH\": 0,\n",
    "  \"UCL\": 1,\n",
    "  \"UoE\": 2\n",
    "}\n",
    "\n",
    "SMART_x = SMART_x.T\n",
    "\n",
    "SMART_x.shape, SMART_y.shape, SMART_sample.shape, SMART_centre.shape, SMART_patient.shape, SMART_wn.shape, SMART_uniquemapID.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alL0gBCAysy5"
   },
   "outputs": [],
   "source": [
    "def remove_saturations(spectra, labels, centre = None, patient = None, sample = None, sats = 400):\n",
    "    \"\"\"\n",
    "    Takes in a dataset of Raman spectra and removes samples that have been saturated.\n",
    "\n",
    "    sats defines the number of contiguous zeros to tolerate in any single spectra\"\"\"\n",
    "    # Identify saturated spectra\n",
    "    saturated_indices = []\n",
    "    for i in range(spectra.shape[0]):\n",
    "        if np.count_nonzero(spectra[i,:]==0) > sats:\n",
    "            saturated_indices.append(i)\n",
    "\n",
    "    # Remove saturated spectra\n",
    "    non_none_vars = [] # Keeps track of variables to output\n",
    "    unsaturated_spectra = np.delete(spectra, saturated_indices, axis=0)\n",
    "    non_none_vars.append('spectra')\n",
    "    unsaturated_labels = np.delete(labels, saturated_indices, axis=0)\n",
    "    non_none_vars.append('labels')\n",
    "    if centre is not None:\n",
    "        unsaturated_centre = np.delete(centre, saturated_indices, axis=0)\n",
    "        non_none_vars.append('centre')\n",
    "    if patient is not None:\n",
    "        unsaturated_patient = np.delete(patient, saturated_indices, axis=0)\n",
    "        non_none_vars.append('patient')\n",
    "    if sample is not None:\n",
    "        unsaturated_sample = np.delete(sample, saturated_indices, axis=0)\n",
    "        non_none_vars.append('sample')\n",
    "    print('Active output variable names:', non_none_vars)\n",
    "    # return spectra, labels, centre, patient, sample\n",
    "    # Return non-None outputs\n",
    "    return (x for x in (unsaturated_spectra, unsaturated_labels, unsaturated_centre, unsaturated_patient, unsaturated_sample) if x is not None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHfdwvvszOF6"
   },
   "outputs": [],
   "source": [
    "def MapandSpectraCount(raw_y, uniqueID, labels):\n",
    "    \"\"\"\n",
    "    Function which counts the number of individual maps and spectra present in each label class\n",
    "\n",
    "    Inputs:\n",
    "        raw_y - a vector of integer labels\n",
    "        uniqueID - a vector denoting unique ID\n",
    "        labels - a dictionary of the labels to be used\n",
    "\n",
    "    Outputs:\n",
    "        MapCount - a vector giving the count maps per of labels\n",
    "        SpectraCount - a vector giving the count spectra per of labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_labels = len(labels)\n",
    "\n",
    "    MapCount = np.zeros(number_of_labels)\n",
    "    SpectraCount = np.zeros(number_of_labels)\n",
    "\n",
    "    for i in range(number_of_labels):\n",
    "        MapCount[i] = len(np.unique(uniqueID[raw_y == i]))\n",
    "        SpectraCount[i] = len(uniqueID[raw_y == i])\n",
    "\n",
    "\n",
    "\n",
    "    return MapCount, SpectraCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1680521861587,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "erg6UgO64Zmy",
    "outputId": "8f516969-0e41-4e74-ae34-fcce72139fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([16., 17.,  6.,  5.,  7.]),\n",
       " array([196505., 188088.,  51924.,  40429.,  83873.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MapandSpectraCount(SMART_y, SMART_patient, Raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7966,
     "status": "ok",
     "timestamp": 1680521869523,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "GOPq350UytEr",
    "outputId": "d4a111ad-9554-4556-a454-efde5064b174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active output variable names: ['spectra', 'labels', 'centre', 'patient', 'sample']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([16., 17.,  6.,  5.,  7.]),\n",
       " array([196505., 188071.,  51924.,  40429.,  83873.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMART_x, SMART_y, SMART_centre, SMART_patient, SMART_uniquemapID = remove_saturations(SMART_x, SMART_y, SMART_centre, SMART_patient, SMART_uniquemapID, sats = 400)\n",
    "MapandSpectraCount(SMART_y, SMART_patient, Raw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1721,
     "status": "ok",
     "timestamp": 1680521871227,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "77wOFlAu05ha",
    "outputId": "dcd54864-1bea-425f-caf0-30eb57a494cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(np.isnan(SMART_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8Fojm-mINb8"
   },
   "outputs": [],
   "source": [
    "# SNV Normalise\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def norm_spectra(spec):\n",
    "    \"\"\"spec - a matrix of spectra of dimension: number of spectra x wavenumber\"\"\"\n",
    "    \"\"\"normed_spec = a matrix of 'individual' SNV normalised spectra\"\"\"\n",
    "\n",
    "    normed_spec = np.zeros(spec.shape)\n",
    "\n",
    "    for i in range(0, spec.shape[0]):\n",
    "        normed_spec[i] = (spec[i] - np.mean(spec[i]))/np.std(spec[i])\n",
    "\n",
    "    return normed_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD_3MvwjINuC"
   },
   "outputs": [],
   "source": [
    "def truncate_spectra_to_region(wn, spectra, trunc_start = 400, trunc_end = 1800):\n",
    "    \"\"\"\n",
    "    Function to truncate spectra within a specified region, default to the fingerprint region\n",
    "    Inputs:\n",
    "        wn - an array (vector) of the wavenumbers\n",
    "        spectra - an array (matrix) of spectra (spectra in the rows)\n",
    "        trunc_start - start of wavenumber region to be included\n",
    "        trunc_end - end of wavenumber region to be included\n",
    "    Outputs:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if trunc_start > trunc_end:\n",
    "        print(\"ERROR: Start of region must be lower than end of region\")\n",
    "        pass\n",
    "\n",
    "    closest_top_wn = min(wn, key=lambda x:abs(x-trunc_end))\n",
    "    closest_bottom_wn = min(wn, key=lambda x:abs(x-trunc_start))\n",
    "\n",
    "    max_wn = np.where(wn == closest_top_wn)\n",
    "    min_wn = np.where(wn == closest_bottom_wn)\n",
    "\n",
    "    trunc_spectra = spectra[:,np.squeeze(max_wn):np.squeeze(min_wn)]\n",
    "    trunc_wn = wn[np.squeeze(max_wn):np.squeeze(min_wn)]\n",
    "\n",
    "    return trunc_spectra, trunc_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26607,
     "status": "ok",
     "timestamp": 1680521897829,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "Yn9g295zIN_9",
    "outputId": "ad5c996f-6a00-483f-8498-ae17cce66108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560802, 677)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMART_x, SMART_wn = truncate_spectra_to_region(SMART_wn, SMART_x, trunc_start = 450, trunc_end = 1800)\n",
    "SMART_x = norm_spectra(SMART_x)\n",
    "SMART_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNtEnhpfIa4Z"
   },
   "outputs": [],
   "source": [
    "# Custom class for stratification by group.\n",
    "# Taken from: https://www.kaggle.com/realsid/stratifiedgroupkfold-using-sklearn\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits\n",
    "from sklearn.utils.validation import check_random_state, column_or_1d\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from collections import defaultdict\n",
    "\n",
    "class StratifiedGroupKFold(_BaseKFold):\n",
    "    \"\"\"Stratified K-Folds iterator variant with non-overlapping groups.\n",
    "    This cross-validation object is a variation of StratifiedKFold attempts to\n",
    "    return stratified folds with non-overlapping groups. The folds are made by\n",
    "    preserving the percentage of samples for each class.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    The difference between GroupKFold and StratifiedGroupKFold is that\n",
    "    the former attempts to create balanced folds such that the number of\n",
    "    distinct groups is approximately the same in each fold, whereas\n",
    "    StratifiedGroupKFold attempts to create folds which preserve the\n",
    "    percentage of samples for each class as much as possible given the\n",
    "    constraint of non-overlapping groups between splits.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : bool, default=False\n",
    "        Whether to shuffle each class's samples before splitting into batches.\n",
    "        Note that the samples within each split will not be shuffled.\n",
    "        This implementation can only shuffle groups that have approximately the\n",
    "        same y distribution, no global shuffle will be performed.\n",
    "    random_state : int or RandomState instance, default=None\n",
    "        When `shuffle` is True, `random_state` affects the ordering of the\n",
    "        indices, which controls the randomness of each fold for each class.\n",
    "        Otherwise, leave `random_state` as `None`.\n",
    "        Pass an int for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import StratifiedGroupKFold\n",
    "    >>> X = np.ones((17, 2))\n",
    "    >>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    >>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n",
    "    >>> cv = StratifiedGroupKFold(n_splits=3)\n",
    "    >>> for train_idxs, test_idxs in cv.split(X, y, groups):\n",
    "    ...     print(\"TRAIN:\", groups[train_idxs])\n",
    "    ...     print(\"      \", y[train_idxs])\n",
    "    ...     print(\" TEST:\", groups[test_idxs])\n",
    "    ...     print(\"      \", y[test_idxs])\n",
    "    TRAIN: [1 1 2 2 4 5 5 5 5 8 8]\n",
    "           [0 0 1 1 1 0 0 0 0 0 0]\n",
    "     TEST: [3 3 3 6 6 7]\n",
    "           [1 1 1 0 0 0]\n",
    "    TRAIN: [3 3 3 4 5 5 5 5 6 6 7]\n",
    "           [1 1 1 1 0 0 0 0 0 0 0]\n",
    "     TEST: [1 1 2 2 8 8]\n",
    "           [0 0 1 1 0 0]\n",
    "    TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]\n",
    "           [0 0 1 1 1 1 1 0 0 0 0 0]\n",
    "     TEST: [4 5 5 5 5]\n",
    "           [1 0 0 0 0]\n",
    "    Notes\n",
    "    -----\n",
    "    The implementation is designed to:\n",
    "    * Mimic the behavior of StratifiedKFold as much as possible for trivial\n",
    "      groups (e.g. when each group contains only one sample).\n",
    "    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
    "      ``y = [1, 0]`` should not change the indices generated.\n",
    "    * Stratify based on samples as much as possible while keeping\n",
    "      non-overlapping groups constraint. That means that in some cases when\n",
    "      there is a small number of groups containing a large number of samples\n",
    "      the stratification will not be possible and the behavior will be close\n",
    "      to GroupKFold.\n",
    "    See also\n",
    "    --------\n",
    "    StratifiedKFold: Takes class information into account to build folds which\n",
    "        retain class distributions (for binary or multiclass classification\n",
    "        tasks).\n",
    "    GroupKFold: K-fold iterator variant with non-overlapping groups.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n",
    "        super().__init__(n_splits=n_splits, shuffle=shuffle,\n",
    "                         random_state=random_state)\n",
    "\n",
    "    def _iter_test_indices(self, X, y, groups):\n",
    "        # Implementation is based on this kaggle kernel:\n",
    "        # https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation\n",
    "        # and is a subject to Apache 2.0 License. You may obtain a copy of the\n",
    "        # License at http://www.apache.org/licenses/LICENSE-2.0\n",
    "        # Changelist:\n",
    "        # - Refactored function to a class following scikit-learn KFold\n",
    "        #   interface.\n",
    "        # - Added heuristic for assigning group to the least populated fold in\n",
    "        #   cases when all other criteria are equal\n",
    "        # - Swtch from using python ``Counter`` to ``np.unique`` to get class\n",
    "        #   distribution\n",
    "        # - Added scikit-learn checks for input: checking that target is binary\n",
    "        #   or multiclass, checking passed random state, checking that number\n",
    "        #   of splits is less than number of members in each class, checking\n",
    "        #   that least populated class has more members than there are splits.\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y = np.asarray(y)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "        allowed_target_types = ('binary', 'multiclass')\n",
    "        if type_of_target_y not in allowed_target_types:\n",
    "            raise ValueError(\n",
    "                'Supported target types are: {}. Got {!r} instead.'.format(\n",
    "                    allowed_target_types, type_of_target_y))\n",
    "\n",
    "        y = column_or_1d(y)\n",
    "        _, y_inv, y_cnt = np.unique(y, return_inverse=True, return_counts=True)\n",
    "        if np.all(self.n_splits > y_cnt):\n",
    "            raise ValueError(\"n_splits=%d cannot be greater than the\"\n",
    "                             \" number of members in each class.\"\n",
    "                             % (self.n_splits))\n",
    "        n_smallest_class = np.min(y_cnt)\n",
    "        if self.n_splits > n_smallest_class:\n",
    "            warnings.warn((\"The least populated class in y has only %d\"\n",
    "                           \" members, which is less than n_splits=%d.\"\n",
    "                           % (n_smallest_class, self.n_splits)), UserWarning)\n",
    "        n_classes = len(y_cnt)\n",
    "\n",
    "\n",
    "        _, groups_inv, groups_cnt = np.unique(\n",
    "            groups, return_inverse=True, return_counts=True)\n",
    "        y_counts_per_group = np.zeros((len(groups_cnt), n_classes))\n",
    "        for class_idx, group_idx in zip(y_inv, groups_inv):\n",
    "            y_counts_per_group[group_idx, class_idx] += 1\n",
    "\n",
    "        y_counts_per_fold = np.zeros((self.n_splits, n_classes))\n",
    "        groups_per_fold = defaultdict(set)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(y_counts_per_group)\n",
    "\n",
    "        # Stable sort to keep shuffled order for groups with the same\n",
    "        # class distribution variance\n",
    "        sorted_groups_idx = np.argsort(-np.std(y_counts_per_group, axis=1),\n",
    "                                       kind='mergesort')\n",
    "\n",
    "        for group_idx in sorted_groups_idx:\n",
    "            group_y_counts = y_counts_per_group[group_idx]\n",
    "            best_fold = self._find_best_fold(\n",
    "                y_counts_per_fold=y_counts_per_fold, y_cnt=y_cnt,\n",
    "                group_y_counts=group_y_counts)\n",
    "            y_counts_per_fold[best_fold] += group_y_counts\n",
    "            groups_per_fold[best_fold].add(group_idx)\n",
    "\n",
    "        for i in range(self.n_splits):\n",
    "            test_indices = [idx for idx, group_idx in enumerate(groups_inv)\n",
    "                            if group_idx in groups_per_fold[i]]\n",
    "            yield test_indices\n",
    "\n",
    "    def _find_best_fold(\n",
    "            self, y_counts_per_fold, y_cnt, group_y_counts):\n",
    "        best_fold = None\n",
    "        min_eval = np.inf\n",
    "        min_samples_in_fold = np.inf\n",
    "        for i in range(self.n_splits):\n",
    "            y_counts_per_fold[i] += group_y_counts\n",
    "            # Summarise the distribution over classes in each proposed fold\n",
    "            std_per_class = np.std(\n",
    "                y_counts_per_fold / y_cnt.reshape(1, -1),\n",
    "                axis=0)\n",
    "            y_counts_per_fold[i] -= group_y_counts\n",
    "            fold_eval = np.mean(std_per_class)\n",
    "            samples_in_fold = np.sum(y_counts_per_fold[i])\n",
    "            is_current_fold_better = (\n",
    "                fold_eval < min_eval or\n",
    "                np.isclose(fold_eval, min_eval)\n",
    "                and samples_in_fold < min_samples_in_fold\n",
    "            )\n",
    "            if is_current_fold_better:\n",
    "                min_eval = fold_eval\n",
    "                min_samples_in_fold = samples_in_fold\n",
    "                best_fold = i\n",
    "        return best_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a663OSMhIbF7"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SpectralDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Builds a dataset of spectral data. Use idxs to specify which samples to use\n",
    "    for dataset - this allows for random splitting into training, validation,\n",
    "    and test sets. Instead of passing in filenames for X and y, we can also\n",
    "    pass in numpy arrays directly.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_fn, y_fn, idxs=None, transform=None):\n",
    "        if type(X_fn) == str:\n",
    "            self.X = np.load(X_fn)\n",
    "        else:\n",
    "            self.X = X_fn\n",
    "        if type(y_fn) == str:\n",
    "            self.y = np.load(y_fn)\n",
    "        else:\n",
    "            self.y = y_fn\n",
    "        if idxs is None: idxs = np.arange(len(self.y))\n",
    "        self.idxs = idxs\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.idxs[idx]\n",
    "        x, y = self.X[i], self.y[i]\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return (x, y)\n",
    "\n",
    "\n",
    "### TRANSFORMS ###\n",
    "\n",
    "\n",
    "class GetInterval(object):\n",
    "    \"\"\"\n",
    "    Gets an interval of each spectrum.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_idx, max_idx):\n",
    "        self.min_idx = min_idx\n",
    "        self.max_idx = max_idx\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x[:,self.min_idx:self.max_idx]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ToFloatTensor(object):\n",
    "    \"\"\"\n",
    "    Converts numpy arrays to float Variables in Pytorch.\n",
    "    \"\"\"\n",
    "    def __call__(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        return x\n",
    "\n",
    "\n",
    "### TRANSFORMS ###\n",
    "\n",
    "\n",
    "def spectral_dataloader(X_fn, y_fn, idxs=None, batch_size=10, shuffle=True,\n",
    "    num_workers=4, min_idx=None, max_idx=None, sampler=None):\n",
    "    \"\"\"\n",
    "    Returns a DataLoader with spectral data.\n",
    "    \"\"\"\n",
    "    transform_list = []\n",
    "    if min_idx is not None and max_idx is not None:\n",
    "        transform_list.append(GetInterval(min_idx, max_idx))\n",
    "    transform_list.append(ToFloatTensor())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    dataset = SpectralDataset(X_fn, y_fn, idxs=idxs, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "        num_workers=num_workers, sampler=sampler)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def spectral_dataloaders(X_fn, y_fn, n_train=None, p_train=0.8, p_val=0.1,\n",
    "    n_test=None, batch_size=10, shuffle=True, num_workers=4, min_idx=None,\n",
    "    max_idx=None):\n",
    "    \"\"\"\n",
    "    Returns train, val, and test DataLoaders by splitting the dataset randomly.\n",
    "    Can also take X_fn and y_fn as numpy arrays.\n",
    "    \"\"\"\n",
    "    if type(y_fn) == str:\n",
    "        idxs = np.arange(len(np.load(y_fn)))\n",
    "    else:\n",
    "        idxs = np.arange(len(y_fn))\n",
    "    np.random.shuffle(idxs)\n",
    "    if n_train is None: n_train = int(p_train * len(idxs))\n",
    "    n_val = int(p_val * n_train)\n",
    "    val_idxs, train_idxs = idxs[:n_val], idxs[n_val:n_train]\n",
    "    if n_test is None: test_idxs = idxs[n_train:]\n",
    "    else: test_idxs = idxs[n_train:n_train+n_test]\n",
    "    trainloader = spectral_dataloader(X_fn, y_fn, train_idxs,\n",
    "        batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,\n",
    "        min_idx=min_idx, max_idx=max_idx)\n",
    "    valloader = spectral_dataloader(X_fn, y_fn, val_idxs,\n",
    "        batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,\n",
    "        min_idx=min_idx, max_idx=max_idx)\n",
    "    testloader = spectral_dataloader(X_fn, y_fn, test_idxs,\n",
    "        batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,\n",
    "        min_idx=min_idx, max_idx=max_idx)\n",
    "    return (trainloader, valloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzlV89B5IbTp"
   },
   "outputs": [],
   "source": [
    "# 3 layer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 20, kernel_size=7, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=7, stride=3))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(20, 40, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=5, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(40, 40, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1))\n",
    "        self.fc1 = nn.Linear(4120, 60)\n",
    "        self.fc2 = nn.Linear(60, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.dropout(out, 0.2)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5ZiCAmuIbfm"
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params+=param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1680521902273,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "Ur_QK1gNIbu5",
    "outputId": "5010911d-d90d-4b0e-f39b-2ece6d5c4de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|     Modules     | Parameters |\n",
      "+-----------------+------------+\n",
      "| layer1.0.weight |    140     |\n",
      "|  layer1.0.bias  |     20     |\n",
      "| layer1.1.weight |     20     |\n",
      "|  layer1.1.bias  |     20     |\n",
      "| layer2.0.weight |    4000    |\n",
      "|  layer2.0.bias  |     40     |\n",
      "| layer2.1.weight |     40     |\n",
      "|  layer2.1.bias  |     40     |\n",
      "| layer3.0.weight |    4800    |\n",
      "|  layer3.0.bias  |     40     |\n",
      "| layer3.1.weight |     40     |\n",
      "|  layer3.1.bias  |     40     |\n",
      "|    fc1.weight   |   247200   |\n",
      "|     fc1.bias    |     60     |\n",
      "|    fc2.weight   |    300     |\n",
      "|     fc2.bias    |     5      |\n",
      "+-----------------+------------+\n",
      "Total Trainable Params: 256805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256805"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed hyperparameters\n",
    "\n",
    "n_epochs = 60\n",
    "num_classes = 5\n",
    "\n",
    "# Hyperparameters\n",
    "#by mode\n",
    "#learning_rate = 0.001\n",
    "#batch_size = 256\n",
    "#bymean\n",
    "learning_rate = 5e-5\n",
    "batch_size = 184\n",
    "\n",
    "# Loss and optimizer\n",
    "convnet = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(convnet.parameters(), lr=learning_rate)\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# See model architecture\n",
    "count_parameters(convnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFY6g4VIIb7u"
   },
   "outputs": [],
   "source": [
    "del convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QbCJjzfI0a2"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_data_weights(train_data):\n",
    "    \"\"\"\n",
    "    Function to extract the class distribution of a dataset and\n",
    "    return those as weights reasy for input into a pytorch optimiser\n",
    "    Input:\n",
    "        train_data - the labels of the data from which to extract class distribution\n",
    "    Output:\n",
    "        max_label / torch.tensor([all_values]) - the realtive weights based on class distribution.\n",
    "        The largest class will have a weight of one. The smallest class will have the largest weight.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    train_count = Counter(train_data)\n",
    "\n",
    "    key_max = max(train_count.keys(), key=(lambda k: train_count[k]))\n",
    "    max_label = train_count[key_max]\n",
    "\n",
    "    all_values = np.array([train_count[k] for k in sorted(train_count.keys())]).flatten()\n",
    "    all_values\n",
    "\n",
    "    return max_label / torch.tensor([all_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mV5CdienI02c"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_list, test_list, epochs):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to plot train and test learning curves, where train score calculated per batch, test score per epoch\n",
    "    Inputs:\n",
    "        train_list - List of training scores per batch.\n",
    "        test_list - List of test scores per epoch\n",
    "        epochs - Integer number of epochs\n",
    "    Outputs:\n",
    "        plt.show() - learning curve\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twiny()\n",
    "\n",
    "    x1 = np.linspace(0, len(train_list), len(train_list))\n",
    "    x2 = np.linspace(0, len(train_list), len(test_list))\n",
    "\n",
    "    ax1.plot(x1,train_list)\n",
    "    ax1.plot(x2,test_list)\n",
    "    ax1.set_xlabel(\"Batch\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    batch_list = np.linspace(0, x1.shape[0], epochs)\n",
    "    batch_labels = [0,1,2,3,4]\n",
    "\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ax2.set_xticks(batch_list)\n",
    "    ax2.set_xticklabels(batch_labels)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFbYjUqSI1Yu"
   },
   "outputs": [],
   "source": [
    "# Cuda enabled version.\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_cnn(model, X_train, y_train, X_test, y_test, learning_rate = 0.0001, beta1 = 0.9, beta2 = 0.999, num_epochs = 30, batch_size = 16, early_stop = 5,weightedLoss = False, learning_curves = True, cuda = False):\n",
    "\n",
    "    train_loader = spectral_dataloader(X_train,y_train,batch_size = batch_size, shuffle=True) # requires dataloader functions\n",
    "    test_loader = spectral_dataloader(X_test,y_test,batch_size = batch_size, shuffle=False)\n",
    "\n",
    "    total_step = len(train_loader)\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    test_acc_list = []\n",
    "    test_logloss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "            # Loss and optimizer\n",
    "\n",
    "  #torch.manual_seed(3407) # If we want to fix random seed\n",
    "    model = model() #this should be called before running the function so is not needed here\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if weightedLoss:\n",
    "        weight = np.squeeze(get_data_weights(y_train)).to(device) # Need to send all variables used in optimisation to the GPU, including this weights\n",
    "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "      model.cuda()\n",
    "\n",
    "    # early stopping set-up\n",
    "    best_val = 0\n",
    "    no_improvement = 0\n",
    "    max_no_improvement = early_stop\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        test_outputs_list = []\n",
    "        test_predicted_list = []\n",
    "        test_labels_list = []\n",
    "        for i, (spectra, labels) in enumerate(train_loader):\n",
    "            spectra, labels = spectra.cuda(), labels.cuda()\n",
    "            # Run the forward pass\n",
    "\n",
    "            outputs = model(spectra)\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and perform Adam optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            acc_list.append(correct / total)\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.3f}, Training Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), (correct / total) * 100))\n",
    "\n",
    "                # Test the model\n",
    "        print('Testing epoch {}'.format(epoch + 1))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            for test_spectra, test_labels in test_loader:\n",
    "                test_spectra, test_labels = test_spectra.cuda(), test_labels.cuda()\n",
    "                test_outputs = model(test_spectra)\n",
    "                test_outputs_list.append(test_outputs)\n",
    "                test_labels_list.append(test_labels)\n",
    "                _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "                test_predicted_list.append(test_predicted)\n",
    "                test_total += test_labels.size(0)\n",
    "                test_correct += (test_predicted == test_labels).sum().item()\n",
    "                acc_te = test_correct / test_total\n",
    "            test_acc_list.append(acc_te)\n",
    "\n",
    "            print('Epoch {}: Test Accuracy {} %'.format(epoch+1,(acc_te) * 100))\n",
    "\n",
    "\n",
    "                # Check performance for early stopping\n",
    "        if acc_te > best_val or epoch == 0:\n",
    "            best_val = acc_te\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "        if no_improvement >= max_no_improvement:\n",
    "            print('Finished early after {} epochs!'.format(epoch+1))\n",
    "            break\n",
    "\n",
    "    # Final train score on a single batch\n",
    "    train_log_loss, train_accuracy, _ = test_a_batch(model ,X_train, y_train, batch_size )\n",
    "\n",
    "    # Final test score on a single batch\n",
    "    test_log_loss, test_accuracy, con_mat = test_a_batch(model ,X_test, y_test, batch_size )\n",
    "\n",
    "    print('Final Training Accuracy {} %'.format((train_accuracy) * 100))\n",
    "    print('Final Test Accuracy {} %'.format((test_accuracy) * 100))\n",
    "\n",
    "    if learning_curves:\n",
    "        plot_learning_curve(acc_list, test_acc_list, epoch)\n",
    "\n",
    "    return model, con_mat, test_log_loss, test_accuracy, train_log_loss, train_accuracy, test_outputs_list, test_predicted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMK4AhexrQA8"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_list, test_list, epochs):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to plot train and test learning curves, where train score calculated per batch, test score per epoch\n",
    "    Inputs:\n",
    "        train_list - List of training scores per batch.\n",
    "        test_list - List of test scores per epoch\n",
    "        epochs - Integer number of epochs\n",
    "    Outputs:\n",
    "        plt.show() - learning curve\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax2 = ax1.twiny()\n",
    "\n",
    "    x1 = np.linspace(0, len(train_list), len(train_list))\n",
    "    x2 = np.linspace(0, len(train_list), len(test_list))\n",
    "\n",
    "    ax1.plot(x1,train_list)\n",
    "    ax1.plot(x2,test_list)\n",
    "    ax1.set_xlabel(\"Batch\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "\n",
    "    batch_list = np.linspace(0, x1.shape[0], epochs)\n",
    "    batch_labels = [0,1,2,3,4]\n",
    "\n",
    "    ax2.set_xlim(ax1.get_xlim())\n",
    "    ax2.set_xticks(batch_list)\n",
    "    ax2.set_xticklabels(batch_labels)\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "155QqsvsI1sw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def test_a_batch(model, X, y, cuda = True, batch_size = 16):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to take in a Pytorch neural network and some data and output the accuracy,\n",
    "    confusion matrix and log loss\n",
    "\n",
    "    Inputs:\n",
    "        model - pytorch model\n",
    "        X - data inputs compatible with the above model\n",
    "        y - data labels\n",
    "        batch_size - set low to ensure compatability even with poor memory CPUs\n",
    "\n",
    "    Outputs:\n",
    "        test_log_loss - log loss\n",
    "        test_accuracy - accuracy\n",
    "        con_mat - confusion matrix\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    test_loader = spectral_dataloader(X, y, batch_size = batch_size, shuffle=False)\n",
    "\n",
    "    y_test_clas = get_predictions(model, test_loader, cuda, get_probs=False)\n",
    "    y_test_pred = get_predictions(model, test_loader, cuda, get_probs=True)\n",
    "\n",
    "\n",
    "    test_log_loss = metrics.log_loss(y, y_test_pred, labels = np.unique(y_train))\n",
    "    test_accuracy = metrics.accuracy_score(y, y_test_clas)\n",
    "    con_mat = confusion_matrix(y, y_test_clas)\n",
    "\n",
    "    return test_log_loss, test_accuracy, con_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx-zNhYIIcJQ"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, cuda, get_probs=False):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        if cuda: inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        inputs, targets = Variable(inputs), Variable(targets.long())\n",
    "        outputs = model(inputs)\n",
    "        if get_probs:\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            if cuda: probs = probs.data.cpu().numpy()\n",
    "            else: probs = probs.data.numpy()\n",
    "            preds.append(probs)\n",
    "        else:\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            if cuda: predicted = predicted.cpu()\n",
    "            preds += list(predicted.numpy().ravel())\n",
    "    if get_probs:\n",
    "        return np.vstack(preds)\n",
    "    else:\n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbvW0kThJlWz"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_normalised_sample_score(counter_object, y_train):\n",
    "    \"\"\"\n",
    "    Takes a collections count of a single sample and an array of labels and return a numerically oredered np.array of the values of the object\n",
    "    Input:\n",
    "    collections object\n",
    "    y_train\n",
    "    Outouts: np.array of values\n",
    "    \"\"\"\n",
    "\n",
    "    # First, create an empty dictionary with keys equal to the possible values\n",
    "\n",
    "    counter_dict = {}\n",
    "    for d_class in np.unique(y_train):\n",
    "        counter_dict[d_class] = 0\n",
    "\n",
    "    # Then, iterate through the counter_object and map the values to the keys in the dictionary\n",
    "    for key, value in counter_object.items():\n",
    "        counter_dict[key] = value\n",
    "\n",
    "    norm_values = []\n",
    "    for key, value in counter_dict.items():\n",
    "        norm_values.append(value)\n",
    "    norm_values = np.array(norm_values)\n",
    "    norm_values = norm_values / np.sum(norm_values)\n",
    "\n",
    "    return norm_values\n",
    "\n",
    "def simple_consensus_sample_CNN(model, sample_spectra, sample_labels, y_train, batch_size = 64):\n",
    "    \"\"\"\n",
    "    Function to classify a single sample with multiple spectra based on consensus. Simple majority wins.\n",
    "    Inputs:\n",
    "        svm_model - trained svm model\n",
    "        sample_spectra - spectra to be classified belonging to a single sample\n",
    "        sample_labels - label of the sample being classified. Should only be one label.\n",
    "        y_train - All possible labels.\n",
    "    Outputs:\n",
    "        true_sample_label - the true sample label\n",
    "        predicted_sample_label - the samples predicted label\n",
    "        predictions - returns proportion for each class\n",
    "    \"\"\"\n",
    "\n",
    "    true_sample_label = np.unique(sample_labels)\n",
    "    if len(true_sample_label) > 1:\n",
    "        print(\"More than one label was present in the sample.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    test_data = spectral_dataloader(sample_spectra,sample_labels,batch_size = batch_size, shuffle=False)\n",
    "    test_pred = get_predictions(model, test_data, cuda)\n",
    "    c = Counter(test_pred)\n",
    "    predictions = get_normalised_sample_score(c, y_train)\n",
    "    predicted_sample_label = max(c, key=c.get)\n",
    "\n",
    "    return true_sample_label, predicted_sample_label, predictions\n",
    "\n",
    "\n",
    "def sample_accuracy_batch(model, centre, uniquemapID, centre_train, batch_spectra, batch_labels, proportioned = False ):\n",
    "    \"\"\"\n",
    "        Function to classify an entire batch of samples which have multiple spectra based on consensus. Simple majority wins.\n",
    "    Inputs:\n",
    "        pca_model - trained pca model\n",
    "        lda_model - trained lda model\n",
    "        uniquemapID - a list of every single unique sample in the data\n",
    "        centre_train - the centre (or other indices) from which to extract a batch\n",
    "        batch_spectra - ALL spectra in the dataset\n",
    "        batch_labels -  ALL labels in the dataset\n",
    "        proportioned - If TRUE classifies using proportional consensus, otherwise by simple consensus\n",
    "    Outputs:\n",
    "        y_sample_label - the true sample labels\n",
    "        y_sample_clas - the samples predicted labels\n",
    "        y_sample_predictions - gives proportions of pixel votes per sample as a list\n",
    "    \"\"\"\n",
    "\n",
    "    y_sample_label = []\n",
    "    y_sample_clas = []\n",
    "    y_sample_predictions = []\n",
    "\n",
    "    training_samples = uniquemapID[centre == centre_train]\n",
    "\n",
    "\n",
    "    for sample in np.unique(training_samples):\n",
    "        if proportioned == True:\n",
    "            sam_lab, sam_clas = proportional_consensus_sample_CNN(model, batch_spectra[uniquemapID == sample], batch_labels[uniquemapID == sample])\n",
    "            y_sample_label.append(sam_lab), y_sample_clas.append(sam_clas)\n",
    "\n",
    "        if proportioned == False:\n",
    "            sam_lab, sam_clas, predictions = simple_consensus_sample_CNN(model, batch_spectra[uniquemapID == sample], batch_labels[uniquemapID == sample], y_train)\n",
    "            y_sample_label.append(sam_lab), y_sample_clas.append(sam_clas), y_sample_predictions.append(predictions)\n",
    "\n",
    "    y_sample_label = np.array(y_sample_label)\n",
    "    y_sample_clas = np.array(y_sample_clas)\n",
    "\n",
    "    return y_sample_label, y_sample_clas, y_sample_predictions\n",
    "\n",
    "\n",
    "def sample_accuracy_whole(model, uniquemapID, batch_spectra, batch_labels, y_train, proportioned = False ):\n",
    "    \"\"\"\n",
    "        Function to classify an entire batch of samples which have multiple spectra based on consensus.\n",
    "    Inputs:\n",
    "        svm_model - trained svm model\n",
    "        uniquemapID - a list of every single unique sample in the data\n",
    "        batch_spectra - ALL spectra in the dataset\n",
    "        batch_labels -  ALL labels in the dataset\n",
    "        y_train - array of all possible classes. Can be any size but must contain all possible class labels\n",
    "        proportioned - If TRUE classifies using proportional consensus, otherwise by simple consensus\n",
    "    Outputs:\n",
    "        y_sample_label - the true sample labels\n",
    "        y_sample_clas - the samples predicted labels\n",
    "        prediction - gives proportions of pixel votes per sample as a list\n",
    "    \"\"\"\n",
    "\n",
    "    y_sample_label = []\n",
    "    y_sample_clas = []\n",
    "    y_sample_predictions = []\n",
    "\n",
    "\n",
    "    for sample in np.unique(uniquemapID):\n",
    "        if proportioned == True:\n",
    "            sam_lab, sam_clas = proportional_consensus_sample_CNN(model, batch_spectra[uniquemapID == sample], batch_labels[uniquemapID == sample])\n",
    "            y_sample_label.append(sam_lab), y_sample_clas.append(sam_clas)\n",
    "\n",
    "        if proportioned == False:\n",
    "            sam_lab, sam_clas, predictions = simple_consensus_sample_CNN(model, batch_spectra[uniquemapID == sample], batch_labels[uniquemapID == sample], y_train)\n",
    "            y_sample_label.append(sam_lab), y_sample_clas.append(sam_clas), y_sample_predictions.append(predictions)\n",
    "\n",
    "    y_sample_label = np.array(y_sample_label)\n",
    "    y_sample_clas = np.array(y_sample_clas)\n",
    "\n",
    "    return y_sample_label, y_sample_clas, y_sample_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1650439,
     "status": "ok",
     "timestamp": 1680523552647,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "D6CgQLSAJllN",
    "outputId": "5f0ce8fc-893a-4c78-af42-a3f0c8279a47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Centre: GRH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-6ad01c70a6be>:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return max_label / torch.tensor([all_values])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Step [100/1048], Loss: 1.224, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [200/1048], Loss: 0.971, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [300/1048], Loss: 0.852, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [400/1048], Loss: 0.907, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [500/1048], Loss: 1.091, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [600/1048], Loss: 0.911, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [700/1048], Loss: 0.809, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [800/1048], Loss: 0.759, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [900/1048], Loss: 0.753, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [1000/1048], Loss: 0.692, Training Accuracy: 77.72%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 64.73721957500909 %\n",
      "Epoch [2/60], Step [100/1048], Loss: 0.843, Training Accuracy: 70.11%\n",
      "Epoch [2/60], Step [200/1048], Loss: 0.794, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [300/1048], Loss: 0.795, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [400/1048], Loss: 0.654, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [500/1048], Loss: 0.763, Training Accuracy: 72.28%\n",
      "Epoch [2/60], Step [600/1048], Loss: 0.689, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [700/1048], Loss: 0.715, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [800/1048], Loss: 0.596, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [900/1048], Loss: 0.561, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1000/1048], Loss: 0.842, Training Accuracy: 71.20%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 65.6576166388664 %\n",
      "Epoch [3/60], Step [100/1048], Loss: 0.744, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [200/1048], Loss: 0.845, Training Accuracy: 70.65%\n",
      "Epoch [3/60], Step [300/1048], Loss: 0.691, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [400/1048], Loss: 0.702, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [500/1048], Loss: 0.727, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [600/1048], Loss: 0.698, Training Accuracy: 73.37%\n",
      "Epoch [3/60], Step [700/1048], Loss: 0.601, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [800/1048], Loss: 0.765, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [900/1048], Loss: 0.582, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [1000/1048], Loss: 0.635, Training Accuracy: 73.37%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 65.92221721153376 %\n",
      "Epoch [4/60], Step [100/1048], Loss: 0.608, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [200/1048], Loss: 0.537, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [300/1048], Loss: 0.593, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [400/1048], Loss: 0.529, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [500/1048], Loss: 0.519, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [600/1048], Loss: 0.551, Training Accuracy: 75.54%\n",
      "Epoch [4/60], Step [700/1048], Loss: 0.620, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [800/1048], Loss: 0.516, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [900/1048], Loss: 0.578, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [1000/1048], Loss: 0.658, Training Accuracy: 75.00%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 65.98551488446137 %\n",
      "Epoch [5/60], Step [100/1048], Loss: 0.507, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [200/1048], Loss: 0.507, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [300/1048], Loss: 0.430, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [400/1048], Loss: 0.538, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [500/1048], Loss: 0.618, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [600/1048], Loss: 0.586, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [700/1048], Loss: 0.541, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [800/1048], Loss: 0.498, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [900/1048], Loss: 0.622, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [1000/1048], Loss: 0.550, Training Accuracy: 84.78%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 66.29629830862098 %\n",
      "Epoch [6/60], Step [100/1048], Loss: 0.588, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [200/1048], Loss: 0.652, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [300/1048], Loss: 0.435, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [400/1048], Loss: 0.476, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [500/1048], Loss: 0.618, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/1048], Loss: 0.487, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [700/1048], Loss: 0.512, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [800/1048], Loss: 0.517, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [900/1048], Loss: 0.507, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1000/1048], Loss: 0.492, Training Accuracy: 83.70%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 66.1900777501888 %\n",
      "Epoch [7/60], Step [100/1048], Loss: 0.592, Training Accuracy: 79.35%\n",
      "Epoch [7/60], Step [200/1048], Loss: 0.501, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [300/1048], Loss: 0.489, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [400/1048], Loss: 0.506, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [500/1048], Loss: 0.474, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [600/1048], Loss: 0.485, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [700/1048], Loss: 0.453, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [800/1048], Loss: 0.458, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [900/1048], Loss: 0.485, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [1000/1048], Loss: 0.462, Training Accuracy: 85.33%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 66.31884640670249 %\n",
      "Epoch [8/60], Step [100/1048], Loss: 0.474, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [200/1048], Loss: 0.466, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [300/1048], Loss: 0.501, Training Accuracy: 81.52%\n",
      "Epoch [8/60], Step [400/1048], Loss: 0.514, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [500/1048], Loss: 0.497, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [600/1048], Loss: 0.458, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [700/1048], Loss: 0.430, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [800/1048], Loss: 0.494, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [900/1048], Loss: 0.435, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1000/1048], Loss: 0.490, Training Accuracy: 80.43%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 66.45141835686847 %\n",
      "Epoch [9/60], Step [100/1048], Loss: 0.442, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [200/1048], Loss: 0.430, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [300/1048], Loss: 0.387, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [400/1048], Loss: 0.494, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [500/1048], Loss: 0.398, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [600/1048], Loss: 0.406, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [700/1048], Loss: 0.487, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [800/1048], Loss: 0.333, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [900/1048], Loss: 0.398, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [1000/1048], Loss: 0.578, Training Accuracy: 82.07%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 66.61278667325904 %\n",
      "Epoch [10/60], Step [100/1048], Loss: 0.363, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [200/1048], Loss: 0.417, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [300/1048], Loss: 0.379, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [400/1048], Loss: 0.341, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [500/1048], Loss: 0.414, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [600/1048], Loss: 0.507, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [700/1048], Loss: 0.357, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [800/1048], Loss: 0.447, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [900/1048], Loss: 0.414, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [1000/1048], Loss: 0.519, Training Accuracy: 83.70%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 66.67717100151589 %\n",
      "Epoch [11/60], Step [100/1048], Loss: 0.385, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [200/1048], Loss: 0.323, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [300/1048], Loss: 0.460, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [400/1048], Loss: 0.430, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [500/1048], Loss: 0.431, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [600/1048], Loss: 0.355, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [700/1048], Loss: 0.521, Training Accuracy: 81.52%\n",
      "Epoch [11/60], Step [800/1048], Loss: 0.389, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [900/1048], Loss: 0.408, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1000/1048], Loss: 0.325, Training Accuracy: 91.30%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 66.62392489038365 %\n",
      "Epoch [12/60], Step [100/1048], Loss: 0.402, Training Accuracy: 82.07%\n",
      "Epoch [12/60], Step [200/1048], Loss: 0.373, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [300/1048], Loss: 0.355, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [400/1048], Loss: 0.319, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [500/1048], Loss: 0.293, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [600/1048], Loss: 0.364, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [700/1048], Loss: 0.431, Training Accuracy: 84.78%\n",
      "Epoch [12/60], Step [800/1048], Loss: 0.358, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [900/1048], Loss: 0.370, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [1000/1048], Loss: 0.413, Training Accuracy: 86.96%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 66.52802755757915 %\n",
      "Epoch [13/60], Step [100/1048], Loss: 0.359, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [200/1048], Loss: 0.436, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [300/1048], Loss: 0.334, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [400/1048], Loss: 0.428, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [500/1048], Loss: 0.363, Training Accuracy: 85.87%\n",
      "Epoch [13/60], Step [600/1048], Loss: 0.359, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [700/1048], Loss: 0.466, Training Accuracy: 85.87%\n",
      "Epoch [13/60], Step [800/1048], Loss: 0.492, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [900/1048], Loss: 0.391, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [1000/1048], Loss: 0.435, Training Accuracy: 88.59%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 66.3639426028655 %\n",
      "Epoch [14/60], Step [100/1048], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [200/1048], Loss: 0.408, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [300/1048], Loss: 0.327, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [400/1048], Loss: 0.425, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [500/1048], Loss: 0.351, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [600/1048], Loss: 0.301, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [700/1048], Loss: 0.391, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [800/1048], Loss: 0.282, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [900/1048], Loss: 0.345, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [1000/1048], Loss: 0.358, Training Accuracy: 87.50%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 66.64375635014208 %\n",
      "Epoch [15/60], Step [100/1048], Loss: 0.346, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [200/1048], Loss: 0.249, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [300/1048], Loss: 0.292, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [400/1048], Loss: 0.408, Training Accuracy: 84.24%\n",
      "Epoch [15/60], Step [500/1048], Loss: 0.449, Training Accuracy: 85.33%\n",
      "Epoch [15/60], Step [600/1048], Loss: 0.323, Training Accuracy: 90.76%\n",
      "Epoch [15/60], Step [700/1048], Loss: 0.371, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [800/1048], Loss: 0.288, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [900/1048], Loss: 0.401, Training Accuracy: 85.87%\n",
      "Epoch [15/60], Step [1000/1048], Loss: 0.388, Training Accuracy: 86.96%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 66.48972295722382 %\n",
      "Finished early after 15 epochs!\n",
      "Final Training Accuracy 87.75194603009861 %\n",
      "Final Test Accuracy 66.50493613183302 %\n",
      "train accuracy: 0.877519460300986, train logloss: 0.32812423224250503\n",
      "\n",
      "By sample accuracy: 0.9180327868852459, log loss 0.22321707493297915\n",
      "Test Centre: UCL\n",
      "By spectra accuracy: 0.6631596298569902, logloss: 2.2692062350940856\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.639344262295082 logloss 3.1483515829127704\n",
      "\n",
      "Test Centre: UoE\n",
      "By spectra accuracy: 0.6672945829213717, logloss: 2.815819519878113\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.639344262295082 logloss 2.772957290829976\n",
      "\n",
      "\n",
      "Training Centre: UCL\n",
      "Epoch [1/60], Step [100/957], Loss: 1.337, Training Accuracy: 50.00%\n",
      "Epoch [1/60], Step [200/957], Loss: 1.182, Training Accuracy: 56.52%\n",
      "Epoch [1/60], Step [300/957], Loss: 1.161, Training Accuracy: 42.93%\n",
      "Epoch [1/60], Step [400/957], Loss: 1.117, Training Accuracy: 55.43%\n",
      "Epoch [1/60], Step [500/957], Loss: 1.048, Training Accuracy: 63.59%\n",
      "Epoch [1/60], Step [600/957], Loss: 0.984, Training Accuracy: 63.04%\n",
      "Epoch [1/60], Step [700/957], Loss: 0.881, Training Accuracy: 59.24%\n",
      "Epoch [1/60], Step [800/957], Loss: 0.921, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [900/957], Loss: 0.905, Training Accuracy: 67.39%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 61.83074804479435 %\n",
      "Epoch [2/60], Step [100/957], Loss: 0.990, Training Accuracy: 63.04%\n",
      "Epoch [2/60], Step [200/957], Loss: 0.826, Training Accuracy: 70.11%\n",
      "Epoch [2/60], Step [300/957], Loss: 0.930, Training Accuracy: 65.76%\n",
      "Epoch [2/60], Step [400/957], Loss: 0.918, Training Accuracy: 65.76%\n",
      "Epoch [2/60], Step [500/957], Loss: 0.859, Training Accuracy: 64.67%\n",
      "Epoch [2/60], Step [600/957], Loss: 0.803, Training Accuracy: 70.65%\n",
      "Epoch [2/60], Step [700/957], Loss: 0.773, Training Accuracy: 68.48%\n",
      "Epoch [2/60], Step [800/957], Loss: 0.940, Training Accuracy: 63.04%\n",
      "Epoch [2/60], Step [900/957], Loss: 0.756, Training Accuracy: 69.57%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 60.106789305479765 %\n",
      "Epoch [3/60], Step [100/957], Loss: 0.842, Training Accuracy: 70.11%\n",
      "Epoch [3/60], Step [200/957], Loss: 0.807, Training Accuracy: 68.48%\n",
      "Epoch [3/60], Step [300/957], Loss: 0.819, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [400/957], Loss: 0.809, Training Accuracy: 71.74%\n",
      "Epoch [3/60], Step [500/957], Loss: 0.783, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [600/957], Loss: 0.809, Training Accuracy: 70.11%\n",
      "Epoch [3/60], Step [700/957], Loss: 0.703, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [800/957], Loss: 0.732, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [900/957], Loss: 0.853, Training Accuracy: 69.02%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 58.79439810845221 %\n",
      "Epoch [4/60], Step [100/957], Loss: 0.742, Training Accuracy: 72.28%\n",
      "Epoch [4/60], Step [200/957], Loss: 0.700, Training Accuracy: 72.83%\n",
      "Epoch [4/60], Step [300/957], Loss: 0.756, Training Accuracy: 72.83%\n",
      "Epoch [4/60], Step [400/957], Loss: 0.541, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [500/957], Loss: 0.713, Training Accuracy: 74.46%\n",
      "Epoch [4/60], Step [600/957], Loss: 0.620, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [700/957], Loss: 0.802, Training Accuracy: 72.28%\n",
      "Epoch [4/60], Step [800/957], Loss: 0.812, Training Accuracy: 68.48%\n",
      "Epoch [4/60], Step [900/957], Loss: 0.720, Training Accuracy: 70.11%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 54.543092472783016 %\n",
      "Epoch [5/60], Step [100/957], Loss: 0.614, Training Accuracy: 76.63%\n",
      "Epoch [5/60], Step [200/957], Loss: 0.628, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [300/957], Loss: 0.747, Training Accuracy: 75.00%\n",
      "Epoch [5/60], Step [400/957], Loss: 0.754, Training Accuracy: 71.20%\n",
      "Epoch [5/60], Step [500/957], Loss: 0.633, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [600/957], Loss: 0.573, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [700/957], Loss: 0.741, Training Accuracy: 72.28%\n",
      "Epoch [5/60], Step [800/957], Loss: 0.692, Training Accuracy: 75.54%\n",
      "Epoch [5/60], Step [900/957], Loss: 0.673, Training Accuracy: 69.57%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 53.99875282562944 %\n",
      "Epoch [6/60], Step [100/957], Loss: 0.655, Training Accuracy: 75.00%\n",
      "Epoch [6/60], Step [200/957], Loss: 0.826, Training Accuracy: 66.30%\n",
      "Epoch [6/60], Step [300/957], Loss: 0.567, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [400/957], Loss: 0.622, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [500/957], Loss: 0.649, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [600/957], Loss: 0.716, Training Accuracy: 75.00%\n",
      "Epoch [6/60], Step [700/957], Loss: 0.637, Training Accuracy: 75.00%\n",
      "Epoch [6/60], Step [800/957], Loss: 0.703, Training Accuracy: 74.46%\n",
      "Epoch [6/60], Step [900/957], Loss: 0.502, Training Accuracy: 80.43%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 56.866994049938945 %\n",
      "Finished early after 6 epochs!\n",
      "Final Training Accuracy 77.84314394197757 %\n",
      "Final Test Accuracy 56.875568373736584 %\n",
      "train accuracy: 0.7784314394197758, train logloss: 0.5681211937330634\n",
      "\n",
      "By sample accuracy: 0.9672131147540983, log loss 0.36787207153440143\n",
      "Test Centre: GRH\n",
      "By spectra accuracy: 0.5759730150492994, logloss: 1.1309110803659557\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.6229508196721312 logloss 1.168745889902834\n",
      "\n",
      "Test Centre: UoE\n",
      "By spectra accuracy: 0.5605245355674663, logloss: 1.2105300142355924\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.5081967213114754 logloss 1.3018524538303131\n",
      "\n",
      "\n",
      "Training Centre: UoE\n",
      "Epoch [1/60], Step [100/1045], Loss: 1.124, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [200/1045], Loss: 0.985, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [300/1045], Loss: 1.008, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [400/1045], Loss: 0.928, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [500/1045], Loss: 0.817, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [600/1045], Loss: 0.858, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [700/1045], Loss: 0.869, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [800/1045], Loss: 0.880, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [900/1045], Loss: 0.711, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [1000/1045], Loss: 0.680, Training Accuracy: 78.80%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 59.44871850517589 %\n",
      "Epoch [2/60], Step [100/1045], Loss: 0.642, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [200/1045], Loss: 0.695, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [300/1045], Loss: 0.641, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [400/1045], Loss: 0.665, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [500/1045], Loss: 0.672, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [600/1045], Loss: 0.675, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [700/1045], Loss: 0.607, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [800/1045], Loss: 0.569, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [900/1045], Loss: 0.618, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1000/1045], Loss: 0.578, Training Accuracy: 81.52%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 61.5000868074394 %\n",
      "Epoch [3/60], Step [100/1045], Loss: 0.637, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [200/1045], Loss: 0.629, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [300/1045], Loss: 0.590, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [400/1045], Loss: 0.523, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [500/1045], Loss: 0.524, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [600/1045], Loss: 0.648, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [700/1045], Loss: 0.586, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [800/1045], Loss: 0.445, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [900/1045], Loss: 0.423, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1000/1045], Loss: 0.517, Training Accuracy: 80.43%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 62.12971201631979 %\n",
      "Epoch [4/60], Step [100/1045], Loss: 0.510, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [200/1045], Loss: 0.537, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [300/1045], Loss: 0.462, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [400/1045], Loss: 0.576, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [500/1045], Loss: 0.564, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [600/1045], Loss: 0.547, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [700/1045], Loss: 0.517, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [800/1045], Loss: 0.409, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [900/1045], Loss: 0.519, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1000/1045], Loss: 0.489, Training Accuracy: 82.61%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 62.464463204496624 %\n",
      "Epoch [5/60], Step [100/1045], Loss: 0.457, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [200/1045], Loss: 0.389, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [300/1045], Loss: 0.629, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [400/1045], Loss: 0.469, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [500/1045], Loss: 0.512, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [600/1045], Loss: 0.433, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [700/1045], Loss: 0.513, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [800/1045], Loss: 0.533, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [900/1045], Loss: 0.525, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1000/1045], Loss: 0.363, Training Accuracy: 86.41%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 63.19690097441351 %\n",
      "Epoch [6/60], Step [100/1045], Loss: 0.373, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [200/1045], Loss: 0.583, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [300/1045], Loss: 0.419, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [400/1045], Loss: 0.498, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [500/1045], Loss: 0.450, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/1045], Loss: 0.500, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [700/1045], Loss: 0.462, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [800/1045], Loss: 0.491, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [900/1045], Loss: 0.476, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1000/1045], Loss: 0.398, Training Accuracy: 86.41%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 64.24482953189087 %\n",
      "Epoch [7/60], Step [100/1045], Loss: 0.460, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [200/1045], Loss: 0.542, Training Accuracy: 79.35%\n",
      "Epoch [7/60], Step [300/1045], Loss: 0.474, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [400/1045], Loss: 0.479, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [500/1045], Loss: 0.392, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [600/1045], Loss: 0.531, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [700/1045], Loss: 0.406, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [800/1045], Loss: 0.548, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [900/1045], Loss: 0.406, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [1000/1045], Loss: 0.404, Training Accuracy: 84.24%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 64.60535167863885 %\n",
      "Epoch [8/60], Step [100/1045], Loss: 0.369, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [200/1045], Loss: 0.357, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [300/1045], Loss: 0.475, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [400/1045], Loss: 0.468, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [500/1045], Loss: 0.413, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [600/1045], Loss: 0.402, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [700/1045], Loss: 0.389, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [800/1045], Loss: 0.457, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [900/1045], Loss: 0.367, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1000/1045], Loss: 0.338, Training Accuracy: 88.04%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 64.70192495496863 %\n",
      "Epoch [9/60], Step [100/1045], Loss: 0.339, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [200/1045], Loss: 0.425, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [300/1045], Loss: 0.381, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [400/1045], Loss: 0.442, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [500/1045], Loss: 0.501, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [600/1045], Loss: 0.382, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [700/1045], Loss: 0.295, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [800/1045], Loss: 0.413, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [900/1045], Loss: 0.425, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [1000/1045], Loss: 0.362, Training Accuracy: 84.24%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 64.54838429653421 %\n",
      "Epoch [10/60], Step [100/1045], Loss: 0.381, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [200/1045], Loss: 0.325, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [300/1045], Loss: 0.321, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [400/1045], Loss: 0.377, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [500/1045], Loss: 0.298, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [600/1045], Loss: 0.356, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [700/1045], Loss: 0.462, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [800/1045], Loss: 0.285, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [900/1045], Loss: 0.285, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1000/1045], Loss: 0.336, Training Accuracy: 87.50%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 65.11236137937021 %\n",
      "Epoch [11/60], Step [100/1045], Loss: 0.471, Training Accuracy: 80.43%\n",
      "Epoch [11/60], Step [200/1045], Loss: 0.287, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [300/1045], Loss: 0.336, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [400/1045], Loss: 0.405, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [500/1045], Loss: 0.372, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [600/1045], Loss: 0.525, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [700/1045], Loss: 0.328, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [800/1045], Loss: 0.357, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [900/1045], Loss: 0.366, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [1000/1045], Loss: 0.434, Training Accuracy: 85.87%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 64.50660821632414 %\n",
      "Epoch [12/60], Step [100/1045], Loss: 0.380, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [200/1045], Loss: 0.355, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [300/1045], Loss: 0.338, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [400/1045], Loss: 0.444, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [500/1045], Loss: 0.380, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [600/1045], Loss: 0.314, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [700/1045], Loss: 0.446, Training Accuracy: 83.70%\n",
      "Epoch [12/60], Step [800/1045], Loss: 0.327, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [900/1045], Loss: 0.374, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1000/1045], Loss: 0.423, Training Accuracy: 86.41%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 63.75029840057292 %\n",
      "Epoch [13/60], Step [100/1045], Loss: 0.315, Training Accuracy: 83.70%\n",
      "Epoch [13/60], Step [200/1045], Loss: 0.339, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [300/1045], Loss: 0.342, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [400/1045], Loss: 0.392, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [500/1045], Loss: 0.310, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [600/1045], Loss: 0.389, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [700/1045], Loss: 0.370, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [800/1045], Loss: 0.348, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [900/1045], Loss: 0.357, Training Accuracy: 83.70%\n",
      "Epoch [13/60], Step [1000/1045], Loss: 0.288, Training Accuracy: 87.50%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 65.22114195186528 %\n",
      "Epoch [14/60], Step [100/1045], Loss: 0.291, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [200/1045], Loss: 0.296, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [300/1045], Loss: 0.352, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [400/1045], Loss: 0.278, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [500/1045], Loss: 0.358, Training Accuracy: 85.87%\n",
      "Epoch [14/60], Step [600/1045], Loss: 0.379, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [700/1045], Loss: 0.308, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [800/1045], Loss: 0.272, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [900/1045], Loss: 0.384, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [1000/1045], Loss: 0.411, Training Accuracy: 85.33%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 64.61511751557109 %\n",
      "Epoch [15/60], Step [100/1045], Loss: 0.253, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [200/1045], Loss: 0.240, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [300/1045], Loss: 0.337, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [400/1045], Loss: 0.246, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [500/1045], Loss: 0.363, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [600/1045], Loss: 0.332, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [700/1045], Loss: 0.222, Training Accuracy: 94.57%\n",
      "Epoch [15/60], Step [800/1045], Loss: 0.326, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [900/1045], Loss: 0.346, Training Accuracy: 85.87%\n",
      "Epoch [15/60], Step [1000/1045], Loss: 0.354, Training Accuracy: 86.41%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 64.93766140758262 %\n",
      "Epoch [16/60], Step [100/1045], Loss: 0.247, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [200/1045], Loss: 0.396, Training Accuracy: 86.96%\n",
      "Epoch [16/60], Step [300/1045], Loss: 0.267, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [400/1045], Loss: 0.272, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [500/1045], Loss: 0.283, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [600/1045], Loss: 0.275, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [700/1045], Loss: 0.371, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [800/1045], Loss: 0.373, Training Accuracy: 85.33%\n",
      "Epoch [16/60], Step [900/1045], Loss: 0.379, Training Accuracy: 84.24%\n",
      "Epoch [16/60], Step [1000/1045], Loss: 0.319, Training Accuracy: 89.13%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 64.29664272228129 %\n",
      "Epoch [17/60], Step [100/1045], Loss: 0.308, Training Accuracy: 91.85%\n",
      "Epoch [17/60], Step [200/1045], Loss: 0.369, Training Accuracy: 86.41%\n",
      "Epoch [17/60], Step [300/1045], Loss: 0.256, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [400/1045], Loss: 0.320, Training Accuracy: 84.78%\n",
      "Epoch [17/60], Step [500/1045], Loss: 0.230, Training Accuracy: 91.85%\n",
      "Epoch [17/60], Step [600/1045], Loss: 0.260, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [700/1045], Loss: 0.309, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [800/1045], Loss: 0.279, Training Accuracy: 89.13%\n",
      "Epoch [17/60], Step [900/1045], Loss: 0.293, Training Accuracy: 89.67%\n",
      "Epoch [17/60], Step [1000/1045], Loss: 0.390, Training Accuracy: 86.96%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 64.21010655613186 %\n",
      "Epoch [18/60], Step [100/1045], Loss: 0.320, Training Accuracy: 87.50%\n",
      "Epoch [18/60], Step [200/1045], Loss: 0.384, Training Accuracy: 85.33%\n",
      "Epoch [18/60], Step [300/1045], Loss: 0.281, Training Accuracy: 91.30%\n",
      "Epoch [18/60], Step [400/1045], Loss: 0.254, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [500/1045], Loss: 0.378, Training Accuracy: 89.13%\n",
      "Epoch [18/60], Step [600/1045], Loss: 0.316, Training Accuracy: 84.24%\n",
      "Epoch [18/60], Step [700/1045], Loss: 0.300, Training Accuracy: 85.87%\n",
      "Epoch [18/60], Step [800/1045], Loss: 0.232, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [900/1045], Loss: 0.186, Training Accuracy: 92.39%\n",
      "Epoch [18/60], Step [1000/1045], Loss: 0.397, Training Accuracy: 88.04%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 64.60643677163132 %\n",
      "Finished early after 18 epochs!\n",
      "Final Training Accuracy 89.2699172607587 %\n",
      "Final Test Accuracy 64.57279888886478 %\n",
      "train accuracy: 0.892699172607587, train logloss: 0.2831900998558568\n",
      "\n",
      "By sample accuracy: 0.9508196721311475, log loss 0.1926562588038069\n",
      "Test Centre: GRH\n",
      "By spectra accuracy: 0.6583393876491956, logloss: 1.4118431620404257\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.5901639344262295 logloss 1.1723847610414675\n",
      "\n",
      "Test Centre: UCL\n",
      "By spectra accuracy: 0.6335970715958439, logloss: 1.5584401464595354\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.7213114754098361 logloss 1.1856335507888411\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on one centre, test on two\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "n_epochs = 60\n",
    "\n",
    "gkf = LeaveOneGroupOut()\n",
    "\n",
    "for test_index, train_index in gkf.split(SMART_x, SMART_y, groups=SMART_centre): #(note switching of test/train indices here as we are leaving out 2 centres)\n",
    "\n",
    "    X_train, X_test = SMART_x[train_index], SMART_x[test_index]\n",
    "    y_train, y_test = SMART_y[train_index], SMART_y[test_index]\n",
    "    centre_train = np.unique(SMART_centre[train_index])\n",
    "    centre_test = np.unique(SMART_centre[test_index])\n",
    "    group_train = SMART_centre[train_index]\n",
    "    group_test = SMART_centre[test_index]\n",
    "    centre_test_1 = np.unique(SMART_centre[test_index])[0]\n",
    "    centre_test_2 = np.unique(SMART_centre[test_index])[1]\n",
    "\n",
    "    print(\"Training Centre: {}\".format(list(Centre.keys())[list(Centre.values()).index(centre_train)]))\n",
    "\n",
    "\n",
    "    # Train CNN on the training and test data\n",
    "    convnet, con_mat, test_ll, test_acc, train_ll, train_acc, test_outputs, test_predicted  = train_cnn(ConvNet, X_train, y_train, X_test, y_test, learning_rate = learning_rate, num_epochs = n_epochs, batch_size = batch_size, early_stop = 5, weightedLoss = True, learning_curves = False, , cuda = True)\n",
    "    print(\"train accuracy: {}, train logloss: {}\".format(train_acc,train_ll))\n",
    "    print()\n",
    "\n",
    "    y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_batch(convnet, SMART_centre, SMART_uniquemapID, centre_train, SMART_x, SMART_y, proportioned = False )\n",
    "    acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "    ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y)  )\n",
    "    print(\"By sample accuracy: {}, log loss {}\".format(acc, ll))\n",
    "\n",
    "\n",
    "\n",
    "    # Test individual centres\n",
    "\n",
    "    print(\"Test Centre: {}\".format(list(Centre.keys())[list(Centre.values()).index(centre_test_1)]))\n",
    "    ll, acc, _  = test_a_batch(convnet, SMART_x[SMART_centre == centre_test_1], SMART_y[SMART_centre == centre_test_1], batch_size = batch_size)\n",
    "\n",
    "    print(\"By spectra accuracy: {}, logloss: {}\".format(acc,ll))\n",
    "    print()\n",
    "\n",
    "    y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_batch(convnet, SMART_centre, SMART_uniquemapID, centre_test_1, SMART_x, SMART_y, proportioned = False )\n",
    "    acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "    ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y) )\n",
    "    print(\"By SAMPLE SIMPLE CONSENSUS test accuracy: {} logloss {}\".format(acc, ll))\n",
    "    print()\n",
    "\n",
    "    print(\"Test Centre: {}\".format(list(Centre.keys())[list(Centre.values()).index(centre_test_2)]))\n",
    "\n",
    "    ll, acc, _  = test_a_batch(convnet, SMART_x[SMART_centre == centre_test_2], SMART_y[SMART_centre == centre_test_2], batch_size = batch_size)\n",
    "\n",
    "    print(\"By spectra accuracy: {}, logloss: {}\".format(acc,ll))\n",
    "    print()\n",
    "\n",
    "\n",
    "    y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_batch(convnet, SMART_centre, SMART_uniquemapID, centre_test_2, SMART_x, SMART_y, proportioned = False )\n",
    "    acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "    ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y) )\n",
    "    print(\"By SAMPLE SIMPLE CONSENSUS test accuracy: {} logloss {}\".format(acc, ll))\n",
    "    print()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7946645,
     "status": "ok",
     "timestamp": 1680532216042,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "QHIjicFsJlxh",
    "outputId": "b630a400-5bf7-410d-a7b0-2a7857e5d704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch [6/60], Step [100/735], Loss: 0.413, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [200/735], Loss: 0.563, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [300/735], Loss: 0.438, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [400/735], Loss: 0.347, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [500/735], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/735], Loss: 0.317, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [700/735], Loss: 0.339, Training Accuracy: 87.50%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 56.03466240622396 %\n",
      "Epoch [7/60], Step [100/735], Loss: 0.287, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [200/735], Loss: 0.352, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [300/735], Loss: 0.366, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [400/735], Loss: 0.405, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [500/735], Loss: 0.289, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [600/735], Loss: 0.413, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [700/735], Loss: 0.382, Training Accuracy: 83.70%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 55.946096137816056 %\n",
      "Epoch [8/60], Step [100/735], Loss: 0.310, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [200/735], Loss: 0.244, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [300/735], Loss: 0.339, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [400/735], Loss: 0.292, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [500/735], Loss: 0.268, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [600/735], Loss: 0.340, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [700/735], Loss: 0.366, Training Accuracy: 85.33%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 55.63350930814115 %\n",
      "Epoch [9/60], Step [100/735], Loss: 0.451, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [200/735], Loss: 0.352, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [300/735], Loss: 0.358, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [400/735], Loss: 0.350, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [500/735], Loss: 0.379, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [600/735], Loss: 0.352, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [700/735], Loss: 0.414, Training Accuracy: 86.96%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 55.635245901639344 %\n",
      "Epoch [10/60], Step [100/735], Loss: 0.230, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [200/735], Loss: 0.339, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [300/735], Loss: 0.299, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [400/735], Loss: 0.267, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [500/735], Loss: 0.450, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [600/735], Loss: 0.263, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [700/735], Loss: 0.379, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 58.44505418171715 %\n",
      "Epoch [11/60], Step [100/735], Loss: 0.234, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [200/735], Loss: 0.290, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [300/735], Loss: 0.271, Training Accuracy: 92.93%\n",
      "Epoch [11/60], Step [400/735], Loss: 0.317, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [500/735], Loss: 0.390, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [600/735], Loss: 0.291, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [700/735], Loss: 0.231, Training Accuracy: 92.93%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 57.04362322867463 %\n",
      "Epoch [12/60], Step [100/735], Loss: 0.237, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [200/735], Loss: 0.295, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [300/735], Loss: 0.444, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [400/735], Loss: 0.186, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [500/735], Loss: 0.268, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [600/735], Loss: 0.428, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [700/735], Loss: 0.295, Training Accuracy: 86.41%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 56.16664351208669 %\n",
      "Epoch [13/60], Step [100/735], Loss: 0.292, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [200/735], Loss: 0.299, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [300/735], Loss: 0.367, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [400/735], Loss: 0.287, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [500/735], Loss: 0.382, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [600/735], Loss: 0.278, Training Accuracy: 92.39%\n",
      "Epoch [13/60], Step [700/735], Loss: 0.159, Training Accuracy: 95.65%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 56.27257571547653 %\n",
      "Epoch [14/60], Step [100/735], Loss: 0.207, Training Accuracy: 92.93%\n",
      "Epoch [14/60], Step [200/735], Loss: 0.233, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [300/735], Loss: 0.330, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [400/735], Loss: 0.153, Training Accuracy: 95.65%\n",
      "Epoch [14/60], Step [500/735], Loss: 0.328, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [600/735], Loss: 0.252, Training Accuracy: 93.48%\n",
      "Epoch [14/60], Step [700/735], Loss: 0.384, Training Accuracy: 86.96%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 56.57821617115866 %\n",
      "Epoch [15/60], Step [100/735], Loss: 0.271, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [200/735], Loss: 0.222, Training Accuracy: 90.76%\n",
      "Epoch [15/60], Step [300/735], Loss: 0.271, Training Accuracy: 88.04%\n",
      "Epoch [15/60], Step [400/735], Loss: 0.277, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [500/735], Loss: 0.197, Training Accuracy: 95.65%\n",
      "Epoch [15/60], Step [600/735], Loss: 0.288, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [700/735], Loss: 0.253, Training Accuracy: 91.85%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 55.06390664073354 %\n",
      "Finished early after 15 epochs!\n",
      "Final Training Accuracy 91.41108380946741 %\n",
      "Final Test Accuracy 55.18373159210892 %\n",
      "Epoch [1/60], Step [100/666], Loss: 1.142, Training Accuracy: 45.11%\n",
      "Epoch [1/60], Step [200/666], Loss: 0.905, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [300/666], Loss: 0.831, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [400/666], Loss: 0.721, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [500/666], Loss: 0.686, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [600/666], Loss: 0.597, Training Accuracy: 82.07%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 50.028500584261984 %\n",
      "Epoch [2/60], Step [100/666], Loss: 0.598, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [200/666], Loss: 0.644, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [300/666], Loss: 0.601, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [400/666], Loss: 0.623, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [500/666], Loss: 0.530, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [600/666], Loss: 0.511, Training Accuracy: 86.96%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 49.18773334853365 %\n",
      "Epoch [3/60], Step [100/666], Loss: 0.537, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [200/666], Loss: 0.566, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [300/666], Loss: 0.515, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [400/666], Loss: 0.448, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [500/666], Loss: 0.427, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [600/666], Loss: 0.438, Training Accuracy: 82.07%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 50.7452902784507 %\n",
      "Epoch [4/60], Step [100/666], Loss: 0.577, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [200/666], Loss: 0.505, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [300/666], Loss: 0.490, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [400/666], Loss: 0.545, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [500/666], Loss: 0.381, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [600/666], Loss: 0.507, Training Accuracy: 80.98%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 48.493744121754496 %\n",
      "Epoch [5/60], Step [100/666], Loss: 0.430, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [200/666], Loss: 0.483, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [300/666], Loss: 0.383, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [400/666], Loss: 0.383, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [500/666], Loss: 0.367, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [600/666], Loss: 0.342, Training Accuracy: 87.50%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 48.11611138028329 %\n",
      "Epoch [6/60], Step [100/666], Loss: 0.442, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [200/666], Loss: 0.352, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [300/666], Loss: 0.397, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [400/666], Loss: 0.392, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [500/666], Loss: 0.412, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [600/666], Loss: 0.475, Training Accuracy: 81.52%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 49.21338387436943 %\n",
      "Epoch [7/60], Step [100/666], Loss: 0.343, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [200/666], Loss: 0.419, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [300/666], Loss: 0.262, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [400/666], Loss: 0.304, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [500/666], Loss: 0.339, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [600/666], Loss: 0.346, Training Accuracy: 89.67%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 50.08692678199903 %\n",
      "Epoch [8/60], Step [100/666], Loss: 0.311, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [200/666], Loss: 0.312, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [300/666], Loss: 0.306, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [400/666], Loss: 0.354, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [500/666], Loss: 0.441, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [600/666], Loss: 0.368, Training Accuracy: 87.50%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 49.8731724000342 %\n",
      "Finished early after 8 epochs!\n",
      "Final Training Accuracy 89.06354569642362 %\n",
      "Final Test Accuracy 49.85179696183772 %\n",
      "Epoch [1/60], Step [100/695], Loss: 1.218, Training Accuracy: 48.91%\n",
      "Epoch [1/60], Step [200/695], Loss: 1.178, Training Accuracy: 54.89%\n",
      "Epoch [1/60], Step [300/695], Loss: 0.977, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [400/695], Loss: 0.918, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [500/695], Loss: 0.945, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [600/695], Loss: 0.918, Training Accuracy: 67.93%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 73.38394259493086 %\n",
      "Epoch [2/60], Step [100/695], Loss: 0.794, Training Accuracy: 67.93%\n",
      "Epoch [2/60], Step [200/695], Loss: 0.754, Training Accuracy: 70.11%\n",
      "Epoch [2/60], Step [300/695], Loss: 0.789, Training Accuracy: 70.65%\n",
      "Epoch [2/60], Step [400/695], Loss: 0.846, Training Accuracy: 67.39%\n",
      "Epoch [2/60], Step [500/695], Loss: 0.804, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [600/695], Loss: 0.746, Training Accuracy: 71.74%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 74.47876566782668 %\n",
      "Epoch [3/60], Step [100/695], Loss: 0.814, Training Accuracy: 71.20%\n",
      "Epoch [3/60], Step [200/695], Loss: 0.695, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [300/695], Loss: 0.745, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [400/695], Loss: 0.613, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [500/695], Loss: 0.765, Training Accuracy: 69.57%\n",
      "Epoch [3/60], Step [600/695], Loss: 0.658, Training Accuracy: 77.17%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 74.30630408672354 %\n",
      "Epoch [4/60], Step [100/695], Loss: 0.736, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [200/695], Loss: 0.678, Training Accuracy: 75.00%\n",
      "Epoch [4/60], Step [300/695], Loss: 0.625, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [400/695], Loss: 0.650, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [500/695], Loss: 0.568, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [600/695], Loss: 0.589, Training Accuracy: 83.15%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 73.1190908810939 %\n",
      "Epoch [5/60], Step [100/695], Loss: 0.640, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [200/695], Loss: 0.598, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [300/695], Loss: 0.612, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [400/695], Loss: 0.627, Training Accuracy: 73.91%\n",
      "Epoch [5/60], Step [500/695], Loss: 0.697, Training Accuracy: 76.09%\n",
      "Epoch [5/60], Step [600/695], Loss: 0.580, Training Accuracy: 79.35%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 73.9598410889717 %\n",
      "Epoch [6/60], Step [100/695], Loss: 0.631, Training Accuracy: 76.09%\n",
      "Epoch [6/60], Step [200/695], Loss: 0.497, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [300/695], Loss: 0.581, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [400/695], Loss: 0.595, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [500/695], Loss: 0.632, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [600/695], Loss: 0.639, Training Accuracy: 77.72%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 72.39998768131564 %\n",
      "Epoch [7/60], Step [100/695], Loss: 0.638, Training Accuracy: 76.63%\n",
      "Epoch [7/60], Step [200/695], Loss: 0.512, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [300/695], Loss: 0.601, Training Accuracy: 78.26%\n",
      "Epoch [7/60], Step [400/695], Loss: 0.569, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [500/695], Loss: 0.539, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [600/695], Loss: 0.533, Training Accuracy: 83.15%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 71.6839641526285 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 82.21872602889839 %\n",
      "Final Test Accuracy 71.65162760617166 %\n",
      "Epoch [1/60], Step [100/788], Loss: 1.041, Training Accuracy: 60.33%\n",
      "Epoch [1/60], Step [200/788], Loss: 0.874, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [300/788], Loss: 0.872, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [400/788], Loss: 0.748, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [500/788], Loss: 0.703, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [600/788], Loss: 0.619, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [700/788], Loss: 0.612, Training Accuracy: 78.26%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 48.24273811264017 %\n",
      "Epoch [2/60], Step [100/788], Loss: 0.597, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [200/788], Loss: 0.613, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [300/788], Loss: 0.591, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [400/788], Loss: 0.714, Training Accuracy: 72.28%\n",
      "Epoch [2/60], Step [500/788], Loss: 0.531, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [600/788], Loss: 0.637, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [700/788], Loss: 0.768, Training Accuracy: 71.74%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 48.436945308747674 %\n",
      "Epoch [3/60], Step [100/788], Loss: 0.498, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [200/788], Loss: 0.552, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [300/788], Loss: 0.491, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [400/788], Loss: 0.602, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [500/788], Loss: 0.568, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [600/788], Loss: 0.505, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [700/788], Loss: 0.585, Training Accuracy: 79.89%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 50.42078225823292 %\n",
      "Epoch [4/60], Step [100/788], Loss: 0.565, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [200/788], Loss: 0.537, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [300/788], Loss: 0.448, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [400/788], Loss: 0.417, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [500/788], Loss: 0.503, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [600/788], Loss: 0.585, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [700/788], Loss: 0.561, Training Accuracy: 80.43%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 52.761709858625515 %\n",
      "Epoch [5/60], Step [100/788], Loss: 0.536, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [200/788], Loss: 0.393, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [300/788], Loss: 0.500, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [400/788], Loss: 0.428, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [500/788], Loss: 0.426, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [600/788], Loss: 0.432, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [700/788], Loss: 0.397, Training Accuracy: 82.61%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 55.534905089063834 %\n",
      "Epoch [6/60], Step [100/788], Loss: 0.352, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [200/788], Loss: 0.374, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [300/788], Loss: 0.362, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [400/788], Loss: 0.370, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [500/788], Loss: 0.296, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [600/788], Loss: 0.403, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [700/788], Loss: 0.415, Training Accuracy: 86.96%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 54.361308914736775 %\n",
      "Epoch [7/60], Step [100/788], Loss: 0.440, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [200/788], Loss: 0.403, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [300/788], Loss: 0.378, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [400/788], Loss: 0.310, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [500/788], Loss: 0.348, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [600/788], Loss: 0.347, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [700/788], Loss: 0.409, Training Accuracy: 82.61%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 54.098189487752414 %\n",
      "Epoch [8/60], Step [100/788], Loss: 0.414, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [200/788], Loss: 0.370, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [300/788], Loss: 0.304, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [400/788], Loss: 0.328, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [500/788], Loss: 0.287, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [600/788], Loss: 0.298, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [700/788], Loss: 0.304, Training Accuracy: 87.50%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 54.37383841125985 %\n",
      "Epoch [9/60], Step [100/788], Loss: 0.367, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [200/788], Loss: 0.379, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [300/788], Loss: 0.418, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [400/788], Loss: 0.310, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [500/788], Loss: 0.279, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [600/788], Loss: 0.313, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [700/788], Loss: 0.298, Training Accuracy: 89.13%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 54.68916407375697 %\n",
      "Epoch [10/60], Step [100/788], Loss: 0.292, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [200/788], Loss: 0.342, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [300/788], Loss: 0.429, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [400/788], Loss: 0.327, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [500/788], Loss: 0.347, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [600/788], Loss: 0.334, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [700/788], Loss: 0.269, Training Accuracy: 90.22%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 54.595192849833985 %\n",
      "Finished early after 10 epochs!\n",
      "Final Training Accuracy 88.47755381077666 %\n",
      "Final Test Accuracy 54.624428341721135 %\n",
      "Epoch [1/60], Step [100/626], Loss: 1.189, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [200/626], Loss: 0.993, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [300/626], Loss: 1.000, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [400/626], Loss: 0.832, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [500/626], Loss: 0.628, Training Accuracy: 83.70%\n",
      "Epoch [1/60], Step [600/626], Loss: 0.738, Training Accuracy: 78.26%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 58.496955027102196 %\n",
      "Epoch [2/60], Step [100/626], Loss: 0.774, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [200/626], Loss: 0.764, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [300/626], Loss: 0.638, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [400/626], Loss: 0.691, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [500/626], Loss: 0.710, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [600/626], Loss: 0.619, Training Accuracy: 80.43%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 59.92480912590609 %\n",
      "Epoch [3/60], Step [100/626], Loss: 0.623, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [200/626], Loss: 0.598, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [300/626], Loss: 0.592, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [400/626], Loss: 0.671, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [500/626], Loss: 0.560, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [600/626], Loss: 0.574, Training Accuracy: 84.24%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 59.02097361881832 %\n",
      "Epoch [4/60], Step [100/626], Loss: 0.649, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [200/626], Loss: 0.620, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [300/626], Loss: 0.592, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [400/626], Loss: 0.555, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [500/626], Loss: 0.480, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [600/626], Loss: 0.510, Training Accuracy: 83.70%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 57.39226718510603 %\n",
      "Epoch [5/60], Step [100/626], Loss: 0.556, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [200/626], Loss: 0.572, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [300/626], Loss: 0.519, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [400/626], Loss: 0.571, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [500/626], Loss: 0.459, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [600/626], Loss: 0.440, Training Accuracy: 87.50%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 57.55320655602622 %\n",
      "Epoch [6/60], Step [100/626], Loss: 0.569, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [200/626], Loss: 0.553, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [300/626], Loss: 0.540, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [400/626], Loss: 0.446, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [500/626], Loss: 0.580, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [600/626], Loss: 0.483, Training Accuracy: 86.41%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 58.634719128609866 %\n",
      "Epoch [7/60], Step [100/626], Loss: 0.474, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [200/626], Loss: 0.412, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [300/626], Loss: 0.493, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [400/626], Loss: 0.517, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [500/626], Loss: 0.423, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [600/626], Loss: 0.508, Training Accuracy: 82.61%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 59.208950804053096 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 86.18894037259521 %\n",
      "Final Test Accuracy 59.369890174973285 %\n",
      "Epoch [1/60], Step [100/683], Loss: 1.070, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [200/683], Loss: 0.980, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [300/683], Loss: 0.825, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [400/683], Loss: 0.948, Training Accuracy: 64.13%\n",
      "Epoch [1/60], Step [500/683], Loss: 0.724, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [600/683], Loss: 0.804, Training Accuracy: 73.37%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 64.9320862623615 %\n",
      "Epoch [2/60], Step [100/683], Loss: 0.687, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [200/683], Loss: 0.623, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [300/683], Loss: 0.806, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [400/683], Loss: 0.638, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [500/683], Loss: 0.593, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [600/683], Loss: 0.627, Training Accuracy: 77.17%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 66.4095079232694 %\n",
      "Epoch [3/60], Step [100/683], Loss: 0.649, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [200/683], Loss: 0.467, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [300/683], Loss: 0.604, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [400/683], Loss: 0.568, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [500/683], Loss: 0.492, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [600/683], Loss: 0.461, Training Accuracy: 79.89%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 66.83099011080662 %\n",
      "Epoch [4/60], Step [100/683], Loss: 0.378, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [200/683], Loss: 0.553, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [300/683], Loss: 0.446, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [400/683], Loss: 0.473, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [500/683], Loss: 0.523, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [600/683], Loss: 0.425, Training Accuracy: 85.33%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 66.85779816513761 %\n",
      "Epoch [5/60], Step [100/683], Loss: 0.584, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [200/683], Loss: 0.471, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [300/683], Loss: 0.623, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [400/683], Loss: 0.412, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [500/683], Loss: 0.497, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [600/683], Loss: 0.348, Training Accuracy: 88.59%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 68.22054092696294 %\n",
      "Epoch [6/60], Step [100/683], Loss: 0.313, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [200/683], Loss: 0.414, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [300/683], Loss: 0.427, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [400/683], Loss: 0.434, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [500/683], Loss: 0.401, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [600/683], Loss: 0.549, Training Accuracy: 79.89%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 66.1205766710354 %\n",
      "Epoch [7/60], Step [100/683], Loss: 0.351, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [200/683], Loss: 0.524, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [300/683], Loss: 0.429, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [400/683], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [500/683], Loss: 0.394, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [600/683], Loss: 0.372, Training Accuracy: 87.50%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 65.13016799714048 %\n",
      "Epoch [8/60], Step [100/683], Loss: 0.331, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [200/683], Loss: 0.397, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [300/683], Loss: 0.320, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [400/683], Loss: 0.353, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [500/683], Loss: 0.316, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [600/683], Loss: 0.300, Training Accuracy: 88.04%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 66.48397474085547 %\n",
      "Epoch [9/60], Step [100/683], Loss: 0.350, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [200/683], Loss: 0.262, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [300/683], Loss: 0.374, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [400/683], Loss: 0.217, Training Accuracy: 93.48%\n",
      "Epoch [9/60], Step [500/683], Loss: 0.339, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [600/683], Loss: 0.310, Training Accuracy: 89.13%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 65.68717979268438 %\n",
      "Epoch [10/60], Step [100/683], Loss: 0.353, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [200/683], Loss: 0.325, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [300/683], Loss: 0.376, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [400/683], Loss: 0.295, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [500/683], Loss: 0.400, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [600/683], Loss: 0.275, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 65.61122363874658 %\n",
      "Finished early after 10 epochs!\n",
      "Final Training Accuracy 89.17853388129599 %\n",
      "Final Test Accuracy 65.70356249255332 %\n",
      "Epoch [1/60], Step [100/738], Loss: 1.227, Training Accuracy: 56.52%\n",
      "Epoch [1/60], Step [200/738], Loss: 1.052, Training Accuracy: 62.50%\n",
      "Epoch [1/60], Step [300/738], Loss: 0.875, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [400/738], Loss: 0.910, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [500/738], Loss: 0.781, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [600/738], Loss: 0.881, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [700/738], Loss: 0.843, Training Accuracy: 70.11%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 62.87597279674683 %\n",
      "Epoch [2/60], Step [100/738], Loss: 0.706, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [200/738], Loss: 0.705, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [300/738], Loss: 0.705, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [400/738], Loss: 0.693, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [500/738], Loss: 0.723, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [600/738], Loss: 0.721, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [700/738], Loss: 0.729, Training Accuracy: 75.00%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 65.85220500595948 %\n",
      "Epoch [3/60], Step [100/738], Loss: 0.703, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [200/738], Loss: 0.707, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [300/738], Loss: 0.659, Training Accuracy: 75.54%\n",
      "Epoch [3/60], Step [400/738], Loss: 0.610, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [500/738], Loss: 0.660, Training Accuracy: 75.54%\n",
      "Epoch [3/60], Step [600/738], Loss: 0.669, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [700/738], Loss: 0.696, Training Accuracy: 77.17%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 64.9284862932062 %\n",
      "Epoch [4/60], Step [100/738], Loss: 0.619, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [200/738], Loss: 0.599, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [300/738], Loss: 0.623, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [400/738], Loss: 0.575, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [500/738], Loss: 0.609, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [600/738], Loss: 0.602, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [700/738], Loss: 0.653, Training Accuracy: 76.63%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 63.76989413166936 %\n",
      "Epoch [5/60], Step [100/738], Loss: 0.635, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [200/738], Loss: 0.548, Training Accuracy: 76.63%\n",
      "Epoch [5/60], Step [300/738], Loss: 0.481, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [400/738], Loss: 0.568, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [500/738], Loss: 0.675, Training Accuracy: 74.46%\n",
      "Epoch [5/60], Step [600/738], Loss: 0.667, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [700/738], Loss: 0.507, Training Accuracy: 82.61%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 62.05216293907313 %\n",
      "Epoch [6/60], Step [100/738], Loss: 0.541, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [200/738], Loss: 0.634, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [300/738], Loss: 0.528, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [400/738], Loss: 0.576, Training Accuracy: 77.72%\n",
      "Epoch [6/60], Step [500/738], Loss: 0.549, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [600/738], Loss: 0.486, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [700/738], Loss: 0.610, Training Accuracy: 79.89%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 63.75411904928837 %\n",
      "Epoch [7/60], Step [100/738], Loss: 0.550, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [200/738], Loss: 0.476, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [300/738], Loss: 0.433, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [400/738], Loss: 0.530, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [500/738], Loss: 0.590, Training Accuracy: 77.17%\n",
      "Epoch [7/60], Step [600/738], Loss: 0.576, Training Accuracy: 76.63%\n",
      "Epoch [7/60], Step [700/738], Loss: 0.586, Training Accuracy: 82.07%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 63.414078384631566 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 82.46490917669261 %\n",
      "Final Test Accuracy 63.38953936759447 %\n",
      "Epoch [1/60], Step [100/672], Loss: 1.114, Training Accuracy: 54.89%\n",
      "Epoch [1/60], Step [200/672], Loss: 0.899, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [300/672], Loss: 0.836, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [400/672], Loss: 0.869, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [500/672], Loss: 0.767, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [600/672], Loss: 0.644, Training Accuracy: 78.80%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 51.52132454923717 %\n",
      "Epoch [2/60], Step [100/672], Loss: 0.570, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [200/672], Loss: 0.825, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/672], Loss: 0.599, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [400/672], Loss: 0.667, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [500/672], Loss: 0.544, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [600/672], Loss: 0.584, Training Accuracy: 78.26%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 52.21047156726768 %\n",
      "Epoch [3/60], Step [100/672], Loss: 0.533, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [200/672], Loss: 0.536, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [300/672], Loss: 0.584, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [400/672], Loss: 0.450, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [500/672], Loss: 0.572, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [600/672], Loss: 0.424, Training Accuracy: 83.15%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 55.11442441054092 %\n",
      "Epoch [4/60], Step [100/672], Loss: 0.568, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [200/672], Loss: 0.487, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [300/672], Loss: 0.510, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [400/672], Loss: 0.441, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [500/672], Loss: 0.516, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [600/672], Loss: 0.403, Training Accuracy: 84.78%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 55.45972029588534 %\n",
      "Epoch [5/60], Step [100/672], Loss: 0.383, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [200/672], Loss: 0.365, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [300/672], Loss: 0.332, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [400/672], Loss: 0.472, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [500/672], Loss: 0.345, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [600/672], Loss: 0.394, Training Accuracy: 85.33%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 53.40094775774388 %\n",
      "Epoch [6/60], Step [100/672], Loss: 0.446, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [200/672], Loss: 0.391, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [300/672], Loss: 0.473, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [400/672], Loss: 0.385, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [500/672], Loss: 0.449, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [600/672], Loss: 0.338, Training Accuracy: 86.96%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 57.06050624133149 %\n",
      "Epoch [7/60], Step [100/672], Loss: 0.309, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [200/672], Loss: 0.306, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [300/672], Loss: 0.248, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [400/672], Loss: 0.535, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [500/672], Loss: 0.380, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [600/672], Loss: 0.377, Training Accuracy: 85.87%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 56.531726768377254 %\n",
      "Epoch [8/60], Step [100/672], Loss: 0.380, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [200/672], Loss: 0.298, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [300/672], Loss: 0.358, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [400/672], Loss: 0.310, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [500/672], Loss: 0.368, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [600/672], Loss: 0.307, Training Accuracy: 86.41%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 55.111534905224225 %\n",
      "Epoch [9/60], Step [100/672], Loss: 0.273, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [200/672], Loss: 0.341, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [300/672], Loss: 0.248, Training Accuracy: 93.48%\n",
      "Epoch [9/60], Step [400/672], Loss: 0.310, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [500/672], Loss: 0.224, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [600/672], Loss: 0.353, Training Accuracy: 87.50%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 55.23433888118354 %\n",
      "Epoch [10/60], Step [100/672], Loss: 0.382, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [200/672], Loss: 0.360, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [300/672], Loss: 0.424, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [400/672], Loss: 0.349, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [500/672], Loss: 0.389, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [600/672], Loss: 0.376, Training Accuracy: 86.41%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 55.83969024503005 %\n",
      "Epoch [11/60], Step [100/672], Loss: 0.217, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [200/672], Loss: 0.284, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [300/672], Loss: 0.254, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [400/672], Loss: 0.257, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [500/672], Loss: 0.323, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [600/672], Loss: 0.307, Training Accuracy: 86.96%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 53.93117198335645 %\n",
      "Finished early after 11 epochs!\n",
      "Final Training Accuracy 88.62119788798549 %\n",
      "Final Test Accuracy 53.90805594082293 %\n",
      "Epoch [1/60], Step [100/687], Loss: 1.139, Training Accuracy: 59.24%\n",
      "Epoch [1/60], Step [200/687], Loss: 1.011, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [300/687], Loss: 0.942, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [400/687], Loss: 0.882, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [500/687], Loss: 0.890, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [600/687], Loss: 0.752, Training Accuracy: 79.35%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 69.49211223506744 %\n",
      "Epoch [2/60], Step [100/687], Loss: 0.800, Training Accuracy: 66.85%\n",
      "Epoch [2/60], Step [200/687], Loss: 0.722, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [300/687], Loss: 0.723, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [400/687], Loss: 0.753, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [500/687], Loss: 0.676, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [600/687], Loss: 0.603, Training Accuracy: 79.35%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 65.32845616570327 %\n",
      "Epoch [3/60], Step [100/687], Loss: 0.718, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [200/687], Loss: 0.687, Training Accuracy: 75.54%\n",
      "Epoch [3/60], Step [300/687], Loss: 0.742, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [400/687], Loss: 0.641, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [500/687], Loss: 0.637, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [600/687], Loss: 0.667, Training Accuracy: 78.80%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 66.37012283236994 %\n",
      "Epoch [4/60], Step [100/687], Loss: 0.640, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [200/687], Loss: 0.620, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [300/687], Loss: 0.581, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [400/687], Loss: 0.483, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [500/687], Loss: 0.524, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [600/687], Loss: 0.474, Training Accuracy: 86.96%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 65.71230732177264 %\n",
      "Epoch [5/60], Step [100/687], Loss: 0.439, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [200/687], Loss: 0.604, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [300/687], Loss: 0.519, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [400/687], Loss: 0.514, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [500/687], Loss: 0.550, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [600/687], Loss: 0.444, Training Accuracy: 86.96%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 66.95267341040463 %\n",
      "Epoch [6/60], Step [100/687], Loss: 0.548, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [200/687], Loss: 0.498, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [300/687], Loss: 0.467, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [400/687], Loss: 0.506, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [500/687], Loss: 0.433, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [600/687], Loss: 0.434, Training Accuracy: 85.33%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 66.7569845857418 %\n",
      "Finished early after 6 epochs!\n",
      "Final Training Accuracy 85.23378844996357 %\n",
      "Final Test Accuracy 66.77053227360308 %\n",
      "Epoch [1/60], Step [100/745], Loss: 1.239, Training Accuracy: 60.33%\n",
      "Epoch [1/60], Step [200/745], Loss: 1.097, Training Accuracy: 60.87%\n",
      "Epoch [1/60], Step [300/745], Loss: 0.904, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [400/745], Loss: 0.908, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [500/745], Loss: 0.868, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [600/745], Loss: 0.869, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [700/745], Loss: 0.869, Training Accuracy: 73.91%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 61.880387931034484 %\n",
      "Epoch [2/60], Step [100/745], Loss: 0.729, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [200/745], Loss: 0.622, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [300/745], Loss: 0.751, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [400/745], Loss: 0.770, Training Accuracy: 70.11%\n",
      "Epoch [2/60], Step [500/745], Loss: 0.616, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [600/745], Loss: 0.695, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [700/745], Loss: 0.575, Training Accuracy: 79.89%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 64.68211206896551 %\n",
      "Epoch [3/60], Step [100/745], Loss: 0.622, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [200/745], Loss: 0.576, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [300/745], Loss: 0.598, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [400/745], Loss: 0.634, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [500/745], Loss: 0.641, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [600/745], Loss: 0.446, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [700/745], Loss: 0.701, Training Accuracy: 75.54%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 66.14403735632183 %\n",
      "Epoch [4/60], Step [100/745], Loss: 0.582, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [200/745], Loss: 0.459, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [300/745], Loss: 0.477, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [400/745], Loss: 0.464, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [500/745], Loss: 0.469, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [600/745], Loss: 0.477, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [700/745], Loss: 0.553, Training Accuracy: 76.09%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 65.316091954023 %\n",
      "Epoch [5/60], Step [100/745], Loss: 0.415, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [200/745], Loss: 0.429, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [300/745], Loss: 0.618, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [400/745], Loss: 0.427, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [500/745], Loss: 0.512, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [600/745], Loss: 0.566, Training Accuracy: 76.09%\n",
      "Epoch [5/60], Step [700/745], Loss: 0.465, Training Accuracy: 85.87%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 66.20150862068965 %\n",
      "Epoch [6/60], Step [100/745], Loss: 0.441, Training Accuracy: 75.00%\n",
      "Epoch [6/60], Step [200/745], Loss: 0.392, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [300/745], Loss: 0.465, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [400/745], Loss: 0.416, Training Accuracy: 77.72%\n",
      "Epoch [6/60], Step [500/745], Loss: 0.415, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [600/745], Loss: 0.511, Training Accuracy: 76.09%\n",
      "Epoch [6/60], Step [700/745], Loss: 0.433, Training Accuracy: 83.15%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 65.3412356321839 %\n",
      "Epoch [7/60], Step [100/745], Loss: 0.478, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [200/745], Loss: 0.461, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [300/745], Loss: 0.345, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [400/745], Loss: 0.409, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [500/745], Loss: 0.376, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [600/745], Loss: 0.318, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [700/745], Loss: 0.375, Training Accuracy: 86.96%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 65.17959770114943 %\n",
      "Epoch [8/60], Step [100/745], Loss: 0.378, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [200/745], Loss: 0.463, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [300/745], Loss: 0.312, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [400/745], Loss: 0.452, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [500/745], Loss: 0.407, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [600/745], Loss: 0.490, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [700/745], Loss: 0.393, Training Accuracy: 84.78%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 63.66918103448276 %\n",
      "Epoch [9/60], Step [100/745], Loss: 0.479, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [200/745], Loss: 0.455, Training Accuracy: 80.98%\n",
      "Epoch [9/60], Step [300/745], Loss: 0.456, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [400/745], Loss: 0.329, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [500/745], Loss: 0.484, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [600/745], Loss: 0.420, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [700/745], Loss: 0.342, Training Accuracy: 90.76%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 63.85775862068965 %\n",
      "Epoch [10/60], Step [100/745], Loss: 0.353, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [200/745], Loss: 0.336, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [300/745], Loss: 0.294, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [400/745], Loss: 0.321, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [500/745], Loss: 0.304, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [600/745], Loss: 0.410, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [700/745], Loss: 0.521, Training Accuracy: 89.67%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 64.16666666666667 %\n",
      "Finished early after 10 epochs!\n",
      "Final Training Accuracy 85.58093708947598 %\n",
      "Final Test Accuracy 64.18283045977012 %\n",
      "Epoch [1/60], Step [100/652], Loss: 1.032, Training Accuracy: 60.33%\n",
      "Epoch [1/60], Step [200/652], Loss: 0.787, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [300/652], Loss: 0.750, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [400/652], Loss: 0.706, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [500/652], Loss: 0.742, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [600/652], Loss: 0.616, Training Accuracy: 81.52%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 58.03415872144866 %\n",
      "Epoch [2/60], Step [100/652], Loss: 0.646, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [200/652], Loss: 0.560, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [300/652], Loss: 0.585, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [400/652], Loss: 0.605, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [500/652], Loss: 0.541, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [600/652], Loss: 0.562, Training Accuracy: 84.24%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 56.774813087317376 %\n",
      "Epoch [3/60], Step [100/652], Loss: 0.576, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [200/652], Loss: 0.486, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [300/652], Loss: 0.531, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [400/652], Loss: 0.514, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [500/652], Loss: 0.478, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [600/652], Loss: 0.457, Training Accuracy: 84.24%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 54.294533232732014 %\n",
      "Epoch [4/60], Step [100/652], Loss: 0.466, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [200/652], Loss: 0.545, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [300/652], Loss: 0.448, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [400/652], Loss: 0.518, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [500/652], Loss: 0.511, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [600/652], Loss: 0.479, Training Accuracy: 88.59%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 53.74031140681803 %\n",
      "Epoch [5/60], Step [100/652], Loss: 0.410, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [200/652], Loss: 0.405, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [300/652], Loss: 0.443, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [400/652], Loss: 0.478, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [500/652], Loss: 0.388, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [600/652], Loss: 0.449, Training Accuracy: 83.15%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 54.87619178270114 %\n",
      "Epoch [6/60], Step [100/652], Loss: 0.519, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [200/652], Loss: 0.413, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [300/652], Loss: 0.398, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [400/652], Loss: 0.410, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [500/652], Loss: 0.324, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [600/652], Loss: 0.341, Training Accuracy: 88.04%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 53.46868783867206 %\n",
      "Finished early after 6 epochs!\n",
      "Final Training Accuracy 87.3444347063979 %\n",
      "Final Test Accuracy 53.666232251869125 %\n",
      "Epoch [1/60], Step [100/699], Loss: 1.187, Training Accuracy: 59.78%\n",
      "Epoch [1/60], Step [200/699], Loss: 0.985, Training Accuracy: 66.85%\n",
      "Epoch [1/60], Step [300/699], Loss: 0.894, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [400/699], Loss: 0.886, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [500/699], Loss: 0.738, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [600/699], Loss: 0.799, Training Accuracy: 77.72%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 60.5504873294347 %\n",
      "Epoch [2/60], Step [100/699], Loss: 0.664, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [200/699], Loss: 0.703, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [300/699], Loss: 0.758, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [400/699], Loss: 0.589, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [500/699], Loss: 0.642, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [600/699], Loss: 0.659, Training Accuracy: 80.43%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 59.968810916179336 %\n",
      "Epoch [3/60], Step [100/699], Loss: 0.627, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [200/699], Loss: 0.543, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [300/699], Loss: 0.620, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [400/699], Loss: 0.664, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [500/699], Loss: 0.508, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [600/699], Loss: 0.650, Training Accuracy: 79.89%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 60.38986354775828 %\n",
      "Epoch [4/60], Step [100/699], Loss: 0.642, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [200/699], Loss: 0.612, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [300/699], Loss: 0.593, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [400/699], Loss: 0.634, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [500/699], Loss: 0.568, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [600/699], Loss: 0.530, Training Accuracy: 82.61%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 60.644054580896686 %\n",
      "Epoch [5/60], Step [100/699], Loss: 0.702, Training Accuracy: 75.54%\n",
      "Epoch [5/60], Step [200/699], Loss: 0.653, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [300/699], Loss: 0.472, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [400/699], Loss: 0.490, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [500/699], Loss: 0.515, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [600/699], Loss: 0.456, Training Accuracy: 85.33%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 57.48927875243665 %\n",
      "Epoch [6/60], Step [100/699], Loss: 0.561, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [200/699], Loss: 0.410, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [300/699], Loss: 0.473, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [400/699], Loss: 0.532, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [500/699], Loss: 0.670, Training Accuracy: 77.17%\n",
      "Epoch [6/60], Step [600/699], Loss: 0.455, Training Accuracy: 86.41%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 57.723196881091624 %\n",
      "Epoch [7/60], Step [100/699], Loss: 0.471, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [200/699], Loss: 0.469, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [300/699], Loss: 0.390, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [400/699], Loss: 0.421, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [500/699], Loss: 0.499, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [600/699], Loss: 0.350, Training Accuracy: 89.13%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 60.09200779727095 %\n",
      "Epoch [8/60], Step [100/699], Loss: 0.399, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [200/699], Loss: 0.495, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [300/699], Loss: 0.268, Training Accuracy: 93.48%\n",
      "Epoch [8/60], Step [400/699], Loss: 0.467, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [500/699], Loss: 0.475, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [600/699], Loss: 0.472, Training Accuracy: 84.78%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 57.81988304093567 %\n",
      "Epoch [9/60], Step [100/699], Loss: 0.446, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [200/699], Loss: 0.330, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [300/699], Loss: 0.426, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [400/699], Loss: 0.329, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [500/699], Loss: 0.516, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [600/699], Loss: 0.501, Training Accuracy: 84.24%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 59.18440545808967 %\n",
      "Finished early after 9 epochs!\n",
      "Final Training Accuracy 87.00213882947696 %\n",
      "Final Test Accuracy 59.15477582846004 %\n",
      "By Spectra accuracy: 0.6030468053326282 +/- 0.06559709960801177\n",
      "By Sample simple accuracy: 0.5763739562810771 +/- 0.12339447666576508\n",
      "By Sample simple log loss: 2.0870822364635284 +/- 1.2221087617521065\n",
      "Centre: UCL\n",
      "Epoch [1/60], Step [100/636], Loss: 1.183, Training Accuracy: 48.37%\n",
      "Epoch [1/60], Step [200/636], Loss: 1.178, Training Accuracy: 54.35%\n",
      "Epoch [1/60], Step [300/636], Loss: 1.067, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [400/636], Loss: 0.882, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [500/636], Loss: 0.907, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [600/636], Loss: 0.916, Training Accuracy: 64.67%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 46.941435341909276 %\n",
      "Epoch [2/60], Step [100/636], Loss: 0.887, Training Accuracy: 62.50%\n",
      "Epoch [2/60], Step [200/636], Loss: 0.779, Training Accuracy: 66.85%\n",
      "Epoch [2/60], Step [300/636], Loss: 0.858, Training Accuracy: 69.57%\n",
      "Epoch [2/60], Step [400/636], Loss: 0.770, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [500/636], Loss: 0.671, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [600/636], Loss: 0.747, Training Accuracy: 72.83%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 53.68314150304671 %\n",
      "Epoch [3/60], Step [100/636], Loss: 0.623, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [200/636], Loss: 0.611, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [300/636], Loss: 0.775, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [400/636], Loss: 0.852, Training Accuracy: 67.39%\n",
      "Epoch [3/60], Step [500/636], Loss: 0.570, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [600/636], Loss: 0.466, Training Accuracy: 80.98%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 57.840216655382534 %\n",
      "Epoch [4/60], Step [100/636], Loss: 0.636, Training Accuracy: 76.63%\n",
      "Epoch [4/60], Step [200/636], Loss: 0.696, Training Accuracy: 76.09%\n",
      "Epoch [4/60], Step [300/636], Loss: 0.650, Training Accuracy: 70.65%\n",
      "Epoch [4/60], Step [400/636], Loss: 0.605, Training Accuracy: 75.54%\n",
      "Epoch [4/60], Step [500/636], Loss: 0.692, Training Accuracy: 69.57%\n",
      "Epoch [4/60], Step [600/636], Loss: 0.635, Training Accuracy: 73.91%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 55.8750846310088 %\n",
      "Epoch [5/60], Step [100/636], Loss: 0.676, Training Accuracy: 73.91%\n",
      "Epoch [5/60], Step [200/636], Loss: 0.855, Training Accuracy: 70.65%\n",
      "Epoch [5/60], Step [300/636], Loss: 0.527, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [400/636], Loss: 0.626, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [500/636], Loss: 0.444, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [600/636], Loss: 0.530, Training Accuracy: 74.46%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 61.15267433987813 %\n",
      "Epoch [6/60], Step [100/636], Loss: 0.535, Training Accuracy: 77.17%\n",
      "Epoch [6/60], Step [200/636], Loss: 0.575, Training Accuracy: 79.89%\n",
      "Epoch [6/60], Step [300/636], Loss: 0.550, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [400/636], Loss: 0.680, Training Accuracy: 76.63%\n",
      "Epoch [6/60], Step [500/636], Loss: 0.525, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [600/636], Loss: 0.493, Training Accuracy: 79.89%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 58.79993229519296 %\n",
      "Epoch [7/60], Step [100/636], Loss: 0.612, Training Accuracy: 73.91%\n",
      "Epoch [7/60], Step [200/636], Loss: 0.494, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [300/636], Loss: 0.533, Training Accuracy: 75.54%\n",
      "Epoch [7/60], Step [400/636], Loss: 0.466, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [500/636], Loss: 0.548, Training Accuracy: 78.80%\n",
      "Epoch [7/60], Step [600/636], Loss: 0.446, Training Accuracy: 80.98%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 57.523696682464454 %\n",
      "Epoch [8/60], Step [100/636], Loss: 0.451, Training Accuracy: 81.52%\n",
      "Epoch [8/60], Step [200/636], Loss: 0.506, Training Accuracy: 80.43%\n",
      "Epoch [8/60], Step [300/636], Loss: 0.479, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [400/636], Loss: 0.438, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [500/636], Loss: 0.444, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [600/636], Loss: 0.357, Training Accuracy: 85.87%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 61.05280974949221 %\n",
      "Epoch [9/60], Step [100/636], Loss: 0.417, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [200/636], Loss: 0.392, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [300/636], Loss: 0.387, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [400/636], Loss: 0.404, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [500/636], Loss: 0.391, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [600/636], Loss: 0.453, Training Accuracy: 85.87%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 61.2136086662153 %\n",
      "Epoch [10/60], Step [100/636], Loss: 0.420, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [200/636], Loss: 0.398, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [300/636], Loss: 0.332, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [400/636], Loss: 0.375, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [500/636], Loss: 0.441, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [600/636], Loss: 0.483, Training Accuracy: 82.61%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 61.71631685849695 %\n",
      "Epoch [11/60], Step [100/636], Loss: 0.507, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [200/636], Loss: 0.430, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [300/636], Loss: 0.435, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [400/636], Loss: 0.338, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [500/636], Loss: 0.475, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [600/636], Loss: 0.451, Training Accuracy: 85.33%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 61.294854434664856 %\n",
      "Epoch [12/60], Step [100/636], Loss: 0.456, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [200/636], Loss: 0.402, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [300/636], Loss: 0.349, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [400/636], Loss: 0.329, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [500/636], Loss: 0.524, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [600/636], Loss: 0.454, Training Accuracy: 84.24%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 62.45768449559919 %\n",
      "Epoch [13/60], Step [100/636], Loss: 0.330, Training Accuracy: 86.41%\n",
      "Epoch [13/60], Step [200/636], Loss: 0.375, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [300/636], Loss: 0.446, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [400/636], Loss: 0.317, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [500/636], Loss: 0.506, Training Accuracy: 80.98%\n",
      "Epoch [13/60], Step [600/636], Loss: 0.468, Training Accuracy: 89.13%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 62.015910629654705 %\n",
      "Epoch [14/60], Step [100/636], Loss: 0.361, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [200/636], Loss: 0.501, Training Accuracy: 84.24%\n",
      "Epoch [14/60], Step [300/636], Loss: 0.301, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [400/636], Loss: 0.275, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [500/636], Loss: 0.340, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [600/636], Loss: 0.340, Training Accuracy: 86.96%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 59.20446851726473 %\n",
      "Epoch [15/60], Step [100/636], Loss: 0.382, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [200/636], Loss: 0.373, Training Accuracy: 80.43%\n",
      "Epoch [15/60], Step [300/636], Loss: 0.385, Training Accuracy: 84.78%\n",
      "Epoch [15/60], Step [400/636], Loss: 0.386, Training Accuracy: 82.07%\n",
      "Epoch [15/60], Step [500/636], Loss: 0.384, Training Accuracy: 84.24%\n",
      "Epoch [15/60], Step [600/636], Loss: 0.458, Training Accuracy: 84.78%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 59.79011509817197 %\n",
      "Epoch [16/60], Step [100/636], Loss: 0.360, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [200/636], Loss: 0.358, Training Accuracy: 84.78%\n",
      "Epoch [16/60], Step [300/636], Loss: 0.300, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [400/636], Loss: 0.347, Training Accuracy: 85.33%\n",
      "Epoch [16/60], Step [500/636], Loss: 0.300, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [600/636], Loss: 0.399, Training Accuracy: 82.61%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 62.31211916046039 %\n",
      "Epoch [17/60], Step [100/636], Loss: 0.274, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [200/636], Loss: 0.441, Training Accuracy: 85.87%\n",
      "Epoch [17/60], Step [300/636], Loss: 0.458, Training Accuracy: 89.13%\n",
      "Epoch [17/60], Step [400/636], Loss: 0.253, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [500/636], Loss: 0.429, Training Accuracy: 84.24%\n",
      "Epoch [17/60], Step [600/636], Loss: 0.347, Training Accuracy: 86.41%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 61.277928232904536 %\n",
      "Finished early after 17 epochs!\n",
      "Final Training Accuracy 87.56461164550029 %\n",
      "Final Test Accuracy 61.28808395396072 %\n",
      "Epoch [1/60], Step [100/632], Loss: 1.316, Training Accuracy: 50.00%\n",
      "Epoch [1/60], Step [200/632], Loss: 1.195, Training Accuracy: 57.07%\n",
      "Epoch [1/60], Step [300/632], Loss: 1.064, Training Accuracy: 63.04%\n",
      "Epoch [1/60], Step [400/632], Loss: 1.122, Training Accuracy: 55.98%\n",
      "Epoch [1/60], Step [500/632], Loss: 1.093, Training Accuracy: 58.70%\n",
      "Epoch [1/60], Step [600/632], Loss: 1.039, Training Accuracy: 69.57%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 38.38187485336998 %\n",
      "Epoch [2/60], Step [100/632], Loss: 0.888, Training Accuracy: 70.11%\n",
      "Epoch [2/60], Step [200/632], Loss: 0.931, Training Accuracy: 69.02%\n",
      "Epoch [2/60], Step [300/632], Loss: 0.900, Training Accuracy: 68.48%\n",
      "Epoch [2/60], Step [400/632], Loss: 0.850, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [500/632], Loss: 0.897, Training Accuracy: 72.83%\n",
      "Epoch [2/60], Step [600/632], Loss: 0.961, Training Accuracy: 63.04%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 40.50340181653652 %\n",
      "Epoch [3/60], Step [100/632], Loss: 0.839, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [200/632], Loss: 0.844, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [300/632], Loss: 0.718, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [400/632], Loss: 0.756, Training Accuracy: 75.54%\n",
      "Epoch [3/60], Step [500/632], Loss: 0.810, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [600/632], Loss: 0.635, Training Accuracy: 75.54%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 42.236149746958475 %\n",
      "Epoch [4/60], Step [100/632], Loss: 0.815, Training Accuracy: 71.20%\n",
      "Epoch [4/60], Step [200/632], Loss: 0.723, Training Accuracy: 74.46%\n",
      "Epoch [4/60], Step [300/632], Loss: 0.732, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [400/632], Loss: 0.786, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [500/632], Loss: 0.710, Training Accuracy: 75.54%\n",
      "Epoch [4/60], Step [600/632], Loss: 0.736, Training Accuracy: 75.00%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 42.74726011328217 %\n",
      "Epoch [5/60], Step [100/632], Loss: 0.700, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [200/632], Loss: 0.687, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [300/632], Loss: 0.782, Training Accuracy: 73.37%\n",
      "Epoch [5/60], Step [400/632], Loss: 0.675, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [500/632], Loss: 0.677, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [600/632], Loss: 0.619, Training Accuracy: 79.89%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 42.91818882595435 %\n",
      "Epoch [6/60], Step [100/632], Loss: 0.684, Training Accuracy: 77.72%\n",
      "Epoch [6/60], Step [200/632], Loss: 0.599, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [300/632], Loss: 0.605, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [400/632], Loss: 0.632, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [500/632], Loss: 0.570, Training Accuracy: 76.63%\n",
      "Epoch [6/60], Step [600/632], Loss: 0.675, Training Accuracy: 78.80%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 43.51811509199987 %\n",
      "Epoch [7/60], Step [100/632], Loss: 0.699, Training Accuracy: 78.80%\n",
      "Epoch [7/60], Step [200/632], Loss: 0.577, Training Accuracy: 79.35%\n",
      "Epoch [7/60], Step [300/632], Loss: 0.583, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [400/632], Loss: 0.636, Training Accuracy: 76.09%\n",
      "Epoch [7/60], Step [500/632], Loss: 0.673, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [600/632], Loss: 0.479, Training Accuracy: 89.67%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 43.49633005999263 %\n",
      "Epoch [8/60], Step [100/632], Loss: 0.633, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [200/632], Loss: 0.499, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [300/632], Loss: 0.623, Training Accuracy: 78.80%\n",
      "Epoch [8/60], Step [400/632], Loss: 0.589, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [500/632], Loss: 0.476, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [600/632], Loss: 0.695, Training Accuracy: 79.35%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 44.8738143915273 %\n",
      "Epoch [9/60], Step [100/632], Loss: 0.612, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [200/632], Loss: 0.440, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [300/632], Loss: 0.555, Training Accuracy: 78.80%\n",
      "Epoch [9/60], Step [400/632], Loss: 0.616, Training Accuracy: 80.98%\n",
      "Epoch [9/60], Step [500/632], Loss: 0.685, Training Accuracy: 79.89%\n",
      "Epoch [9/60], Step [600/632], Loss: 0.568, Training Accuracy: 79.89%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 47.34892918188826 %\n",
      "Epoch [10/60], Step [100/632], Loss: 0.484, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [200/632], Loss: 0.516, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [300/632], Loss: 0.550, Training Accuracy: 81.52%\n",
      "Epoch [10/60], Step [400/632], Loss: 0.529, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [500/632], Loss: 0.488, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [600/632], Loss: 0.539, Training Accuracy: 85.87%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 41.75017595602775 %\n",
      "Epoch [11/60], Step [100/632], Loss: 0.382, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [200/632], Loss: 0.496, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [300/632], Loss: 0.543, Training Accuracy: 81.52%\n",
      "Epoch [11/60], Step [400/632], Loss: 0.389, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [500/632], Loss: 0.578, Training Accuracy: 82.61%\n",
      "Epoch [11/60], Step [600/632], Loss: 0.474, Training Accuracy: 86.41%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 45.219023360257395 %\n",
      "Epoch [12/60], Step [100/632], Loss: 0.583, Training Accuracy: 78.80%\n",
      "Epoch [12/60], Step [200/632], Loss: 0.554, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [300/632], Loss: 0.372, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [400/632], Loss: 0.503, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [500/632], Loss: 0.503, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [600/632], Loss: 0.489, Training Accuracy: 90.22%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 41.931159298857125 %\n",
      "Epoch [13/60], Step [100/632], Loss: 0.423, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [200/632], Loss: 0.718, Training Accuracy: 80.98%\n",
      "Epoch [13/60], Step [300/632], Loss: 0.464, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [400/632], Loss: 0.428, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [500/632], Loss: 0.421, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [600/632], Loss: 0.550, Training Accuracy: 81.52%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 43.80802359486544 %\n",
      "Epoch [14/60], Step [100/632], Loss: 0.645, Training Accuracy: 78.80%\n",
      "Epoch [14/60], Step [200/632], Loss: 0.537, Training Accuracy: 83.15%\n",
      "Epoch [14/60], Step [300/632], Loss: 0.481, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [400/632], Loss: 0.460, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [500/632], Loss: 0.473, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [600/632], Loss: 0.542, Training Accuracy: 83.15%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 44.38951637228944 %\n",
      "Finished early after 14 epochs!\n",
      "Final Training Accuracy 85.03242787593112 %\n",
      "Final Test Accuracy 44.26886081040319 %\n",
      "Epoch [1/60], Step [100/646], Loss: 1.263, Training Accuracy: 36.41%\n",
      "Epoch [1/60], Step [200/646], Loss: 1.129, Training Accuracy: 53.80%\n",
      "Epoch [1/60], Step [300/646], Loss: 1.115, Training Accuracy: 52.72%\n",
      "Epoch [1/60], Step [400/646], Loss: 0.991, Training Accuracy: 61.41%\n",
      "Epoch [1/60], Step [500/646], Loss: 0.961, Training Accuracy: 62.50%\n",
      "Epoch [1/60], Step [600/646], Loss: 1.007, Training Accuracy: 59.24%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 56.10024834726644 %\n",
      "Epoch [2/60], Step [100/646], Loss: 0.860, Training Accuracy: 69.02%\n",
      "Epoch [2/60], Step [200/646], Loss: 0.913, Training Accuracy: 62.50%\n",
      "Epoch [2/60], Step [300/646], Loss: 0.961, Training Accuracy: 63.59%\n",
      "Epoch [2/60], Step [400/646], Loss: 0.800, Training Accuracy: 64.67%\n",
      "Epoch [2/60], Step [500/646], Loss: 0.870, Training Accuracy: 65.22%\n",
      "Epoch [2/60], Step [600/646], Loss: 0.790, Training Accuracy: 67.39%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 56.97820840183287 %\n",
      "Epoch [3/60], Step [100/646], Loss: 0.837, Training Accuracy: 67.93%\n",
      "Epoch [3/60], Step [200/646], Loss: 0.841, Training Accuracy: 71.74%\n",
      "Epoch [3/60], Step [300/646], Loss: 0.672, Training Accuracy: 70.11%\n",
      "Epoch [3/60], Step [400/646], Loss: 0.660, Training Accuracy: 71.20%\n",
      "Epoch [3/60], Step [500/646], Loss: 0.799, Training Accuracy: 66.30%\n",
      "Epoch [3/60], Step [600/646], Loss: 0.732, Training Accuracy: 77.72%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 59.876176151666726 %\n",
      "Epoch [4/60], Step [100/646], Loss: 0.786, Training Accuracy: 71.74%\n",
      "Epoch [4/60], Step [200/646], Loss: 0.832, Training Accuracy: 66.85%\n",
      "Epoch [4/60], Step [300/646], Loss: 0.705, Training Accuracy: 72.83%\n",
      "Epoch [4/60], Step [400/646], Loss: 0.658, Training Accuracy: 70.65%\n",
      "Epoch [4/60], Step [500/646], Loss: 0.760, Training Accuracy: 71.20%\n",
      "Epoch [4/60], Step [600/646], Loss: 0.648, Training Accuracy: 74.46%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 61.609360243450276 %\n",
      "Epoch [5/60], Step [100/646], Loss: 0.745, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [200/646], Loss: 0.618, Training Accuracy: 73.91%\n",
      "Epoch [5/60], Step [300/646], Loss: 0.686, Training Accuracy: 74.46%\n",
      "Epoch [5/60], Step [400/646], Loss: 0.704, Training Accuracy: 76.09%\n",
      "Epoch [5/60], Step [500/646], Loss: 0.609, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [600/646], Loss: 0.680, Training Accuracy: 77.17%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 60.95701143796565 %\n",
      "Epoch [6/60], Step [100/646], Loss: 0.605, Training Accuracy: 79.35%\n",
      "Epoch [6/60], Step [200/646], Loss: 0.544, Training Accuracy: 76.63%\n",
      "Epoch [6/60], Step [300/646], Loss: 0.547, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [400/646], Loss: 0.538, Training Accuracy: 77.17%\n",
      "Epoch [6/60], Step [500/646], Loss: 0.571, Training Accuracy: 76.09%\n",
      "Epoch [6/60], Step [600/646], Loss: 0.610, Training Accuracy: 76.63%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 62.56252404771065 %\n",
      "Epoch [7/60], Step [100/646], Loss: 0.718, Training Accuracy: 74.46%\n",
      "Epoch [7/60], Step [200/646], Loss: 0.630, Training Accuracy: 71.74%\n",
      "Epoch [7/60], Step [300/646], Loss: 0.488, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [400/646], Loss: 0.666, Training Accuracy: 78.26%\n",
      "Epoch [7/60], Step [500/646], Loss: 0.668, Training Accuracy: 76.09%\n",
      "Epoch [7/60], Step [600/646], Loss: 0.560, Training Accuracy: 77.17%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 64.80114729441394 %\n",
      "Epoch [8/60], Step [100/646], Loss: 0.507, Training Accuracy: 72.83%\n",
      "Epoch [8/60], Step [200/646], Loss: 0.544, Training Accuracy: 75.54%\n",
      "Epoch [8/60], Step [300/646], Loss: 0.410, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [400/646], Loss: 0.457, Training Accuracy: 79.89%\n",
      "Epoch [8/60], Step [500/646], Loss: 0.479, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [600/646], Loss: 0.493, Training Accuracy: 82.61%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 62.11829724719298 %\n",
      "Epoch [9/60], Step [100/646], Loss: 0.467, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [200/646], Loss: 0.492, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [300/646], Loss: 0.518, Training Accuracy: 78.80%\n",
      "Epoch [9/60], Step [400/646], Loss: 0.528, Training Accuracy: 76.63%\n",
      "Epoch [9/60], Step [500/646], Loss: 0.516, Training Accuracy: 76.09%\n",
      "Epoch [9/60], Step [600/646], Loss: 0.588, Training Accuracy: 79.35%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 64.32194200566651 %\n",
      "Epoch [10/60], Step [100/646], Loss: 0.518, Training Accuracy: 79.35%\n",
      "Epoch [10/60], Step [200/646], Loss: 0.604, Training Accuracy: 78.26%\n",
      "Epoch [10/60], Step [300/646], Loss: 0.477, Training Accuracy: 81.52%\n",
      "Epoch [10/60], Step [400/646], Loss: 0.642, Training Accuracy: 80.98%\n",
      "Epoch [10/60], Step [500/646], Loss: 0.421, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [600/646], Loss: 0.416, Training Accuracy: 84.78%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 64.60701668473888 %\n",
      "Epoch [11/60], Step [100/646], Loss: 0.377, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [200/646], Loss: 0.501, Training Accuracy: 78.80%\n",
      "Epoch [11/60], Step [300/646], Loss: 0.352, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [400/646], Loss: 0.556, Training Accuracy: 77.72%\n",
      "Epoch [11/60], Step [500/646], Loss: 0.534, Training Accuracy: 76.09%\n",
      "Epoch [11/60], Step [600/646], Loss: 0.537, Training Accuracy: 80.98%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 63.06796320263038 %\n",
      "Epoch [12/60], Step [100/646], Loss: 0.413, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [200/646], Loss: 0.545, Training Accuracy: 73.37%\n",
      "Epoch [12/60], Step [300/646], Loss: 0.471, Training Accuracy: 80.98%\n",
      "Epoch [12/60], Step [400/646], Loss: 0.469, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [500/646], Loss: 0.426, Training Accuracy: 83.70%\n",
      "Epoch [12/60], Step [600/646], Loss: 0.426, Training Accuracy: 82.07%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 65.1142047640701 %\n",
      "Epoch [13/60], Step [100/646], Loss: 0.540, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [200/646], Loss: 0.458, Training Accuracy: 83.70%\n",
      "Epoch [13/60], Step [300/646], Loss: 0.516, Training Accuracy: 80.43%\n",
      "Epoch [13/60], Step [400/646], Loss: 0.415, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [500/646], Loss: 0.453, Training Accuracy: 79.35%\n",
      "Epoch [13/60], Step [600/646], Loss: 0.509, Training Accuracy: 82.61%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 65.65287348280808 %\n",
      "Epoch [14/60], Step [100/646], Loss: 0.390, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [200/646], Loss: 0.387, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [300/646], Loss: 0.331, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [400/646], Loss: 0.361, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [500/646], Loss: 0.369, Training Accuracy: 84.78%\n",
      "Epoch [14/60], Step [600/646], Loss: 0.423, Training Accuracy: 83.15%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 65.82776592395678 %\n",
      "Epoch [15/60], Step [100/646], Loss: 0.401, Training Accuracy: 83.15%\n",
      "Epoch [15/60], Step [200/646], Loss: 0.421, Training Accuracy: 80.43%\n",
      "Epoch [15/60], Step [300/646], Loss: 0.510, Training Accuracy: 76.63%\n",
      "Epoch [15/60], Step [400/646], Loss: 0.425, Training Accuracy: 83.15%\n",
      "Epoch [15/60], Step [500/646], Loss: 0.377, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [600/646], Loss: 0.478, Training Accuracy: 76.63%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 65.1666724964147 %\n",
      "Epoch [16/60], Step [100/646], Loss: 0.424, Training Accuracy: 79.89%\n",
      "Epoch [16/60], Step [200/646], Loss: 0.383, Training Accuracy: 84.24%\n",
      "Epoch [16/60], Step [300/646], Loss: 0.464, Training Accuracy: 83.70%\n",
      "Epoch [16/60], Step [400/646], Loss: 0.520, Training Accuracy: 83.15%\n",
      "Epoch [16/60], Step [500/646], Loss: 0.366, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [600/646], Loss: 0.413, Training Accuracy: 82.61%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 65.37654342579313 %\n",
      "Epoch [17/60], Step [100/646], Loss: 0.480, Training Accuracy: 75.54%\n",
      "Epoch [17/60], Step [200/646], Loss: 0.449, Training Accuracy: 81.52%\n",
      "Epoch [17/60], Step [300/646], Loss: 0.377, Training Accuracy: 83.70%\n",
      "Epoch [17/60], Step [400/646], Loss: 0.466, Training Accuracy: 83.15%\n",
      "Epoch [17/60], Step [500/646], Loss: 0.456, Training Accuracy: 85.33%\n",
      "Epoch [17/60], Step [600/646], Loss: 0.491, Training Accuracy: 81.52%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 66.97680926230369 %\n",
      "Epoch [18/60], Step [100/646], Loss: 0.372, Training Accuracy: 87.50%\n",
      "Epoch [18/60], Step [200/646], Loss: 0.336, Training Accuracy: 90.76%\n",
      "Epoch [18/60], Step [300/646], Loss: 0.417, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [400/646], Loss: 0.510, Training Accuracy: 83.70%\n",
      "Epoch [18/60], Step [500/646], Loss: 0.455, Training Accuracy: 84.24%\n",
      "Epoch [18/60], Step [600/646], Loss: 0.382, Training Accuracy: 82.61%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 66.4521319388576 %\n",
      "Epoch [19/60], Step [100/646], Loss: 0.356, Training Accuracy: 82.07%\n",
      "Epoch [19/60], Step [200/646], Loss: 0.342, Training Accuracy: 85.33%\n",
      "Epoch [19/60], Step [300/646], Loss: 0.475, Training Accuracy: 83.70%\n",
      "Epoch [19/60], Step [400/646], Loss: 0.399, Training Accuracy: 85.33%\n",
      "Epoch [19/60], Step [500/646], Loss: 0.371, Training Accuracy: 85.33%\n",
      "Epoch [19/60], Step [600/646], Loss: 0.435, Training Accuracy: 80.43%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 65.95893525481829 %\n",
      "Epoch [20/60], Step [100/646], Loss: 0.388, Training Accuracy: 83.70%\n",
      "Epoch [20/60], Step [200/646], Loss: 0.376, Training Accuracy: 88.59%\n",
      "Epoch [20/60], Step [300/646], Loss: 0.277, Training Accuracy: 91.85%\n",
      "Epoch [20/60], Step [400/646], Loss: 0.242, Training Accuracy: 90.22%\n",
      "Epoch [20/60], Step [500/646], Loss: 0.452, Training Accuracy: 85.33%\n",
      "Epoch [20/60], Step [600/646], Loss: 0.466, Training Accuracy: 80.98%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 68.16957571093776 %\n",
      "Epoch [21/60], Step [100/646], Loss: 0.290, Training Accuracy: 90.22%\n",
      "Epoch [21/60], Step [200/646], Loss: 0.343, Training Accuracy: 85.33%\n",
      "Epoch [21/60], Step [300/646], Loss: 0.343, Training Accuracy: 84.24%\n",
      "Epoch [21/60], Step [400/646], Loss: 0.319, Training Accuracy: 87.50%\n",
      "Epoch [21/60], Step [500/646], Loss: 0.364, Training Accuracy: 88.59%\n",
      "Epoch [21/60], Step [600/646], Loss: 0.335, Training Accuracy: 87.50%\n",
      "Testing epoch 21\n",
      "Epoch 21: Test Accuracy 65.69484766868376 %\n",
      "Epoch [22/60], Step [100/646], Loss: 0.529, Training Accuracy: 78.26%\n",
      "Epoch [22/60], Step [200/646], Loss: 0.323, Training Accuracy: 83.70%\n",
      "Epoch [22/60], Step [300/646], Loss: 0.291, Training Accuracy: 85.87%\n",
      "Epoch [22/60], Step [400/646], Loss: 0.306, Training Accuracy: 88.59%\n",
      "Epoch [22/60], Step [500/646], Loss: 0.519, Training Accuracy: 79.89%\n",
      "Epoch [22/60], Step [600/646], Loss: 0.441, Training Accuracy: 86.96%\n",
      "Testing epoch 22\n",
      "Epoch 22: Test Accuracy 66.93658400083949 %\n",
      "Epoch [23/60], Step [100/646], Loss: 0.295, Training Accuracy: 88.59%\n",
      "Epoch [23/60], Step [200/646], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [23/60], Step [300/646], Loss: 0.367, Training Accuracy: 89.13%\n",
      "Epoch [23/60], Step [400/646], Loss: 0.435, Training Accuracy: 84.24%\n",
      "Epoch [23/60], Step [500/646], Loss: 0.350, Training Accuracy: 88.59%\n",
      "Epoch [23/60], Step [600/646], Loss: 0.400, Training Accuracy: 85.33%\n",
      "Testing epoch 23\n",
      "Epoch 23: Test Accuracy 67.98768757214313 %\n",
      "Epoch [24/60], Step [100/646], Loss: 0.405, Training Accuracy: 82.61%\n",
      "Epoch [24/60], Step [200/646], Loss: 0.333, Training Accuracy: 86.96%\n",
      "Epoch [24/60], Step [300/646], Loss: 0.480, Training Accuracy: 81.52%\n",
      "Epoch [24/60], Step [400/646], Loss: 0.402, Training Accuracy: 83.70%\n",
      "Epoch [24/60], Step [500/646], Loss: 0.415, Training Accuracy: 86.41%\n",
      "Epoch [24/60], Step [600/646], Loss: 0.376, Training Accuracy: 83.70%\n",
      "Testing epoch 24\n",
      "Epoch 24: Test Accuracy 67.96844940361677 %\n",
      "Epoch [25/60], Step [100/646], Loss: 0.243, Training Accuracy: 89.67%\n",
      "Epoch [25/60], Step [200/646], Loss: 0.412, Training Accuracy: 88.04%\n",
      "Epoch [25/60], Step [300/646], Loss: 0.388, Training Accuracy: 89.13%\n",
      "Epoch [25/60], Step [400/646], Loss: 0.308, Training Accuracy: 86.41%\n",
      "Epoch [25/60], Step [500/646], Loss: 0.381, Training Accuracy: 85.33%\n",
      "Epoch [25/60], Step [600/646], Loss: 0.296, Training Accuracy: 91.85%\n",
      "Testing epoch 25\n",
      "Epoch 25: Test Accuracy 67.22515652873483 %\n",
      "Finished early after 25 epochs!\n",
      "Final Training Accuracy 84.81566936692658 %\n",
      "Final Test Accuracy 67.21990975550037 %\n",
      "Epoch [1/60], Step [100/587], Loss: 1.165, Training Accuracy: 52.17%\n",
      "Epoch [1/60], Step [200/587], Loss: 0.887, Training Accuracy: 55.43%\n",
      "Epoch [1/60], Step [300/587], Loss: 1.077, Training Accuracy: 64.13%\n",
      "Epoch [1/60], Step [400/587], Loss: 0.917, Training Accuracy: 55.98%\n",
      "Epoch [1/60], Step [500/587], Loss: 0.874, Training Accuracy: 76.09%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 36.96924253116914 %\n",
      "Epoch [2/60], Step [100/587], Loss: 0.825, Training Accuracy: 64.67%\n",
      "Epoch [2/60], Step [200/587], Loss: 0.637, Training Accuracy: 68.48%\n",
      "Epoch [2/60], Step [300/587], Loss: 0.835, Training Accuracy: 70.65%\n",
      "Epoch [2/60], Step [400/587], Loss: 0.754, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [500/587], Loss: 0.729, Training Accuracy: 75.00%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 43.28687367678194 %\n",
      "Epoch [3/60], Step [100/587], Loss: 0.374, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [200/587], Loss: 0.525, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [300/587], Loss: 0.784, Training Accuracy: 69.57%\n",
      "Epoch [3/60], Step [400/587], Loss: 0.624, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [500/587], Loss: 0.557, Training Accuracy: 78.80%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 42.59880028228652 %\n",
      "Epoch [4/60], Step [100/587], Loss: 0.493, Training Accuracy: 76.09%\n",
      "Epoch [4/60], Step [200/587], Loss: 0.503, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [300/587], Loss: 0.416, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/587], Loss: 0.450, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [500/587], Loss: 0.700, Training Accuracy: 78.26%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 43.41184427193602 %\n",
      "Epoch [5/60], Step [100/587], Loss: 0.506, Training Accuracy: 76.09%\n",
      "Epoch [5/60], Step [200/587], Loss: 0.541, Training Accuracy: 72.28%\n",
      "Epoch [5/60], Step [300/587], Loss: 0.500, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [400/587], Loss: 0.427, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [500/587], Loss: 0.416, Training Accuracy: 82.61%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 44.84533051046812 %\n",
      "Epoch [6/60], Step [100/587], Loss: 0.562, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [200/587], Loss: 0.536, Training Accuracy: 76.09%\n",
      "Epoch [6/60], Step [300/587], Loss: 0.481, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [400/587], Loss: 0.399, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [500/587], Loss: 0.397, Training Accuracy: 83.15%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 42.77522935779817 %\n",
      "Epoch [7/60], Step [100/587], Loss: 0.452, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [200/587], Loss: 0.466, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [300/587], Loss: 0.448, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [400/587], Loss: 0.392, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [500/587], Loss: 0.349, Training Accuracy: 88.04%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 44.117560573982594 %\n",
      "Epoch [8/60], Step [100/587], Loss: 0.396, Training Accuracy: 79.35%\n",
      "Epoch [8/60], Step [200/587], Loss: 0.363, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [300/587], Loss: 0.354, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [400/587], Loss: 0.520, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [500/587], Loss: 0.432, Training Accuracy: 83.70%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 46.06563161609033 %\n",
      "Epoch [9/60], Step [100/587], Loss: 0.466, Training Accuracy: 80.98%\n",
      "Epoch [9/60], Step [200/587], Loss: 0.412, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [300/587], Loss: 0.365, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [400/587], Loss: 0.316, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [500/587], Loss: 0.338, Training Accuracy: 82.07%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 44.96147965184662 %\n",
      "Epoch [10/60], Step [100/587], Loss: 0.476, Training Accuracy: 82.07%\n",
      "Epoch [10/60], Step [200/587], Loss: 0.402, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [300/587], Loss: 0.270, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [400/587], Loss: 0.328, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [500/587], Loss: 0.343, Training Accuracy: 83.70%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 45.8994942366502 %\n",
      "Epoch [11/60], Step [100/587], Loss: 0.298, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [200/587], Loss: 0.273, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [300/587], Loss: 0.466, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [400/587], Loss: 0.213, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [500/587], Loss: 0.281, Training Accuracy: 85.87%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 47.10656316160903 %\n",
      "Epoch [12/60], Step [100/587], Loss: 0.430, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [200/587], Loss: 0.368, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [300/587], Loss: 0.386, Training Accuracy: 84.78%\n",
      "Epoch [12/60], Step [400/587], Loss: 0.278, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [500/587], Loss: 0.232, Training Accuracy: 91.30%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 45.886262055986826 %\n",
      "Epoch [13/60], Step [100/587], Loss: 0.392, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [200/587], Loss: 0.284, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [300/587], Loss: 0.445, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [400/587], Loss: 0.338, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [500/587], Loss: 0.283, Training Accuracy: 89.67%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 45.479004940014114 %\n",
      "Epoch [14/60], Step [100/587], Loss: 0.288, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [200/587], Loss: 0.418, Training Accuracy: 79.35%\n",
      "Epoch [14/60], Step [300/587], Loss: 0.353, Training Accuracy: 84.78%\n",
      "Epoch [14/60], Step [400/587], Loss: 0.218, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [500/587], Loss: 0.300, Training Accuracy: 88.04%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 47.141848976711366 %\n",
      "Epoch [15/60], Step [100/587], Loss: 0.223, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [200/587], Loss: 0.248, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [300/587], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [400/587], Loss: 0.292, Training Accuracy: 88.04%\n",
      "Epoch [15/60], Step [500/587], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 45.74658903787344 %\n",
      "Epoch [16/60], Step [100/587], Loss: 0.232, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [200/587], Loss: 0.296, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [300/587], Loss: 0.323, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [400/587], Loss: 0.230, Training Accuracy: 91.85%\n",
      "Epoch [16/60], Step [500/587], Loss: 0.222, Training Accuracy: 91.30%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 46.63608562691132 %\n",
      "Epoch [17/60], Step [100/587], Loss: 0.277, Training Accuracy: 89.67%\n",
      "Epoch [17/60], Step [200/587], Loss: 0.335, Training Accuracy: 85.87%\n",
      "Epoch [17/60], Step [300/587], Loss: 0.381, Training Accuracy: 85.33%\n",
      "Epoch [17/60], Step [400/587], Loss: 0.313, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [500/587], Loss: 0.211, Training Accuracy: 91.30%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 45.79216654904728 %\n",
      "Epoch [18/60], Step [100/587], Loss: 0.232, Training Accuracy: 89.67%\n",
      "Epoch [18/60], Step [200/587], Loss: 0.295, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [300/587], Loss: 0.303, Training Accuracy: 87.50%\n",
      "Epoch [18/60], Step [400/587], Loss: 0.327, Training Accuracy: 88.04%\n",
      "Epoch [18/60], Step [500/587], Loss: 0.357, Training Accuracy: 87.50%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 46.68460362267702 %\n",
      "Epoch [19/60], Step [100/587], Loss: 0.233, Training Accuracy: 86.41%\n",
      "Epoch [19/60], Step [200/587], Loss: 0.249, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [300/587], Loss: 0.234, Training Accuracy: 90.76%\n",
      "Epoch [19/60], Step [400/587], Loss: 0.368, Training Accuracy: 84.24%\n",
      "Epoch [19/60], Step [500/587], Loss: 0.318, Training Accuracy: 88.59%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 46.53757939308398 %\n",
      "Finished early after 19 epochs!\n",
      "Final Training Accuracy 89.42418177100708 %\n",
      "Final Test Accuracy 46.54051987767584 %\n",
      "Epoch [1/60], Step [100/615], Loss: 1.338, Training Accuracy: 49.46%\n",
      "Epoch [1/60], Step [200/615], Loss: 1.086, Training Accuracy: 60.33%\n",
      "Epoch [1/60], Step [300/615], Loss: 1.075, Training Accuracy: 60.87%\n",
      "Epoch [1/60], Step [400/615], Loss: 0.925, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [500/615], Loss: 0.980, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [600/615], Loss: 0.905, Training Accuracy: 66.30%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 54.85538833561361 %\n",
      "Epoch [2/60], Step [100/615], Loss: 0.901, Training Accuracy: 66.85%\n",
      "Epoch [2/60], Step [200/615], Loss: 0.782, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [300/615], Loss: 0.800, Training Accuracy: 72.28%\n",
      "Epoch [2/60], Step [400/615], Loss: 0.772, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [500/615], Loss: 0.813, Training Accuracy: 71.20%\n",
      "Epoch [2/60], Step [600/615], Loss: 0.839, Training Accuracy: 67.93%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 55.15447516624773 %\n",
      "Epoch [3/60], Step [100/615], Loss: 0.761, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [200/615], Loss: 0.813, Training Accuracy: 70.11%\n",
      "Epoch [3/60], Step [300/615], Loss: 0.785, Training Accuracy: 71.20%\n",
      "Epoch [3/60], Step [400/615], Loss: 0.701, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [500/615], Loss: 0.683, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [600/615], Loss: 0.780, Training Accuracy: 77.17%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 56.36832225015114 %\n",
      "Epoch [4/60], Step [100/615], Loss: 0.669, Training Accuracy: 76.09%\n",
      "Epoch [4/60], Step [200/615], Loss: 0.760, Training Accuracy: 72.28%\n",
      "Epoch [4/60], Step [300/615], Loss: 0.644, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [400/615], Loss: 0.611, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [500/615], Loss: 0.688, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [600/615], Loss: 0.694, Training Accuracy: 72.28%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 53.194501893156 %\n",
      "Epoch [5/60], Step [100/615], Loss: 0.650, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [200/615], Loss: 0.579, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [300/615], Loss: 0.767, Training Accuracy: 69.02%\n",
      "Epoch [5/60], Step [400/615], Loss: 0.474, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [500/615], Loss: 0.586, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [600/615], Loss: 0.533, Training Accuracy: 82.07%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 56.400139998090935 %\n",
      "Epoch [6/60], Step [100/615], Loss: 0.661, Training Accuracy: 76.63%\n",
      "Epoch [6/60], Step [200/615], Loss: 0.611, Training Accuracy: 71.74%\n",
      "Epoch [6/60], Step [300/615], Loss: 0.741, Training Accuracy: 70.65%\n",
      "Epoch [6/60], Step [400/615], Loss: 0.541, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [500/615], Loss: 0.562, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [600/615], Loss: 0.517, Training Accuracy: 80.43%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 57.47717076585319 %\n",
      "Epoch [7/60], Step [100/615], Loss: 0.596, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [200/615], Loss: 0.577, Training Accuracy: 77.72%\n",
      "Epoch [7/60], Step [300/615], Loss: 0.492, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [400/615], Loss: 0.514, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [500/615], Loss: 0.508, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [600/615], Loss: 0.567, Training Accuracy: 79.35%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 61.40188997422762 %\n",
      "Epoch [8/60], Step [100/615], Loss: 0.599, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [200/615], Loss: 0.691, Training Accuracy: 79.89%\n",
      "Epoch [8/60], Step [300/615], Loss: 0.589, Training Accuracy: 78.80%\n",
      "Epoch [8/60], Step [400/615], Loss: 0.542, Training Accuracy: 81.52%\n",
      "Epoch [8/60], Step [500/615], Loss: 0.517, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [600/615], Loss: 0.496, Training Accuracy: 83.70%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 60.75439880365268 %\n",
      "Epoch [9/60], Step [100/615], Loss: 0.537, Training Accuracy: 77.72%\n",
      "Epoch [9/60], Step [200/615], Loss: 0.424, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [300/615], Loss: 0.566, Training Accuracy: 77.72%\n",
      "Epoch [9/60], Step [400/615], Loss: 0.495, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [500/615], Loss: 0.482, Training Accuracy: 80.98%\n",
      "Epoch [9/60], Step [600/615], Loss: 0.585, Training Accuracy: 79.35%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 56.920360176906684 %\n",
      "Epoch [10/60], Step [100/615], Loss: 0.498, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [200/615], Loss: 0.394, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [300/615], Loss: 0.514, Training Accuracy: 76.63%\n",
      "Epoch [10/60], Step [400/615], Loss: 0.484, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [500/615], Loss: 0.528, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [600/615], Loss: 0.499, Training Accuracy: 80.43%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 59.01555887874256 %\n",
      "Epoch [11/60], Step [100/615], Loss: 0.485, Training Accuracy: 82.07%\n",
      "Epoch [11/60], Step [200/615], Loss: 0.436, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [300/615], Loss: 0.470, Training Accuracy: 83.15%\n",
      "Epoch [11/60], Step [400/615], Loss: 0.476, Training Accuracy: 81.52%\n",
      "Epoch [11/60], Step [500/615], Loss: 0.474, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [600/615], Loss: 0.379, Training Accuracy: 85.87%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 59.49123421044259 %\n",
      "Epoch [12/60], Step [100/615], Loss: 0.491, Training Accuracy: 81.52%\n",
      "Epoch [12/60], Step [200/615], Loss: 0.582, Training Accuracy: 81.52%\n",
      "Epoch [12/60], Step [300/615], Loss: 0.486, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [400/615], Loss: 0.556, Training Accuracy: 80.98%\n",
      "Epoch [12/60], Step [500/615], Loss: 0.586, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [600/615], Loss: 0.436, Training Accuracy: 85.33%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 62.17665213656177 %\n",
      "Epoch [13/60], Step [100/615], Loss: 0.437, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [200/615], Loss: 0.523, Training Accuracy: 82.61%\n",
      "Epoch [13/60], Step [300/615], Loss: 0.428, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [400/615], Loss: 0.420, Training Accuracy: 83.70%\n",
      "Epoch [13/60], Step [500/615], Loss: 0.486, Training Accuracy: 79.89%\n",
      "Epoch [13/60], Step [600/615], Loss: 0.436, Training Accuracy: 85.87%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 56.34445893919629 %\n",
      "Epoch [14/60], Step [100/615], Loss: 0.494, Training Accuracy: 81.52%\n",
      "Epoch [14/60], Step [200/615], Loss: 0.389, Training Accuracy: 85.87%\n",
      "Epoch [14/60], Step [300/615], Loss: 0.385, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [400/615], Loss: 0.424, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [500/615], Loss: 0.336, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [600/615], Loss: 0.336, Training Accuracy: 87.50%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 62.493238728562794 %\n",
      "Epoch [15/60], Step [100/615], Loss: 0.421, Training Accuracy: 82.07%\n",
      "Epoch [15/60], Step [200/615], Loss: 0.382, Training Accuracy: 84.78%\n",
      "Epoch [15/60], Step [300/615], Loss: 0.404, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [400/615], Loss: 0.390, Training Accuracy: 86.96%\n",
      "Epoch [15/60], Step [500/615], Loss: 0.356, Training Accuracy: 88.04%\n",
      "Epoch [15/60], Step [600/615], Loss: 0.396, Training Accuracy: 87.50%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 60.97553215183429 %\n",
      "Epoch [16/60], Step [100/615], Loss: 0.432, Training Accuracy: 84.78%\n",
      "Epoch [16/60], Step [200/615], Loss: 0.424, Training Accuracy: 84.24%\n",
      "Epoch [16/60], Step [300/615], Loss: 0.502, Training Accuracy: 79.89%\n",
      "Epoch [16/60], Step [400/615], Loss: 0.442, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [500/615], Loss: 0.499, Training Accuracy: 83.70%\n",
      "Epoch [16/60], Step [600/615], Loss: 0.509, Training Accuracy: 80.98%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 61.79324827388717 %\n",
      "Epoch [17/60], Step [100/615], Loss: 0.421, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [200/615], Loss: 0.432, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [300/615], Loss: 0.446, Training Accuracy: 83.70%\n",
      "Epoch [17/60], Step [400/615], Loss: 0.383, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [500/615], Loss: 0.403, Training Accuracy: 85.33%\n",
      "Epoch [17/60], Step [600/615], Loss: 0.335, Training Accuracy: 89.67%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 60.1116802952687 %\n",
      "Epoch [18/60], Step [100/615], Loss: 0.427, Training Accuracy: 83.70%\n",
      "Epoch [18/60], Step [200/615], Loss: 0.436, Training Accuracy: 83.15%\n",
      "Epoch [18/60], Step [300/615], Loss: 0.399, Training Accuracy: 84.78%\n",
      "Epoch [18/60], Step [400/615], Loss: 0.409, Training Accuracy: 80.43%\n",
      "Epoch [18/60], Step [500/615], Loss: 0.397, Training Accuracy: 86.41%\n",
      "Epoch [18/60], Step [600/615], Loss: 0.368, Training Accuracy: 85.87%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 58.97578669381781 %\n",
      "Epoch [19/60], Step [100/615], Loss: 0.418, Training Accuracy: 86.96%\n",
      "Epoch [19/60], Step [200/615], Loss: 0.488, Training Accuracy: 86.41%\n",
      "Epoch [19/60], Step [300/615], Loss: 0.487, Training Accuracy: 79.89%\n",
      "Epoch [19/60], Step [400/615], Loss: 0.338, Training Accuracy: 89.13%\n",
      "Epoch [19/60], Step [500/615], Loss: 0.366, Training Accuracy: 89.67%\n",
      "Epoch [19/60], Step [600/615], Loss: 0.347, Training Accuracy: 89.13%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 58.698972286741544 %\n",
      "Finished early after 19 epochs!\n",
      "Final Training Accuracy 86.11351858075243 %\n",
      "Final Test Accuracy 58.62101880428904 %\n",
      "Epoch [1/60], Step [100/712], Loss: 1.238, Training Accuracy: 48.91%\n",
      "Epoch [1/60], Step [200/712], Loss: 1.092, Training Accuracy: 60.33%\n",
      "Epoch [1/60], Step [300/712], Loss: 0.950, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [400/712], Loss: 0.973, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [500/712], Loss: 1.044, Training Accuracy: 54.35%\n",
      "Epoch [1/60], Step [600/712], Loss: 1.013, Training Accuracy: 61.96%\n",
      "Epoch [1/60], Step [700/712], Loss: 0.920, Training Accuracy: 63.59%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 47.87385148031426 %\n",
      "Epoch [2/60], Step [100/712], Loss: 1.013, Training Accuracy: 57.61%\n",
      "Epoch [2/60], Step [200/712], Loss: 0.793, Training Accuracy: 67.93%\n",
      "Epoch [2/60], Step [300/712], Loss: 0.841, Training Accuracy: 71.20%\n",
      "Epoch [2/60], Step [400/712], Loss: 0.833, Training Accuracy: 65.22%\n",
      "Epoch [2/60], Step [500/712], Loss: 0.743, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [600/712], Loss: 0.801, Training Accuracy: 66.85%\n",
      "Epoch [2/60], Step [700/712], Loss: 0.802, Training Accuracy: 64.67%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 54.902570020861994 %\n",
      "Epoch [3/60], Step [100/712], Loss: 0.835, Training Accuracy: 65.76%\n",
      "Epoch [3/60], Step [200/712], Loss: 0.790, Training Accuracy: 69.02%\n",
      "Epoch [3/60], Step [300/712], Loss: 0.704, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [400/712], Loss: 0.737, Training Accuracy: 67.93%\n",
      "Epoch [3/60], Step [500/712], Loss: 0.703, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [600/712], Loss: 0.651, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [700/712], Loss: 0.854, Training Accuracy: 70.65%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 59.53437791291225 %\n",
      "Epoch [4/60], Step [100/712], Loss: 0.702, Training Accuracy: 71.74%\n",
      "Epoch [4/60], Step [200/712], Loss: 0.725, Training Accuracy: 72.83%\n",
      "Epoch [4/60], Step [300/712], Loss: 0.838, Training Accuracy: 66.85%\n",
      "Epoch [4/60], Step [400/712], Loss: 0.724, Training Accuracy: 67.93%\n",
      "Epoch [4/60], Step [500/712], Loss: 0.691, Training Accuracy: 74.46%\n",
      "Epoch [4/60], Step [600/712], Loss: 0.575, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [700/712], Loss: 0.613, Training Accuracy: 76.63%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 59.84730791424386 %\n",
      "Epoch [5/60], Step [100/712], Loss: 0.635, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [200/712], Loss: 0.608, Training Accuracy: 76.63%\n",
      "Epoch [5/60], Step [300/712], Loss: 0.618, Training Accuracy: 75.00%\n",
      "Epoch [5/60], Step [400/712], Loss: 0.634, Training Accuracy: 74.46%\n",
      "Epoch [5/60], Step [500/712], Loss: 0.524, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [600/712], Loss: 0.528, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [700/712], Loss: 0.649, Training Accuracy: 77.17%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 64.80758133960674 %\n",
      "Epoch [6/60], Step [100/712], Loss: 0.683, Training Accuracy: 76.63%\n",
      "Epoch [6/60], Step [200/712], Loss: 0.566, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [300/712], Loss: 0.555, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [400/712], Loss: 0.604, Training Accuracy: 75.00%\n",
      "Epoch [6/60], Step [500/712], Loss: 0.565, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [600/712], Loss: 0.556, Training Accuracy: 79.35%\n",
      "Epoch [6/60], Step [700/712], Loss: 0.590, Training Accuracy: 78.80%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 61.9889919659106 %\n",
      "Epoch [7/60], Step [100/712], Loss: 0.662, Training Accuracy: 77.72%\n",
      "Epoch [7/60], Step [200/712], Loss: 0.568, Training Accuracy: 78.80%\n",
      "Epoch [7/60], Step [300/712], Loss: 0.654, Training Accuracy: 77.72%\n",
      "Epoch [7/60], Step [400/712], Loss: 0.656, Training Accuracy: 73.91%\n",
      "Epoch [7/60], Step [500/712], Loss: 0.551, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [600/712], Loss: 0.603, Training Accuracy: 76.09%\n",
      "Epoch [7/60], Step [700/712], Loss: 0.522, Training Accuracy: 80.43%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 67.20005326468109 %\n",
      "Epoch [8/60], Step [100/712], Loss: 0.434, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [200/712], Loss: 0.639, Training Accuracy: 71.20%\n",
      "Epoch [8/60], Step [300/712], Loss: 0.598, Training Accuracy: 78.80%\n",
      "Epoch [8/60], Step [400/712], Loss: 0.644, Training Accuracy: 77.72%\n",
      "Epoch [8/60], Step [500/712], Loss: 0.625, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [600/712], Loss: 0.570, Training Accuracy: 79.35%\n",
      "Epoch [8/60], Step [700/712], Loss: 0.550, Training Accuracy: 80.98%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 62.93887877846331 %\n",
      "Epoch [9/60], Step [100/712], Loss: 0.472, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [200/712], Loss: 0.491, Training Accuracy: 80.43%\n",
      "Epoch [9/60], Step [300/712], Loss: 0.661, Training Accuracy: 79.35%\n",
      "Epoch [9/60], Step [400/712], Loss: 0.496, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [500/712], Loss: 0.565, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [600/712], Loss: 0.499, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [700/712], Loss: 0.515, Training Accuracy: 80.43%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 65.7530294287363 %\n",
      "Epoch [10/60], Step [100/712], Loss: 0.438, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [200/712], Loss: 0.450, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [300/712], Loss: 0.583, Training Accuracy: 79.89%\n",
      "Epoch [10/60], Step [400/712], Loss: 0.433, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [500/712], Loss: 0.431, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [600/712], Loss: 0.460, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [700/712], Loss: 0.431, Training Accuracy: 84.24%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 65.99050113187447 %\n",
      "Epoch [11/60], Step [100/712], Loss: 0.561, Training Accuracy: 80.98%\n",
      "Epoch [11/60], Step [200/712], Loss: 0.418, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [300/712], Loss: 0.380, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [400/712], Loss: 0.582, Training Accuracy: 80.98%\n",
      "Epoch [11/60], Step [500/712], Loss: 0.448, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [600/712], Loss: 0.377, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [700/712], Loss: 0.351, Training Accuracy: 88.59%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 66.31452794176394 %\n",
      "Epoch [12/60], Step [100/712], Loss: 0.504, Training Accuracy: 83.15%\n",
      "Epoch [12/60], Step [200/712], Loss: 0.464, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [300/712], Loss: 0.591, Training Accuracy: 80.98%\n",
      "Epoch [12/60], Step [400/712], Loss: 0.478, Training Accuracy: 81.52%\n",
      "Epoch [12/60], Step [500/712], Loss: 0.368, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [600/712], Loss: 0.415, Training Accuracy: 84.78%\n",
      "Epoch [12/60], Step [700/712], Loss: 0.378, Training Accuracy: 87.50%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 70.1451462559368 %\n",
      "Epoch [13/60], Step [100/712], Loss: 0.444, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [200/712], Loss: 0.440, Training Accuracy: 85.33%\n",
      "Epoch [13/60], Step [300/712], Loss: 0.472, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [400/712], Loss: 0.419, Training Accuracy: 86.41%\n",
      "Epoch [13/60], Step [500/712], Loss: 0.382, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [600/712], Loss: 0.528, Training Accuracy: 80.98%\n",
      "Epoch [13/60], Step [700/712], Loss: 0.483, Training Accuracy: 80.43%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 68.95556837853434 %\n",
      "Epoch [14/60], Step [100/712], Loss: 0.516, Training Accuracy: 79.89%\n",
      "Epoch [14/60], Step [200/712], Loss: 0.517, Training Accuracy: 77.72%\n",
      "Epoch [14/60], Step [300/712], Loss: 0.518, Training Accuracy: 83.15%\n",
      "Epoch [14/60], Step [400/712], Loss: 0.313, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [500/712], Loss: 0.506, Training Accuracy: 80.43%\n",
      "Epoch [14/60], Step [600/712], Loss: 0.461, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [700/712], Loss: 0.379, Training Accuracy: 88.59%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 66.68294198588485 %\n",
      "Epoch [15/60], Step [100/712], Loss: 0.465, Training Accuracy: 80.98%\n",
      "Epoch [15/60], Step [200/712], Loss: 0.506, Training Accuracy: 85.33%\n",
      "Epoch [15/60], Step [300/712], Loss: 0.431, Training Accuracy: 84.78%\n",
      "Epoch [15/60], Step [400/712], Loss: 0.349, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [500/712], Loss: 0.420, Training Accuracy: 85.87%\n",
      "Epoch [15/60], Step [600/712], Loss: 0.389, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [700/712], Loss: 0.320, Training Accuracy: 90.76%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 67.0890851791025 %\n",
      "Epoch [16/60], Step [100/712], Loss: 0.341, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [200/712], Loss: 0.350, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [300/712], Loss: 0.348, Training Accuracy: 83.70%\n",
      "Epoch [16/60], Step [400/712], Loss: 0.385, Training Accuracy: 86.96%\n",
      "Epoch [16/60], Step [500/712], Loss: 0.467, Training Accuracy: 84.24%\n",
      "Epoch [16/60], Step [600/712], Loss: 0.535, Training Accuracy: 80.98%\n",
      "Epoch [16/60], Step [700/712], Loss: 0.514, Training Accuracy: 84.24%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 69.58586710462072 %\n",
      "Epoch [17/60], Step [100/712], Loss: 0.385, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [200/712], Loss: 0.394, Training Accuracy: 86.41%\n",
      "Epoch [17/60], Step [300/712], Loss: 0.524, Training Accuracy: 84.78%\n",
      "Epoch [17/60], Step [400/712], Loss: 0.322, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [500/712], Loss: 0.438, Training Accuracy: 80.98%\n",
      "Epoch [17/60], Step [600/712], Loss: 0.449, Training Accuracy: 81.52%\n",
      "Epoch [17/60], Step [700/712], Loss: 0.365, Training Accuracy: 88.04%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 69.9720360424342 %\n",
      "Finished early after 17 epochs!\n",
      "Final Training Accuracy 86.72540000305638 %\n",
      "Final Test Accuracy 69.8455324248746 %\n",
      "Epoch [1/60], Step [100/610], Loss: 1.394, Training Accuracy: 35.33%\n",
      "Epoch [1/60], Step [200/610], Loss: 1.107, Training Accuracy: 56.52%\n",
      "Epoch [1/60], Step [300/610], Loss: 1.115, Training Accuracy: 57.07%\n",
      "Epoch [1/60], Step [400/610], Loss: 1.039, Training Accuracy: 57.07%\n",
      "Epoch [1/60], Step [500/610], Loss: 0.901, Training Accuracy: 60.87%\n",
      "Epoch [1/60], Step [600/610], Loss: 0.759, Training Accuracy: 70.11%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 51.214190408916096 %\n",
      "Epoch [2/60], Step [100/610], Loss: 0.837, Training Accuracy: 62.50%\n",
      "Epoch [2/60], Step [200/610], Loss: 0.735, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [300/610], Loss: 0.774, Training Accuracy: 67.93%\n",
      "Epoch [2/60], Step [400/610], Loss: 0.781, Training Accuracy: 67.93%\n",
      "Epoch [2/60], Step [500/610], Loss: 1.024, Training Accuracy: 65.22%\n",
      "Epoch [2/60], Step [600/610], Loss: 0.854, Training Accuracy: 63.04%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 53.27211364885017 %\n",
      "Epoch [3/60], Step [100/610], Loss: 0.791, Training Accuracy: 65.76%\n",
      "Epoch [3/60], Step [200/610], Loss: 0.654, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [300/610], Loss: 0.633, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [400/610], Loss: 0.717, Training Accuracy: 68.48%\n",
      "Epoch [3/60], Step [500/610], Loss: 0.724, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [600/610], Loss: 0.637, Training Accuracy: 73.91%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 53.374146456322116 %\n",
      "Epoch [4/60], Step [100/610], Loss: 0.579, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [200/610], Loss: 0.518, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [300/610], Loss: 0.641, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [400/610], Loss: 0.797, Training Accuracy: 67.93%\n",
      "Epoch [4/60], Step [500/610], Loss: 0.772, Training Accuracy: 67.39%\n",
      "Epoch [4/60], Step [600/610], Loss: 0.710, Training Accuracy: 71.20%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 53.625304136253035 %\n",
      "Epoch [5/60], Step [100/610], Loss: 0.613, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [200/610], Loss: 0.682, Training Accuracy: 66.30%\n",
      "Epoch [5/60], Step [300/610], Loss: 0.593, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [400/610], Loss: 0.666, Training Accuracy: 69.57%\n",
      "Epoch [5/60], Step [500/610], Loss: 0.596, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [600/610], Loss: 0.533, Training Accuracy: 84.78%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 54.100933992622245 %\n",
      "Epoch [6/60], Step [100/610], Loss: 0.633, Training Accuracy: 73.91%\n",
      "Epoch [6/60], Step [200/610], Loss: 0.625, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [300/610], Loss: 0.572, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [400/610], Loss: 0.624, Training Accuracy: 72.28%\n",
      "Epoch [6/60], Step [500/610], Loss: 0.476, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [600/610], Loss: 0.639, Training Accuracy: 77.17%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 55.73816811867201 %\n",
      "Epoch [7/60], Step [100/610], Loss: 0.653, Training Accuracy: 75.00%\n",
      "Epoch [7/60], Step [200/610], Loss: 0.617, Training Accuracy: 73.37%\n",
      "Epoch [7/60], Step [300/610], Loss: 0.592, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [400/610], Loss: 0.532, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [500/610], Loss: 0.491, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [600/610], Loss: 0.514, Training Accuracy: 83.70%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 56.53088454595401 %\n",
      "Epoch [8/60], Step [100/610], Loss: 0.536, Training Accuracy: 75.00%\n",
      "Epoch [8/60], Step [200/610], Loss: 0.493, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [300/610], Loss: 0.515, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [400/610], Loss: 0.436, Training Accuracy: 77.72%\n",
      "Epoch [8/60], Step [500/610], Loss: 0.580, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [600/610], Loss: 0.445, Training Accuracy: 77.72%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 54.92347539439605 %\n",
      "Epoch [9/60], Step [100/610], Loss: 0.519, Training Accuracy: 77.72%\n",
      "Epoch [9/60], Step [200/610], Loss: 0.590, Training Accuracy: 76.09%\n",
      "Epoch [9/60], Step [300/610], Loss: 0.554, Training Accuracy: 75.54%\n",
      "Epoch [9/60], Step [400/610], Loss: 0.501, Training Accuracy: 80.43%\n",
      "Epoch [9/60], Step [500/610], Loss: 0.415, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [600/610], Loss: 0.443, Training Accuracy: 84.78%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 55.306490856290715 %\n",
      "Epoch [10/60], Step [100/610], Loss: 0.486, Training Accuracy: 79.89%\n",
      "Epoch [10/60], Step [200/610], Loss: 0.519, Training Accuracy: 80.98%\n",
      "Epoch [10/60], Step [300/610], Loss: 0.499, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [400/610], Loss: 0.515, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [500/610], Loss: 0.451, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [600/610], Loss: 0.553, Training Accuracy: 83.15%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 56.69256730240954 %\n",
      "Epoch [11/60], Step [100/610], Loss: 0.432, Training Accuracy: 79.35%\n",
      "Epoch [11/60], Step [200/610], Loss: 0.395, Training Accuracy: 82.07%\n",
      "Epoch [11/60], Step [300/610], Loss: 0.425, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [400/610], Loss: 0.461, Training Accuracy: 83.15%\n",
      "Epoch [11/60], Step [500/610], Loss: 0.546, Training Accuracy: 80.98%\n",
      "Epoch [11/60], Step [600/610], Loss: 0.392, Training Accuracy: 82.61%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 56.13217172906365 %\n",
      "Epoch [12/60], Step [100/610], Loss: 0.426, Training Accuracy: 79.35%\n",
      "Epoch [12/60], Step [200/610], Loss: 0.362, Training Accuracy: 84.78%\n",
      "Epoch [12/60], Step [300/610], Loss: 0.432, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [400/610], Loss: 0.368, Training Accuracy: 87.50%\n",
      "Epoch [12/60], Step [500/610], Loss: 0.451, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [600/610], Loss: 0.433, Training Accuracy: 83.15%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 56.9970959893258 %\n",
      "Epoch [13/60], Step [100/610], Loss: 0.421, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [200/610], Loss: 0.436, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [300/610], Loss: 0.394, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [400/610], Loss: 0.497, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [500/610], Loss: 0.426, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [600/610], Loss: 0.404, Training Accuracy: 83.15%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 58.09434110352406 %\n",
      "Epoch [14/60], Step [100/610], Loss: 0.536, Training Accuracy: 78.26%\n",
      "Epoch [14/60], Step [200/610], Loss: 0.277, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [300/610], Loss: 0.300, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [400/610], Loss: 0.476, Training Accuracy: 85.33%\n",
      "Epoch [14/60], Step [500/610], Loss: 0.474, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [600/610], Loss: 0.315, Training Accuracy: 86.96%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 53.5483870967742 %\n",
      "Epoch [15/60], Step [100/610], Loss: 0.398, Training Accuracy: 81.52%\n",
      "Epoch [15/60], Step [200/610], Loss: 0.297, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [300/610], Loss: 0.352, Training Accuracy: 84.78%\n",
      "Epoch [15/60], Step [400/610], Loss: 0.485, Training Accuracy: 80.43%\n",
      "Epoch [15/60], Step [500/610], Loss: 0.437, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [600/610], Loss: 0.296, Training Accuracy: 85.33%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 59.1868770112236 %\n",
      "Epoch [16/60], Step [100/610], Loss: 0.365, Training Accuracy: 82.61%\n",
      "Epoch [16/60], Step [200/610], Loss: 0.303, Training Accuracy: 82.61%\n",
      "Epoch [16/60], Step [300/610], Loss: 0.372, Training Accuracy: 84.78%\n",
      "Epoch [16/60], Step [400/610], Loss: 0.379, Training Accuracy: 85.87%\n",
      "Epoch [16/60], Step [500/610], Loss: 0.412, Training Accuracy: 85.33%\n",
      "Epoch [16/60], Step [600/610], Loss: 0.343, Training Accuracy: 83.15%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 58.551134133898444 %\n",
      "Epoch [17/60], Step [100/610], Loss: 0.378, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [200/610], Loss: 0.368, Training Accuracy: 84.78%\n",
      "Epoch [17/60], Step [300/610], Loss: 0.277, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [400/610], Loss: 0.262, Training Accuracy: 89.13%\n",
      "Epoch [17/60], Step [500/610], Loss: 0.343, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [600/610], Loss: 0.310, Training Accuracy: 82.61%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 58.28741856997096 %\n",
      "Epoch [18/60], Step [100/610], Loss: 0.346, Training Accuracy: 84.78%\n",
      "Epoch [18/60], Step [200/610], Loss: 0.395, Training Accuracy: 83.70%\n",
      "Epoch [18/60], Step [300/610], Loss: 0.356, Training Accuracy: 85.33%\n",
      "Epoch [18/60], Step [400/610], Loss: 0.356, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [500/610], Loss: 0.401, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [600/610], Loss: 0.296, Training Accuracy: 89.13%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 55.69735499568323 %\n",
      "Epoch [19/60], Step [100/610], Loss: 0.329, Training Accuracy: 84.78%\n",
      "Epoch [19/60], Step [200/610], Loss: 0.364, Training Accuracy: 84.78%\n",
      "Epoch [19/60], Step [300/610], Loss: 0.338, Training Accuracy: 83.70%\n",
      "Epoch [19/60], Step [400/610], Loss: 0.279, Training Accuracy: 89.13%\n",
      "Epoch [19/60], Step [500/610], Loss: 0.401, Training Accuracy: 88.59%\n",
      "Epoch [19/60], Step [600/610], Loss: 0.296, Training Accuracy: 85.87%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 57.30162467624206 %\n",
      "Epoch [20/60], Step [100/610], Loss: 0.313, Training Accuracy: 85.33%\n",
      "Epoch [20/60], Step [200/610], Loss: 0.308, Training Accuracy: 85.33%\n",
      "Epoch [20/60], Step [300/610], Loss: 0.463, Training Accuracy: 84.24%\n",
      "Epoch [20/60], Step [400/610], Loss: 0.426, Training Accuracy: 88.04%\n",
      "Epoch [20/60], Step [500/610], Loss: 0.242, Training Accuracy: 90.76%\n",
      "Epoch [20/60], Step [600/610], Loss: 0.307, Training Accuracy: 88.04%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 57.25610234675457 %\n",
      "Finished early after 20 epochs!\n",
      "Final Training Accuracy 86.92293298404128 %\n",
      "Final Test Accuracy 57.340868063731264 %\n",
      "Epoch [1/60], Step [100/638], Loss: 1.312, Training Accuracy: 54.89%\n",
      "Epoch [1/60], Step [200/638], Loss: 1.097, Training Accuracy: 52.72%\n",
      "Epoch [1/60], Step [300/638], Loss: 0.957, Training Accuracy: 60.87%\n",
      "Epoch [1/60], Step [400/638], Loss: 0.962, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [500/638], Loss: 1.012, Training Accuracy: 57.07%\n",
      "Epoch [1/60], Step [600/638], Loss: 0.829, Training Accuracy: 64.67%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 50.53589104686487 %\n",
      "Epoch [2/60], Step [100/638], Loss: 0.846, Training Accuracy: 64.13%\n",
      "Epoch [2/60], Step [200/638], Loss: 0.840, Training Accuracy: 63.59%\n",
      "Epoch [2/60], Step [300/638], Loss: 0.976, Training Accuracy: 62.50%\n",
      "Epoch [2/60], Step [400/638], Loss: 0.817, Training Accuracy: 70.65%\n",
      "Epoch [2/60], Step [500/638], Loss: 0.823, Training Accuracy: 69.57%\n",
      "Epoch [2/60], Step [600/638], Loss: 0.863, Training Accuracy: 67.39%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 51.09567532511862 %\n",
      "Epoch [3/60], Step [100/638], Loss: 0.746, Training Accuracy: 71.74%\n",
      "Epoch [3/60], Step [200/638], Loss: 0.736, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [300/638], Loss: 0.790, Training Accuracy: 64.67%\n",
      "Epoch [3/60], Step [400/638], Loss: 0.682, Training Accuracy: 68.48%\n",
      "Epoch [3/60], Step [500/638], Loss: 0.770, Training Accuracy: 71.20%\n",
      "Epoch [3/60], Step [600/638], Loss: 0.862, Training Accuracy: 66.85%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 53.44233197938355 %\n",
      "Epoch [4/60], Step [100/638], Loss: 0.748, Training Accuracy: 67.93%\n",
      "Epoch [4/60], Step [200/638], Loss: 0.627, Training Accuracy: 64.13%\n",
      "Epoch [4/60], Step [300/638], Loss: 0.789, Training Accuracy: 65.76%\n",
      "Epoch [4/60], Step [400/638], Loss: 0.797, Training Accuracy: 68.48%\n",
      "Epoch [4/60], Step [500/638], Loss: 0.821, Training Accuracy: 71.74%\n",
      "Epoch [4/60], Step [600/638], Loss: 0.587, Training Accuracy: 77.17%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 55.79410861180326 %\n",
      "Epoch [5/60], Step [100/638], Loss: 0.613, Training Accuracy: 75.54%\n",
      "Epoch [5/60], Step [200/638], Loss: 0.670, Training Accuracy: 65.76%\n",
      "Epoch [5/60], Step [300/638], Loss: 0.735, Training Accuracy: 74.46%\n",
      "Epoch [5/60], Step [400/638], Loss: 0.627, Training Accuracy: 75.00%\n",
      "Epoch [5/60], Step [500/638], Loss: 0.850, Training Accuracy: 71.20%\n",
      "Epoch [5/60], Step [600/638], Loss: 0.610, Training Accuracy: 76.63%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 58.74662934771479 %\n",
      "Epoch [6/60], Step [100/638], Loss: 0.631, Training Accuracy: 75.00%\n",
      "Epoch [6/60], Step [200/638], Loss: 0.580, Training Accuracy: 73.91%\n",
      "Epoch [6/60], Step [300/638], Loss: 0.604, Training Accuracy: 71.74%\n",
      "Epoch [6/60], Step [400/638], Loss: 0.684, Training Accuracy: 71.20%\n",
      "Epoch [6/60], Step [500/638], Loss: 0.488, Training Accuracy: 76.63%\n",
      "Epoch [6/60], Step [600/638], Loss: 0.535, Training Accuracy: 83.70%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 57.708980441683444 %\n",
      "Epoch [7/60], Step [100/638], Loss: 0.537, Training Accuracy: 72.83%\n",
      "Epoch [7/60], Step [200/638], Loss: 0.586, Training Accuracy: 73.37%\n",
      "Epoch [7/60], Step [300/638], Loss: 0.618, Training Accuracy: 78.26%\n",
      "Epoch [7/60], Step [400/638], Loss: 0.619, Training Accuracy: 78.26%\n",
      "Epoch [7/60], Step [500/638], Loss: 0.594, Training Accuracy: 76.09%\n",
      "Epoch [7/60], Step [600/638], Loss: 0.671, Training Accuracy: 79.35%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 60.670034474519575 %\n",
      "Epoch [8/60], Step [100/638], Loss: 0.567, Training Accuracy: 70.65%\n",
      "Epoch [8/60], Step [200/638], Loss: 0.599, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [300/638], Loss: 0.430, Training Accuracy: 79.89%\n",
      "Epoch [8/60], Step [400/638], Loss: 0.582, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [500/638], Loss: 0.580, Training Accuracy: 75.54%\n",
      "Epoch [8/60], Step [600/638], Loss: 0.621, Training Accuracy: 78.80%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 61.95344233197938 %\n",
      "Epoch [9/60], Step [100/638], Loss: 0.437, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [200/638], Loss: 0.568, Training Accuracy: 73.91%\n",
      "Epoch [9/60], Step [300/638], Loss: 0.553, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [400/638], Loss: 0.454, Training Accuracy: 80.43%\n",
      "Epoch [9/60], Step [500/638], Loss: 0.501, Training Accuracy: 80.98%\n",
      "Epoch [9/60], Step [600/638], Loss: 0.531, Training Accuracy: 82.07%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 59.0043349148377 %\n",
      "Epoch [10/60], Step [100/638], Loss: 0.502, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [200/638], Loss: 0.491, Training Accuracy: 77.72%\n",
      "Epoch [10/60], Step [300/638], Loss: 0.453, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [400/638], Loss: 0.515, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [500/638], Loss: 0.607, Training Accuracy: 79.35%\n",
      "Epoch [10/60], Step [600/638], Loss: 0.489, Training Accuracy: 80.43%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 62.13093490801106 %\n",
      "Epoch [11/60], Step [100/638], Loss: 0.460, Training Accuracy: 80.98%\n",
      "Epoch [11/60], Step [200/638], Loss: 0.500, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [300/638], Loss: 0.503, Training Accuracy: 83.15%\n",
      "Epoch [11/60], Step [400/638], Loss: 0.406, Training Accuracy: 83.15%\n",
      "Epoch [11/60], Step [500/638], Loss: 0.449, Training Accuracy: 76.63%\n",
      "Epoch [11/60], Step [600/638], Loss: 0.388, Training Accuracy: 82.61%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 62.004642113526984 %\n",
      "Epoch [12/60], Step [100/638], Loss: 0.541, Training Accuracy: 78.80%\n",
      "Epoch [12/60], Step [200/638], Loss: 0.460, Training Accuracy: 80.43%\n",
      "Epoch [12/60], Step [300/638], Loss: 0.558, Training Accuracy: 77.72%\n",
      "Epoch [12/60], Step [400/638], Loss: 0.372, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [500/638], Loss: 0.482, Training Accuracy: 79.89%\n",
      "Epoch [12/60], Step [600/638], Loss: 0.527, Training Accuracy: 78.26%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 61.77765641533263 %\n",
      "Epoch [13/60], Step [100/638], Loss: 0.461, Training Accuracy: 79.35%\n",
      "Epoch [13/60], Step [200/638], Loss: 0.499, Training Accuracy: 76.63%\n",
      "Epoch [13/60], Step [300/638], Loss: 0.504, Training Accuracy: 82.61%\n",
      "Epoch [13/60], Step [400/638], Loss: 0.415, Training Accuracy: 86.41%\n",
      "Epoch [13/60], Step [500/638], Loss: 0.538, Training Accuracy: 82.61%\n",
      "Epoch [13/60], Step [600/638], Loss: 0.507, Training Accuracy: 81.52%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 64.84622998941872 %\n",
      "Epoch [14/60], Step [100/638], Loss: 0.389, Training Accuracy: 85.33%\n",
      "Epoch [14/60], Step [200/638], Loss: 0.478, Training Accuracy: 77.17%\n",
      "Epoch [14/60], Step [300/638], Loss: 0.402, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [400/638], Loss: 0.404, Training Accuracy: 83.15%\n",
      "Epoch [14/60], Step [500/638], Loss: 0.533, Training Accuracy: 77.17%\n",
      "Epoch [14/60], Step [600/638], Loss: 0.425, Training Accuracy: 81.52%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 64.11407311328804 %\n",
      "Epoch [15/60], Step [100/638], Loss: 0.345, Training Accuracy: 86.41%\n",
      "Epoch [15/60], Step [200/638], Loss: 0.423, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [300/638], Loss: 0.438, Training Accuracy: 83.15%\n",
      "Epoch [15/60], Step [400/638], Loss: 0.426, Training Accuracy: 82.61%\n",
      "Epoch [15/60], Step [500/638], Loss: 0.419, Training Accuracy: 86.96%\n",
      "Epoch [15/60], Step [600/638], Loss: 0.411, Training Accuracy: 82.61%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 65.948731952077 %\n",
      "Epoch [16/60], Step [100/638], Loss: 0.422, Training Accuracy: 86.96%\n",
      "Epoch [16/60], Step [200/638], Loss: 0.411, Training Accuracy: 78.26%\n",
      "Epoch [16/60], Step [300/638], Loss: 0.412, Training Accuracy: 84.24%\n",
      "Epoch [16/60], Step [400/638], Loss: 0.412, Training Accuracy: 79.89%\n",
      "Epoch [16/60], Step [500/638], Loss: 0.403, Training Accuracy: 85.33%\n",
      "Epoch [16/60], Step [600/638], Loss: 0.389, Training Accuracy: 82.61%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 65.12612212854559 %\n",
      "Epoch [17/60], Step [100/638], Loss: 0.440, Training Accuracy: 81.52%\n",
      "Epoch [17/60], Step [200/638], Loss: 0.320, Training Accuracy: 86.41%\n",
      "Epoch [17/60], Step [300/638], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [17/60], Step [400/638], Loss: 0.408, Training Accuracy: 83.15%\n",
      "Epoch [17/60], Step [500/638], Loss: 0.437, Training Accuracy: 85.87%\n",
      "Epoch [17/60], Step [600/638], Loss: 0.556, Training Accuracy: 80.43%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 62.70949243949892 %\n",
      "Epoch [18/60], Step [100/638], Loss: 0.350, Training Accuracy: 83.70%\n",
      "Epoch [18/60], Step [200/638], Loss: 0.488, Training Accuracy: 80.43%\n",
      "Epoch [18/60], Step [300/638], Loss: 0.443, Training Accuracy: 79.89%\n",
      "Epoch [18/60], Step [400/638], Loss: 0.476, Training Accuracy: 83.70%\n",
      "Epoch [18/60], Step [500/638], Loss: 0.407, Training Accuracy: 84.24%\n",
      "Epoch [18/60], Step [600/638], Loss: 0.476, Training Accuracy: 83.15%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 63.46554254701846 %\n",
      "Epoch [19/60], Step [100/638], Loss: 0.362, Training Accuracy: 85.33%\n",
      "Epoch [19/60], Step [200/638], Loss: 0.411, Training Accuracy: 84.78%\n",
      "Epoch [19/60], Step [300/638], Loss: 0.437, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [400/638], Loss: 0.332, Training Accuracy: 85.87%\n",
      "Epoch [19/60], Step [500/638], Loss: 0.484, Training Accuracy: 82.07%\n",
      "Epoch [19/60], Step [600/638], Loss: 0.436, Training Accuracy: 84.78%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 63.74714134553025 %\n",
      "Epoch [20/60], Step [100/638], Loss: 0.378, Training Accuracy: 84.24%\n",
      "Epoch [20/60], Step [200/638], Loss: 0.347, Training Accuracy: 86.96%\n",
      "Epoch [20/60], Step [300/638], Loss: 0.358, Training Accuracy: 81.52%\n",
      "Epoch [20/60], Step [400/638], Loss: 0.383, Training Accuracy: 83.15%\n",
      "Epoch [20/60], Step [500/638], Loss: 0.504, Training Accuracy: 83.15%\n",
      "Epoch [20/60], Step [600/638], Loss: 0.446, Training Accuracy: 83.15%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 66.46755640509268 %\n",
      "Epoch [21/60], Step [100/638], Loss: 0.381, Training Accuracy: 86.96%\n",
      "Epoch [21/60], Step [200/638], Loss: 0.354, Training Accuracy: 86.96%\n",
      "Epoch [21/60], Step [300/638], Loss: 0.374, Training Accuracy: 86.41%\n",
      "Epoch [21/60], Step [400/638], Loss: 0.342, Training Accuracy: 87.50%\n",
      "Epoch [21/60], Step [500/638], Loss: 0.362, Training Accuracy: 85.87%\n",
      "Epoch [21/60], Step [600/638], Loss: 0.389, Training Accuracy: 85.33%\n",
      "Testing epoch 21\n",
      "Epoch 21: Test Accuracy 62.88186503737584 %\n",
      "Epoch [22/60], Step [100/638], Loss: 0.448, Training Accuracy: 80.98%\n",
      "Epoch [22/60], Step [200/638], Loss: 0.468, Training Accuracy: 86.41%\n",
      "Epoch [22/60], Step [300/638], Loss: 0.462, Training Accuracy: 83.15%\n",
      "Epoch [22/60], Step [400/638], Loss: 0.334, Training Accuracy: 86.41%\n",
      "Epoch [22/60], Step [500/638], Loss: 0.405, Training Accuracy: 84.24%\n",
      "Epoch [22/60], Step [600/638], Loss: 0.301, Training Accuracy: 89.67%\n",
      "Testing epoch 22\n",
      "Epoch 22: Test Accuracy 67.7782708127112 %\n",
      "Epoch [23/60], Step [100/638], Loss: 0.440, Training Accuracy: 82.07%\n",
      "Epoch [23/60], Step [200/638], Loss: 0.477, Training Accuracy: 83.70%\n",
      "Epoch [23/60], Step [300/638], Loss: 0.247, Training Accuracy: 90.22%\n",
      "Epoch [23/60], Step [400/638], Loss: 0.351, Training Accuracy: 88.04%\n",
      "Epoch [23/60], Step [500/638], Loss: 0.303, Training Accuracy: 86.96%\n",
      "Epoch [23/60], Step [600/638], Loss: 0.339, Training Accuracy: 84.24%\n",
      "Testing epoch 23\n",
      "Epoch 23: Test Accuracy 65.50670717138274 %\n",
      "Epoch [24/60], Step [100/638], Loss: 0.335, Training Accuracy: 88.04%\n",
      "Epoch [24/60], Step [200/638], Loss: 0.356, Training Accuracy: 85.33%\n",
      "Epoch [24/60], Step [300/638], Loss: 0.297, Training Accuracy: 90.22%\n",
      "Epoch [24/60], Step [400/638], Loss: 0.517, Training Accuracy: 82.61%\n",
      "Epoch [24/60], Step [500/638], Loss: 0.329, Training Accuracy: 86.41%\n",
      "Epoch [24/60], Step [600/638], Loss: 0.379, Training Accuracy: 83.15%\n",
      "Testing epoch 24\n",
      "Epoch 24: Test Accuracy 65.33092125473598 %\n",
      "Epoch [25/60], Step [100/638], Loss: 0.299, Training Accuracy: 90.76%\n",
      "Epoch [25/60], Step [200/638], Loss: 0.378, Training Accuracy: 78.26%\n",
      "Epoch [25/60], Step [300/638], Loss: 0.367, Training Accuracy: 84.24%\n",
      "Epoch [25/60], Step [400/638], Loss: 0.447, Training Accuracy: 83.70%\n",
      "Epoch [25/60], Step [500/638], Loss: 0.340, Training Accuracy: 88.59%\n",
      "Epoch [25/60], Step [600/638], Loss: 0.338, Training Accuracy: 84.78%\n",
      "Testing epoch 25\n",
      "Epoch 25: Test Accuracy 66.63651568419975 %\n",
      "Epoch [26/60], Step [100/638], Loss: 0.354, Training Accuracy: 85.87%\n",
      "Epoch [26/60], Step [200/638], Loss: 0.439, Training Accuracy: 84.78%\n",
      "Epoch [26/60], Step [300/638], Loss: 0.393, Training Accuracy: 84.24%\n",
      "Epoch [26/60], Step [400/638], Loss: 0.379, Training Accuracy: 84.78%\n",
      "Epoch [26/60], Step [500/638], Loss: 0.297, Training Accuracy: 85.87%\n",
      "Epoch [26/60], Step [600/638], Loss: 0.301, Training Accuracy: 85.87%\n",
      "Testing epoch 26\n",
      "Epoch 26: Test Accuracy 66.10574461548964 %\n",
      "Epoch [27/60], Step [100/638], Loss: 0.333, Training Accuracy: 84.78%\n",
      "Epoch [27/60], Step [200/638], Loss: 0.381, Training Accuracy: 82.07%\n",
      "Epoch [27/60], Step [300/638], Loss: 0.356, Training Accuracy: 88.04%\n",
      "Epoch [27/60], Step [400/638], Loss: 0.314, Training Accuracy: 88.59%\n",
      "Epoch [27/60], Step [500/638], Loss: 0.263, Training Accuracy: 89.67%\n",
      "Epoch [27/60], Step [600/638], Loss: 0.369, Training Accuracy: 88.59%\n",
      "Testing epoch 27\n",
      "Epoch 27: Test Accuracy 68.04109635798888 %\n",
      "Epoch [28/60], Step [100/638], Loss: 0.282, Training Accuracy: 84.78%\n",
      "Epoch [28/60], Step [200/638], Loss: 0.325, Training Accuracy: 86.96%\n",
      "Epoch [28/60], Step [300/638], Loss: 0.379, Training Accuracy: 86.41%\n",
      "Epoch [28/60], Step [400/638], Loss: 0.258, Training Accuracy: 91.30%\n",
      "Epoch [28/60], Step [500/638], Loss: 0.347, Training Accuracy: 85.87%\n",
      "Epoch [28/60], Step [600/638], Loss: 0.327, Training Accuracy: 88.04%\n",
      "Testing epoch 28\n",
      "Epoch 28: Test Accuracy 66.34638358876336 %\n",
      "Epoch [29/60], Step [100/638], Loss: 0.432, Training Accuracy: 88.59%\n",
      "Epoch [29/60], Step [200/638], Loss: 0.293, Training Accuracy: 86.96%\n",
      "Epoch [29/60], Step [300/638], Loss: 0.356, Training Accuracy: 84.78%\n",
      "Epoch [29/60], Step [400/638], Loss: 0.328, Training Accuracy: 90.22%\n",
      "Epoch [29/60], Step [500/638], Loss: 0.297, Training Accuracy: 85.33%\n",
      "Epoch [29/60], Step [600/638], Loss: 0.296, Training Accuracy: 88.04%\n",
      "Testing epoch 29\n",
      "Epoch 29: Test Accuracy 65.70979963818822 %\n",
      "Epoch [30/60], Step [100/638], Loss: 0.423, Training Accuracy: 82.61%\n",
      "Epoch [30/60], Step [200/638], Loss: 0.297, Training Accuracy: 85.87%\n",
      "Epoch [30/60], Step [300/638], Loss: 0.254, Training Accuracy: 92.39%\n",
      "Epoch [30/60], Step [400/638], Loss: 0.379, Training Accuracy: 83.15%\n",
      "Epoch [30/60], Step [500/638], Loss: 0.327, Training Accuracy: 82.61%\n",
      "Epoch [30/60], Step [600/638], Loss: 0.280, Training Accuracy: 91.85%\n",
      "Testing epoch 30\n",
      "Epoch 30: Test Accuracy 67.42157900126293 %\n",
      "Epoch [31/60], Step [100/638], Loss: 0.255, Training Accuracy: 88.59%\n",
      "Epoch [31/60], Step [200/638], Loss: 0.352, Training Accuracy: 85.33%\n",
      "Epoch [31/60], Step [300/638], Loss: 0.199, Training Accuracy: 94.02%\n",
      "Epoch [31/60], Step [400/638], Loss: 0.453, Training Accuracy: 84.78%\n",
      "Epoch [31/60], Step [500/638], Loss: 0.306, Training Accuracy: 88.59%\n",
      "Epoch [31/60], Step [600/638], Loss: 0.272, Training Accuracy: 90.76%\n",
      "Testing epoch 31\n",
      "Epoch 31: Test Accuracy 65.46574734614465 %\n",
      "Epoch [32/60], Step [100/638], Loss: 0.332, Training Accuracy: 85.33%\n",
      "Epoch [32/60], Step [200/638], Loss: 0.297, Training Accuracy: 86.41%\n",
      "Epoch [32/60], Step [300/638], Loss: 0.362, Training Accuracy: 87.50%\n",
      "Epoch [32/60], Step [400/638], Loss: 0.317, Training Accuracy: 86.96%\n",
      "Epoch [32/60], Step [500/638], Loss: 0.436, Training Accuracy: 85.33%\n",
      "Epoch [32/60], Step [600/638], Loss: 0.297, Training Accuracy: 86.96%\n",
      "Testing epoch 32\n",
      "Epoch 32: Test Accuracy 65.60740007509301 %\n",
      "Finished early after 32 epochs!\n",
      "Final Training Accuracy 87.56924440505207 %\n",
      "Final Test Accuracy 65.57668020616445 %\n",
      "Epoch [1/60], Step [100/665], Loss: 1.128, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [200/665], Loss: 1.080, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [300/665], Loss: 1.053, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [400/665], Loss: 0.934, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [500/665], Loss: 0.814, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [600/665], Loss: 0.965, Training Accuracy: 67.93%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 42.604366714522776 %\n",
      "Epoch [2/60], Step [100/665], Loss: 0.867, Training Accuracy: 72.28%\n",
      "Epoch [2/60], Step [200/665], Loss: 0.722, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/665], Loss: 0.813, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [400/665], Loss: 0.785, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [500/665], Loss: 0.664, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [600/665], Loss: 0.789, Training Accuracy: 77.17%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 41.75414390393974 %\n",
      "Epoch [3/60], Step [100/665], Loss: 0.561, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [200/665], Loss: 0.684, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [300/665], Loss: 0.615, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [400/665], Loss: 0.810, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [500/665], Loss: 0.788, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [600/665], Loss: 0.586, Training Accuracy: 84.24%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 45.81507653869819 %\n",
      "Epoch [4/60], Step [100/665], Loss: 0.635, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [200/665], Loss: 0.697, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [300/665], Loss: 0.718, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [400/665], Loss: 0.713, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [500/665], Loss: 0.710, Training Accuracy: 73.37%\n",
      "Epoch [4/60], Step [600/665], Loss: 0.623, Training Accuracy: 82.61%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 49.02205731545877 %\n",
      "Epoch [5/60], Step [100/665], Loss: 0.651, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [200/665], Loss: 0.615, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [300/665], Loss: 0.692, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [400/665], Loss: 0.624, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [500/665], Loss: 0.659, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [600/665], Loss: 0.568, Training Accuracy: 83.15%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 47.053120280424366 %\n",
      "Epoch [6/60], Step [100/665], Loss: 0.516, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [200/665], Loss: 0.600, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [300/665], Loss: 0.631, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [400/665], Loss: 0.581, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [500/665], Loss: 0.567, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [600/665], Loss: 0.555, Training Accuracy: 80.98%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 43.42662166949452 %\n",
      "Epoch [7/60], Step [100/665], Loss: 0.573, Training Accuracy: 76.63%\n",
      "Epoch [7/60], Step [200/665], Loss: 0.619, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [300/665], Loss: 0.555, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [400/665], Loss: 0.654, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [500/665], Loss: 0.466, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [600/665], Loss: 0.407, Training Accuracy: 88.59%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 48.16996998116831 %\n",
      "Epoch [8/60], Step [100/665], Loss: 0.536, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [200/665], Loss: 0.582, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [300/665], Loss: 0.548, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [400/665], Loss: 0.481, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [500/665], Loss: 0.576, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [600/665], Loss: 0.474, Training Accuracy: 83.70%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 48.7889918520314 %\n",
      "Epoch [9/60], Step [100/665], Loss: 0.507, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [200/665], Loss: 0.536, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [300/665], Loss: 0.499, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [400/665], Loss: 0.460, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [500/665], Loss: 0.517, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [600/665], Loss: 0.487, Training Accuracy: 83.70%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 49.726847276863126 %\n",
      "Epoch [10/60], Step [100/665], Loss: 0.433, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [200/665], Loss: 0.398, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [300/665], Loss: 0.439, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [400/665], Loss: 0.547, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [500/665], Loss: 0.537, Training Accuracy: 82.07%\n",
      "Epoch [10/60], Step [600/665], Loss: 0.533, Training Accuracy: 78.26%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 49.989745119609196 %\n",
      "Epoch [11/60], Step [100/665], Loss: 0.467, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [200/665], Loss: 0.619, Training Accuracy: 78.26%\n",
      "Epoch [11/60], Step [300/665], Loss: 0.479, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [400/665], Loss: 0.463, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [500/665], Loss: 0.431, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [600/665], Loss: 0.446, Training Accuracy: 85.33%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 50.99845244532284 %\n",
      "Epoch [12/60], Step [100/665], Loss: 0.428, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [200/665], Loss: 0.385, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [300/665], Loss: 0.377, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [400/665], Loss: 0.561, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [500/665], Loss: 0.395, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [600/665], Loss: 0.473, Training Accuracy: 83.15%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 50.748606268528704 %\n",
      "Epoch [13/60], Step [100/665], Loss: 0.499, Training Accuracy: 86.41%\n",
      "Epoch [13/60], Step [200/665], Loss: 0.646, Training Accuracy: 78.26%\n",
      "Epoch [13/60], Step [300/665], Loss: 0.357, Training Accuracy: 86.41%\n",
      "Epoch [13/60], Step [400/665], Loss: 0.456, Training Accuracy: 86.41%\n",
      "Epoch [13/60], Step [500/665], Loss: 0.386, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [600/665], Loss: 0.489, Training Accuracy: 84.78%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 49.868551078626965 %\n",
      "Epoch [14/60], Step [100/665], Loss: 0.371, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [200/665], Loss: 0.401, Training Accuracy: 84.24%\n",
      "Epoch [14/60], Step [300/665], Loss: 0.355, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [400/665], Loss: 0.419, Training Accuracy: 85.33%\n",
      "Epoch [14/60], Step [500/665], Loss: 0.471, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [600/665], Loss: 0.423, Training Accuracy: 86.96%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 50.726231984039686 %\n",
      "Epoch [15/60], Step [100/665], Loss: 0.457, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [200/665], Loss: 0.386, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [300/665], Loss: 0.475, Training Accuracy: 86.41%\n",
      "Epoch [15/60], Step [400/665], Loss: 0.487, Training Accuracy: 85.87%\n",
      "Epoch [15/60], Step [500/665], Loss: 0.468, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [600/665], Loss: 0.377, Training Accuracy: 88.59%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 49.68209870788507 %\n",
      "Epoch [16/60], Step [100/665], Loss: 0.377, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [200/665], Loss: 0.451, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [300/665], Loss: 0.393, Training Accuracy: 90.76%\n",
      "Epoch [16/60], Step [400/665], Loss: 0.424, Training Accuracy: 84.78%\n",
      "Epoch [16/60], Step [500/665], Loss: 0.421, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [600/665], Loss: 0.411, Training Accuracy: 89.67%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 49.76040870359667 %\n",
      "Finished early after 16 epochs!\n",
      "Final Training Accuracy 87.5918854610422 %\n",
      "Final Test Accuracy 49.79024108291537 %\n",
      "Epoch [1/60], Step [100/575], Loss: 1.183, Training Accuracy: 52.72%\n",
      "Epoch [1/60], Step [200/575], Loss: 1.133, Training Accuracy: 53.26%\n",
      "Epoch [1/60], Step [300/575], Loss: 0.959, Training Accuracy: 59.78%\n",
      "Epoch [1/60], Step [400/575], Loss: 0.964, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [500/575], Loss: 0.990, Training Accuracy: 59.78%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 54.69128415534092 %\n",
      "Epoch [2/60], Step [100/575], Loss: 0.828, Training Accuracy: 59.24%\n",
      "Epoch [2/60], Step [200/575], Loss: 0.824, Training Accuracy: 58.70%\n",
      "Epoch [2/60], Step [300/575], Loss: 0.828, Training Accuracy: 60.33%\n",
      "Epoch [2/60], Step [400/575], Loss: 0.851, Training Accuracy: 60.87%\n",
      "Epoch [2/60], Step [500/575], Loss: 0.674, Training Accuracy: 72.83%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 57.03763854460495 %\n",
      "Epoch [3/60], Step [100/575], Loss: 0.673, Training Accuracy: 65.22%\n",
      "Epoch [3/60], Step [200/575], Loss: 0.765, Training Accuracy: 65.22%\n",
      "Epoch [3/60], Step [300/575], Loss: 0.715, Training Accuracy: 70.65%\n",
      "Epoch [3/60], Step [400/575], Loss: 0.695, Training Accuracy: 70.65%\n",
      "Epoch [3/60], Step [500/575], Loss: 0.627, Training Accuracy: 76.09%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 57.66589737014559 %\n",
      "Epoch [4/60], Step [100/575], Loss: 0.485, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [200/575], Loss: 0.529, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [300/575], Loss: 0.579, Training Accuracy: 69.02%\n",
      "Epoch [4/60], Step [400/575], Loss: 0.565, Training Accuracy: 72.83%\n",
      "Epoch [4/60], Step [500/575], Loss: 0.440, Training Accuracy: 76.63%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 58.23004815226373 %\n",
      "Epoch [5/60], Step [100/575], Loss: 0.560, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [200/575], Loss: 0.561, Training Accuracy: 70.65%\n",
      "Epoch [5/60], Step [300/575], Loss: 0.691, Training Accuracy: 72.83%\n",
      "Epoch [5/60], Step [400/575], Loss: 0.649, Training Accuracy: 70.65%\n",
      "Epoch [5/60], Step [500/575], Loss: 0.582, Training Accuracy: 73.37%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 57.955095877140494 %\n",
      "Epoch [6/60], Step [100/575], Loss: 0.440, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [200/575], Loss: 0.474, Training Accuracy: 79.35%\n",
      "Epoch [6/60], Step [300/575], Loss: 0.612, Training Accuracy: 72.28%\n",
      "Epoch [6/60], Step [400/575], Loss: 0.634, Training Accuracy: 72.83%\n",
      "Epoch [6/60], Step [500/575], Loss: 0.563, Training Accuracy: 76.63%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 58.70587229677751 %\n",
      "Epoch [7/60], Step [100/575], Loss: 0.436, Training Accuracy: 77.17%\n",
      "Epoch [7/60], Step [200/575], Loss: 0.647, Training Accuracy: 77.17%\n",
      "Epoch [7/60], Step [300/575], Loss: 0.468, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [400/575], Loss: 0.444, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [500/575], Loss: 0.540, Training Accuracy: 79.89%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 61.49955836681198 %\n",
      "Epoch [8/60], Step [100/575], Loss: 0.404, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [200/575], Loss: 0.459, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [300/575], Loss: 0.539, Training Accuracy: 73.37%\n",
      "Epoch [8/60], Step [400/575], Loss: 0.414, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [500/575], Loss: 0.398, Training Accuracy: 82.61%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 61.10351312077955 %\n",
      "Epoch [9/60], Step [100/575], Loss: 0.432, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [200/575], Loss: 0.553, Training Accuracy: 76.63%\n",
      "Epoch [9/60], Step [300/575], Loss: 0.374, Training Accuracy: 80.98%\n",
      "Epoch [9/60], Step [400/575], Loss: 0.457, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [500/575], Loss: 0.453, Training Accuracy: 83.70%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 60.3156964982762 %\n",
      "Epoch [10/60], Step [100/575], Loss: 0.474, Training Accuracy: 77.72%\n",
      "Epoch [10/60], Step [200/575], Loss: 0.368, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [300/575], Loss: 0.351, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [400/575], Loss: 0.423, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [500/575], Loss: 0.410, Training Accuracy: 80.98%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 61.26164629455509 %\n",
      "Epoch [11/60], Step [100/575], Loss: 0.361, Training Accuracy: 80.43%\n",
      "Epoch [11/60], Step [200/575], Loss: 0.447, Training Accuracy: 79.35%\n",
      "Epoch [11/60], Step [300/575], Loss: 0.508, Training Accuracy: 75.54%\n",
      "Epoch [11/60], Step [400/575], Loss: 0.278, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [500/575], Loss: 0.410, Training Accuracy: 81.52%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 60.28720403453287 %\n",
      "Epoch [12/60], Step [100/575], Loss: 0.551, Training Accuracy: 81.52%\n",
      "Epoch [12/60], Step [200/575], Loss: 0.460, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [300/575], Loss: 0.440, Training Accuracy: 78.26%\n",
      "Epoch [12/60], Step [400/575], Loss: 0.418, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [500/575], Loss: 0.454, Training Accuracy: 82.07%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 61.056500555603044 %\n",
      "Finished early after 12 epochs!\n",
      "Final Training Accuracy 82.66942820934763 %\n",
      "Final Test Accuracy 60.97244778756019 %\n",
      "Epoch [1/60], Step [100/692], Loss: 1.313, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [200/692], Loss: 1.285, Training Accuracy: 51.09%\n",
      "Epoch [1/60], Step [300/692], Loss: 1.012, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [400/692], Loss: 1.134, Training Accuracy: 60.87%\n",
      "Epoch [1/60], Step [500/692], Loss: 0.988, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [600/692], Loss: 1.003, Training Accuracy: 72.28%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 49.83476672345491 %\n",
      "Epoch [2/60], Step [100/692], Loss: 0.896, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [200/692], Loss: 0.867, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [300/692], Loss: 0.830, Training Accuracy: 70.65%\n",
      "Epoch [2/60], Step [400/692], Loss: 0.887, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [500/692], Loss: 0.872, Training Accuracy: 67.39%\n",
      "Epoch [2/60], Step [600/692], Loss: 0.852, Training Accuracy: 72.83%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 47.00219626839631 %\n",
      "Epoch [3/60], Step [100/692], Loss: 0.894, Training Accuracy: 71.20%\n",
      "Epoch [3/60], Step [200/692], Loss: 0.832, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [300/692], Loss: 0.742, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [400/692], Loss: 0.681, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [500/692], Loss: 0.695, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [600/692], Loss: 0.818, Training Accuracy: 77.72%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 55.3254377142388 %\n",
      "Epoch [4/60], Step [100/692], Loss: 0.796, Training Accuracy: 73.37%\n",
      "Epoch [4/60], Step [200/692], Loss: 0.745, Training Accuracy: 76.09%\n",
      "Epoch [4/60], Step [300/692], Loss: 0.670, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [400/692], Loss: 0.615, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [500/692], Loss: 0.589, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [600/692], Loss: 0.608, Training Accuracy: 82.07%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 53.46579363287424 %\n",
      "Epoch [5/60], Step [100/692], Loss: 0.665, Training Accuracy: 73.91%\n",
      "Epoch [5/60], Step [200/692], Loss: 0.706, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [300/692], Loss: 0.574, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [400/692], Loss: 0.640, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [500/692], Loss: 0.636, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [600/692], Loss: 0.556, Training Accuracy: 86.96%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 54.37714238798005 %\n",
      "Epoch [6/60], Step [100/692], Loss: 0.613, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [200/692], Loss: 0.648, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [300/692], Loss: 0.550, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [400/692], Loss: 0.648, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [500/692], Loss: 0.586, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [600/692], Loss: 0.603, Training Accuracy: 82.07%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 55.59227406145446 %\n",
      "Epoch [7/60], Step [100/692], Loss: 0.666, Training Accuracy: 77.72%\n",
      "Epoch [7/60], Step [200/692], Loss: 0.594, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [300/692], Loss: 0.568, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [400/692], Loss: 0.520, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [500/692], Loss: 0.462, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [600/692], Loss: 0.644, Training Accuracy: 80.43%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 58.53773681725816 %\n",
      "Epoch [8/60], Step [100/692], Loss: 0.532, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [200/692], Loss: 0.729, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [300/692], Loss: 0.551, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [400/692], Loss: 0.615, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [500/692], Loss: 0.593, Training Accuracy: 76.09%\n",
      "Epoch [8/60], Step [600/692], Loss: 0.581, Training Accuracy: 81.52%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 54.68708306820748 %\n",
      "Epoch [9/60], Step [100/692], Loss: 0.649, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [200/692], Loss: 0.559, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [300/692], Loss: 0.487, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [400/692], Loss: 0.635, Training Accuracy: 80.43%\n",
      "Epoch [9/60], Step [500/692], Loss: 0.420, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [600/692], Loss: 0.510, Training Accuracy: 82.07%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 53.61563250477227 %\n",
      "Epoch [10/60], Step [100/692], Loss: 0.601, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [200/692], Loss: 0.503, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [300/692], Loss: 0.596, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [400/692], Loss: 0.515, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [500/692], Loss: 0.531, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [600/692], Loss: 0.438, Training Accuracy: 87.50%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 51.85656520043514 %\n",
      "Epoch [11/60], Step [100/692], Loss: 0.510, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [200/692], Loss: 0.476, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [300/692], Loss: 0.478, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [400/692], Loss: 0.431, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [500/692], Loss: 0.510, Training Accuracy: 83.15%\n",
      "Epoch [11/60], Step [600/692], Loss: 0.365, Training Accuracy: 89.13%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 54.978550462858436 %\n",
      "Epoch [12/60], Step [100/692], Loss: 0.497, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [200/692], Loss: 0.450, Training Accuracy: 83.70%\n",
      "Epoch [12/60], Step [300/692], Loss: 0.374, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [400/692], Loss: 0.467, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [500/692], Loss: 0.425, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [600/692], Loss: 0.509, Training Accuracy: 82.61%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 53.749050678380094 %\n",
      "Finished early after 12 epochs!\n",
      "Final Training Accuracy 85.22006398717113 %\n",
      "Final Test Accuracy 53.78599724953303 %\n",
      "Epoch [1/60], Step [100/647], Loss: 1.198, Training Accuracy: 49.46%\n",
      "Epoch [1/60], Step [200/647], Loss: 1.083, Training Accuracy: 52.17%\n",
      "Epoch [1/60], Step [300/647], Loss: 1.059, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [400/647], Loss: 0.940, Training Accuracy: 63.59%\n",
      "Epoch [1/60], Step [500/647], Loss: 0.985, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [600/647], Loss: 0.904, Training Accuracy: 69.02%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 37.177081323769265 %\n",
      "Epoch [2/60], Step [100/647], Loss: 0.860, Training Accuracy: 71.20%\n",
      "Epoch [2/60], Step [200/647], Loss: 0.946, Training Accuracy: 62.50%\n",
      "Epoch [2/60], Step [300/647], Loss: 0.789, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [400/647], Loss: 0.821, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [500/647], Loss: 0.740, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [600/647], Loss: 0.843, Training Accuracy: 71.74%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 40.3917992248198 %\n",
      "Epoch [3/60], Step [100/647], Loss: 0.686, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [200/647], Loss: 0.879, Training Accuracy: 65.76%\n",
      "Epoch [3/60], Step [300/647], Loss: 0.769, Training Accuracy: 68.48%\n",
      "Epoch [3/60], Step [400/647], Loss: 0.626, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [500/647], Loss: 0.790, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [600/647], Loss: 0.698, Training Accuracy: 71.74%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 46.342447254423966 %\n",
      "Epoch [4/60], Step [100/647], Loss: 0.709, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [200/647], Loss: 0.692, Training Accuracy: 73.37%\n",
      "Epoch [4/60], Step [300/647], Loss: 0.721, Training Accuracy: 76.63%\n",
      "Epoch [4/60], Step [400/647], Loss: 0.767, Training Accuracy: 73.91%\n",
      "Epoch [4/60], Step [500/647], Loss: 0.678, Training Accuracy: 75.54%\n",
      "Epoch [4/60], Step [600/647], Loss: 0.814, Training Accuracy: 76.63%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 46.88963328013469 %\n",
      "Epoch [5/60], Step [100/647], Loss: 0.699, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [200/647], Loss: 0.557, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [300/647], Loss: 0.754, Training Accuracy: 70.65%\n",
      "Epoch [5/60], Step [400/647], Loss: 0.700, Training Accuracy: 73.91%\n",
      "Epoch [5/60], Step [500/647], Loss: 0.716, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [600/647], Loss: 0.582, Training Accuracy: 81.52%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 50.03770672933584 %\n",
      "Epoch [6/60], Step [100/647], Loss: 0.572, Training Accuracy: 77.17%\n",
      "Epoch [6/60], Step [200/647], Loss: 0.603, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [300/647], Loss: 0.604, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [400/647], Loss: 0.664, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [500/647], Loss: 0.533, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [600/647], Loss: 0.583, Training Accuracy: 82.61%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 51.370595766323504 %\n",
      "Epoch [7/60], Step [100/647], Loss: 0.502, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [200/647], Loss: 0.693, Training Accuracy: 76.63%\n",
      "Epoch [7/60], Step [300/647], Loss: 0.608, Training Accuracy: 77.72%\n",
      "Epoch [7/60], Step [400/647], Loss: 0.635, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [500/647], Loss: 0.653, Training Accuracy: 77.72%\n",
      "Epoch [7/60], Step [600/647], Loss: 0.709, Training Accuracy: 75.00%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 52.66840877602203 %\n",
      "Epoch [8/60], Step [100/647], Loss: 0.504, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [200/647], Loss: 0.614, Training Accuracy: 79.89%\n",
      "Epoch [8/60], Step [300/647], Loss: 0.609, Training Accuracy: 78.80%\n",
      "Epoch [8/60], Step [400/647], Loss: 0.510, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [500/647], Loss: 0.412, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [600/647], Loss: 0.579, Training Accuracy: 73.91%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 52.11596134621793 %\n",
      "Epoch [9/60], Step [100/647], Loss: 0.577, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [200/647], Loss: 0.558, Training Accuracy: 76.63%\n",
      "Epoch [9/60], Step [300/647], Loss: 0.687, Training Accuracy: 76.63%\n",
      "Epoch [9/60], Step [400/647], Loss: 0.506, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [500/647], Loss: 0.464, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [600/647], Loss: 0.417, Training Accuracy: 84.24%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 51.98442624388362 %\n",
      "Epoch [10/60], Step [100/647], Loss: 0.532, Training Accuracy: 79.89%\n",
      "Epoch [10/60], Step [200/647], Loss: 0.479, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [300/647], Loss: 0.510, Training Accuracy: 81.52%\n",
      "Epoch [10/60], Step [400/647], Loss: 0.482, Training Accuracy: 78.80%\n",
      "Epoch [10/60], Step [500/647], Loss: 0.564, Training Accuracy: 78.80%\n",
      "Epoch [10/60], Step [600/647], Loss: 0.476, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 54.069696066223536 %\n",
      "Epoch [11/60], Step [100/647], Loss: 0.558, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [200/647], Loss: 0.431, Training Accuracy: 80.43%\n",
      "Epoch [11/60], Step [300/647], Loss: 0.559, Training Accuracy: 77.17%\n",
      "Epoch [11/60], Step [400/647], Loss: 0.428, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [500/647], Loss: 0.570, Training Accuracy: 82.61%\n",
      "Epoch [11/60], Step [600/647], Loss: 0.540, Training Accuracy: 82.07%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 56.0216769848647 %\n",
      "Epoch [12/60], Step [100/647], Loss: 0.443, Training Accuracy: 83.15%\n",
      "Epoch [12/60], Step [200/647], Loss: 0.400, Training Accuracy: 87.50%\n",
      "Epoch [12/60], Step [300/647], Loss: 0.509, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [400/647], Loss: 0.519, Training Accuracy: 83.15%\n",
      "Epoch [12/60], Step [500/647], Loss: 0.559, Training Accuracy: 79.35%\n",
      "Epoch [12/60], Step [600/647], Loss: 0.551, Training Accuracy: 82.07%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 54.63442010557884 %\n",
      "Epoch [13/60], Step [100/647], Loss: 0.473, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [200/647], Loss: 0.408, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [300/647], Loss: 0.454, Training Accuracy: 85.33%\n",
      "Epoch [13/60], Step [400/647], Loss: 0.417, Training Accuracy: 81.52%\n",
      "Epoch [13/60], Step [500/647], Loss: 0.518, Training Accuracy: 81.52%\n",
      "Epoch [13/60], Step [600/647], Loss: 0.479, Training Accuracy: 85.33%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 55.89891088935267 %\n",
      "Epoch [14/60], Step [100/647], Loss: 0.478, Training Accuracy: 80.98%\n",
      "Epoch [14/60], Step [200/647], Loss: 0.333, Training Accuracy: 83.70%\n",
      "Epoch [14/60], Step [300/647], Loss: 0.444, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [400/647], Loss: 0.404, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [500/647], Loss: 0.479, Training Accuracy: 79.89%\n",
      "Epoch [14/60], Step [600/647], Loss: 0.445, Training Accuracy: 79.89%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 53.33660709588032 %\n",
      "Epoch [15/60], Step [100/647], Loss: 0.449, Training Accuracy: 86.41%\n",
      "Epoch [15/60], Step [200/647], Loss: 0.324, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [300/647], Loss: 0.452, Training Accuracy: 79.89%\n",
      "Epoch [15/60], Step [400/647], Loss: 0.470, Training Accuracy: 80.98%\n",
      "Epoch [15/60], Step [500/647], Loss: 0.550, Training Accuracy: 84.78%\n",
      "Epoch [15/60], Step [600/647], Loss: 0.389, Training Accuracy: 88.04%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 57.740402322033 %\n",
      "Epoch [16/60], Step [100/647], Loss: 0.525, Training Accuracy: 82.07%\n",
      "Epoch [16/60], Step [200/647], Loss: 0.586, Training Accuracy: 79.89%\n",
      "Epoch [16/60], Step [300/647], Loss: 0.396, Training Accuracy: 85.87%\n",
      "Epoch [16/60], Step [400/647], Loss: 0.325, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [500/647], Loss: 0.372, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [600/647], Loss: 0.494, Training Accuracy: 86.41%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 55.64460969150634 %\n",
      "Epoch [17/60], Step [100/647], Loss: 0.477, Training Accuracy: 84.24%\n",
      "Epoch [17/60], Step [200/647], Loss: 0.431, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [300/647], Loss: 0.413, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [400/647], Loss: 0.454, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [500/647], Loss: 0.494, Training Accuracy: 84.24%\n",
      "Epoch [17/60], Step [600/647], Loss: 0.318, Training Accuracy: 86.96%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 57.25810694680721 %\n",
      "Epoch [18/60], Step [100/647], Loss: 0.401, Training Accuracy: 86.41%\n",
      "Epoch [18/60], Step [200/647], Loss: 0.439, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [300/647], Loss: 0.400, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [400/647], Loss: 0.335, Training Accuracy: 88.04%\n",
      "Epoch [18/60], Step [500/647], Loss: 0.469, Training Accuracy: 79.89%\n",
      "Epoch [18/60], Step [600/647], Loss: 0.396, Training Accuracy: 86.41%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 58.022764341710655 %\n",
      "Epoch [19/60], Step [100/647], Loss: 0.314, Training Accuracy: 88.59%\n",
      "Epoch [19/60], Step [200/647], Loss: 0.413, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [300/647], Loss: 0.401, Training Accuracy: 83.70%\n",
      "Epoch [19/60], Step [400/647], Loss: 0.387, Training Accuracy: 89.13%\n",
      "Epoch [19/60], Step [500/647], Loss: 0.469, Training Accuracy: 85.87%\n",
      "Epoch [19/60], Step [600/647], Loss: 0.346, Training Accuracy: 89.67%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 55.977831950753256 %\n",
      "Epoch [20/60], Step [100/647], Loss: 0.448, Training Accuracy: 84.24%\n",
      "Epoch [20/60], Step [200/647], Loss: 0.376, Training Accuracy: 84.78%\n",
      "Epoch [20/60], Step [300/647], Loss: 0.443, Training Accuracy: 85.87%\n",
      "Epoch [20/60], Step [400/647], Loss: 0.458, Training Accuracy: 84.24%\n",
      "Epoch [20/60], Step [500/647], Loss: 0.350, Training Accuracy: 84.78%\n",
      "Epoch [20/60], Step [600/647], Loss: 0.343, Training Accuracy: 88.04%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 57.619390027885444 %\n",
      "Epoch [21/60], Step [100/647], Loss: 0.356, Training Accuracy: 86.96%\n",
      "Epoch [21/60], Step [200/647], Loss: 0.388, Training Accuracy: 85.33%\n",
      "Epoch [21/60], Step [300/647], Loss: 0.249, Training Accuracy: 94.02%\n",
      "Epoch [21/60], Step [400/647], Loss: 0.395, Training Accuracy: 83.15%\n",
      "Epoch [21/60], Step [500/647], Loss: 0.439, Training Accuracy: 85.87%\n",
      "Epoch [21/60], Step [600/647], Loss: 0.339, Training Accuracy: 89.13%\n",
      "Testing epoch 21\n",
      "Epoch 21: Test Accuracy 57.293182974096354 %\n",
      "Epoch [22/60], Step [100/647], Loss: 0.377, Training Accuracy: 86.41%\n",
      "Epoch [22/60], Step [200/647], Loss: 0.302, Training Accuracy: 89.13%\n",
      "Epoch [22/60], Step [300/647], Loss: 0.378, Training Accuracy: 86.96%\n",
      "Epoch [22/60], Step [400/647], Loss: 0.421, Training Accuracy: 88.04%\n",
      "Epoch [22/60], Step [500/647], Loss: 0.233, Training Accuracy: 88.59%\n",
      "Epoch [22/60], Step [600/647], Loss: 0.426, Training Accuracy: 79.89%\n",
      "Testing epoch 22\n",
      "Epoch 22: Test Accuracy 58.40333923779792 %\n",
      "Epoch [23/60], Step [100/647], Loss: 0.390, Training Accuracy: 89.13%\n",
      "Epoch [23/60], Step [200/647], Loss: 0.300, Training Accuracy: 89.67%\n",
      "Epoch [23/60], Step [300/647], Loss: 0.348, Training Accuracy: 90.76%\n",
      "Epoch [23/60], Step [400/647], Loss: 0.385, Training Accuracy: 88.04%\n",
      "Epoch [23/60], Step [500/647], Loss: 0.301, Training Accuracy: 90.76%\n",
      "Epoch [23/60], Step [600/647], Loss: 0.501, Training Accuracy: 82.07%\n",
      "Testing epoch 23\n",
      "Epoch 23: Test Accuracy 59.662568617478385 %\n",
      "Epoch [24/60], Step [100/647], Loss: 0.311, Training Accuracy: 90.76%\n",
      "Epoch [24/60], Step [200/647], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [24/60], Step [300/647], Loss: 0.380, Training Accuracy: 89.13%\n",
      "Epoch [24/60], Step [400/647], Loss: 0.326, Training Accuracy: 86.41%\n",
      "Epoch [24/60], Step [500/647], Loss: 0.476, Training Accuracy: 80.98%\n",
      "Epoch [24/60], Step [600/647], Loss: 0.352, Training Accuracy: 84.24%\n",
      "Testing epoch 24\n",
      "Epoch 24: Test Accuracy 59.37669899507182 %\n",
      "Epoch [25/60], Step [100/647], Loss: 0.274, Training Accuracy: 89.67%\n",
      "Epoch [25/60], Step [200/647], Loss: 0.357, Training Accuracy: 87.50%\n",
      "Epoch [25/60], Step [300/647], Loss: 0.578, Training Accuracy: 77.72%\n",
      "Epoch [25/60], Step [400/647], Loss: 0.319, Training Accuracy: 87.50%\n",
      "Epoch [25/60], Step [500/647], Loss: 0.391, Training Accuracy: 87.50%\n",
      "Epoch [25/60], Step [600/647], Loss: 0.356, Training Accuracy: 88.59%\n",
      "Testing epoch 25\n",
      "Epoch 25: Test Accuracy 58.55241235377681 %\n",
      "Epoch [26/60], Step [100/647], Loss: 0.394, Training Accuracy: 85.33%\n",
      "Epoch [26/60], Step [200/647], Loss: 0.437, Training Accuracy: 82.61%\n",
      "Epoch [26/60], Step [300/647], Loss: 0.362, Training Accuracy: 85.33%\n",
      "Epoch [26/60], Step [400/647], Loss: 0.482, Training Accuracy: 83.70%\n",
      "Epoch [26/60], Step [500/647], Loss: 0.346, Training Accuracy: 89.13%\n",
      "Epoch [26/60], Step [600/647], Loss: 0.357, Training Accuracy: 88.59%\n",
      "Testing epoch 26\n",
      "Epoch 26: Test Accuracy 59.027692523544786 %\n",
      "Epoch [27/60], Step [100/647], Loss: 0.283, Training Accuracy: 89.13%\n",
      "Epoch [27/60], Step [200/647], Loss: 0.367, Training Accuracy: 88.04%\n",
      "Epoch [27/60], Step [300/647], Loss: 0.275, Training Accuracy: 89.67%\n",
      "Epoch [27/60], Step [400/647], Loss: 0.382, Training Accuracy: 85.33%\n",
      "Epoch [27/60], Step [500/647], Loss: 0.324, Training Accuracy: 87.50%\n",
      "Epoch [27/60], Step [600/647], Loss: 0.328, Training Accuracy: 86.41%\n",
      "Testing epoch 27\n",
      "Epoch 27: Test Accuracy 61.926726178992965 %\n",
      "Epoch [28/60], Step [100/647], Loss: 0.344, Training Accuracy: 85.33%\n",
      "Epoch [28/60], Step [200/647], Loss: 0.220, Training Accuracy: 91.30%\n",
      "Epoch [28/60], Step [300/647], Loss: 0.417, Training Accuracy: 85.87%\n",
      "Epoch [28/60], Step [400/647], Loss: 0.353, Training Accuracy: 90.76%\n",
      "Epoch [28/60], Step [500/647], Loss: 0.312, Training Accuracy: 88.04%\n",
      "Epoch [28/60], Step [600/647], Loss: 0.278, Training Accuracy: 91.30%\n",
      "Testing epoch 28\n",
      "Epoch 28: Test Accuracy 62.556340868833196 %\n",
      "Epoch [29/60], Step [100/647], Loss: 0.363, Training Accuracy: 90.76%\n",
      "Epoch [29/60], Step [200/647], Loss: 0.217, Training Accuracy: 94.02%\n",
      "Epoch [29/60], Step [300/647], Loss: 0.339, Training Accuracy: 88.04%\n",
      "Epoch [29/60], Step [400/647], Loss: 0.369, Training Accuracy: 83.70%\n",
      "Epoch [29/60], Step [500/647], Loss: 0.338, Training Accuracy: 86.41%\n",
      "Epoch [29/60], Step [600/647], Loss: 0.425, Training Accuracy: 85.87%\n",
      "Testing epoch 29\n",
      "Epoch 29: Test Accuracy 59.287255125484485 %\n",
      "Epoch [30/60], Step [100/647], Loss: 0.332, Training Accuracy: 89.67%\n",
      "Epoch [30/60], Step [200/647], Loss: 0.292, Training Accuracy: 88.59%\n",
      "Epoch [30/60], Step [300/647], Loss: 0.335, Training Accuracy: 90.22%\n",
      "Epoch [30/60], Step [400/647], Loss: 0.334, Training Accuracy: 87.50%\n",
      "Epoch [30/60], Step [500/647], Loss: 0.480, Training Accuracy: 83.70%\n",
      "Epoch [30/60], Step [600/647], Loss: 0.339, Training Accuracy: 90.22%\n",
      "Testing epoch 30\n",
      "Epoch 30: Test Accuracy 59.33460776232484 %\n",
      "Epoch [31/60], Step [100/647], Loss: 0.233, Training Accuracy: 92.39%\n",
      "Epoch [31/60], Step [200/647], Loss: 0.356, Training Accuracy: 85.87%\n",
      "Epoch [31/60], Step [300/647], Loss: 0.242, Training Accuracy: 91.85%\n",
      "Epoch [31/60], Step [400/647], Loss: 0.253, Training Accuracy: 91.30%\n",
      "Epoch [31/60], Step [500/647], Loss: 0.440, Training Accuracy: 86.41%\n",
      "Epoch [31/60], Step [600/647], Loss: 0.424, Training Accuracy: 88.04%\n",
      "Testing epoch 31\n",
      "Epoch 31: Test Accuracy 60.20800084182466 %\n",
      "Epoch [32/60], Step [100/647], Loss: 0.256, Training Accuracy: 91.85%\n",
      "Epoch [32/60], Step [200/647], Loss: 0.391, Training Accuracy: 86.96%\n",
      "Epoch [32/60], Step [300/647], Loss: 0.329, Training Accuracy: 89.13%\n",
      "Epoch [32/60], Step [400/647], Loss: 0.289, Training Accuracy: 91.85%\n",
      "Epoch [32/60], Step [500/647], Loss: 0.244, Training Accuracy: 91.85%\n",
      "Epoch [32/60], Step [600/647], Loss: 0.276, Training Accuracy: 90.22%\n",
      "Testing epoch 32\n",
      "Epoch 32: Test Accuracy 59.587155158806716 %\n",
      "Epoch [33/60], Step [100/647], Loss: 0.219, Training Accuracy: 92.39%\n",
      "Epoch [33/60], Step [200/647], Loss: 0.233, Training Accuracy: 90.22%\n",
      "Epoch [33/60], Step [300/647], Loss: 0.248, Training Accuracy: 91.85%\n",
      "Epoch [33/60], Step [400/647], Loss: 0.412, Training Accuracy: 83.70%\n",
      "Epoch [33/60], Step [500/647], Loss: 0.287, Training Accuracy: 88.59%\n",
      "Epoch [33/60], Step [600/647], Loss: 0.279, Training Accuracy: 87.50%\n",
      "Testing epoch 33\n",
      "Epoch 33: Test Accuracy 59.37669899507182 %\n",
      "Finished early after 33 epochs!\n",
      "Final Training Accuracy 88.46635775735201 %\n",
      "Final Test Accuracy 59.332853960960385 %\n",
      "Epoch [1/60], Step [100/626], Loss: 1.261, Training Accuracy: 52.17%\n",
      "Epoch [1/60], Step [200/626], Loss: 1.094, Training Accuracy: 57.61%\n",
      "Epoch [1/60], Step [300/626], Loss: 1.108, Training Accuracy: 54.89%\n",
      "Epoch [1/60], Step [400/626], Loss: 0.892, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [500/626], Loss: 0.971, Training Accuracy: 61.41%\n",
      "Epoch [1/60], Step [600/626], Loss: 0.931, Training Accuracy: 65.76%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 46.41502618746613 %\n",
      "Epoch [2/60], Step [100/626], Loss: 0.786, Training Accuracy: 69.57%\n",
      "Epoch [2/60], Step [200/626], Loss: 0.835, Training Accuracy: 69.57%\n",
      "Epoch [2/60], Step [300/626], Loss: 0.757, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [400/626], Loss: 0.655, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [500/626], Loss: 0.709, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [600/626], Loss: 0.734, Training Accuracy: 78.26%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 48.470619140657064 %\n",
      "Epoch [3/60], Step [100/626], Loss: 0.793, Training Accuracy: 67.93%\n",
      "Epoch [3/60], Step [200/626], Loss: 0.694, Training Accuracy: 71.20%\n",
      "Epoch [3/60], Step [300/626], Loss: 0.728, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [400/626], Loss: 0.673, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [500/626], Loss: 0.691, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [600/626], Loss: 0.750, Training Accuracy: 72.83%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 52.78539412546998 %\n",
      "Epoch [4/60], Step [100/626], Loss: 0.655, Training Accuracy: 75.00%\n",
      "Epoch [4/60], Step [200/626], Loss: 0.758, Training Accuracy: 73.91%\n",
      "Epoch [4/60], Step [300/626], Loss: 0.658, Training Accuracy: 76.09%\n",
      "Epoch [4/60], Step [400/626], Loss: 0.677, Training Accuracy: 71.74%\n",
      "Epoch [4/60], Step [500/626], Loss: 0.566, Training Accuracy: 75.00%\n",
      "Epoch [4/60], Step [600/626], Loss: 0.622, Training Accuracy: 73.37%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 55.66519447682532 %\n",
      "Epoch [5/60], Step [100/626], Loss: 0.684, Training Accuracy: 75.54%\n",
      "Epoch [5/60], Step [200/626], Loss: 0.662, Training Accuracy: 75.00%\n",
      "Epoch [5/60], Step [300/626], Loss: 0.616, Training Accuracy: 72.28%\n",
      "Epoch [5/60], Step [400/626], Loss: 0.669, Training Accuracy: 75.54%\n",
      "Epoch [5/60], Step [500/626], Loss: 0.624, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [600/626], Loss: 0.588, Training Accuracy: 77.17%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 56.43850460538198 %\n",
      "Epoch [6/60], Step [100/626], Loss: 0.605, Training Accuracy: 79.89%\n",
      "Epoch [6/60], Step [200/626], Loss: 0.627, Training Accuracy: 79.89%\n",
      "Epoch [6/60], Step [300/626], Loss: 0.565, Training Accuracy: 74.46%\n",
      "Epoch [6/60], Step [400/626], Loss: 0.514, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [500/626], Loss: 0.756, Training Accuracy: 73.37%\n",
      "Epoch [6/60], Step [600/626], Loss: 0.521, Training Accuracy: 82.61%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 56.92941697998588 %\n",
      "Epoch [7/60], Step [100/626], Loss: 0.578, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [200/626], Loss: 0.570, Training Accuracy: 78.26%\n",
      "Epoch [7/60], Step [300/626], Loss: 0.618, Training Accuracy: 77.17%\n",
      "Epoch [7/60], Step [400/626], Loss: 0.536, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [500/626], Loss: 0.459, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [600/626], Loss: 0.609, Training Accuracy: 74.46%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 58.20677426240005 %\n",
      "Epoch [8/60], Step [100/626], Loss: 0.472, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [200/626], Loss: 0.588, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [300/626], Loss: 0.495, Training Accuracy: 80.43%\n",
      "Epoch [8/60], Step [400/626], Loss: 0.519, Training Accuracy: 81.52%\n",
      "Epoch [8/60], Step [500/626], Loss: 0.664, Training Accuracy: 75.00%\n",
      "Epoch [8/60], Step [600/626], Loss: 0.472, Training Accuracy: 84.78%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 53.87393895611342 %\n",
      "Epoch [9/60], Step [100/626], Loss: 0.531, Training Accuracy: 77.17%\n",
      "Epoch [9/60], Step [200/626], Loss: 0.494, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [300/626], Loss: 0.600, Training Accuracy: 77.17%\n",
      "Epoch [9/60], Step [400/626], Loss: 0.621, Training Accuracy: 78.80%\n",
      "Epoch [9/60], Step [500/626], Loss: 0.528, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [600/626], Loss: 0.503, Training Accuracy: 78.80%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 60.367445449619915 %\n",
      "Epoch [10/60], Step [100/626], Loss: 0.475, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [200/626], Loss: 0.452, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [300/626], Loss: 0.487, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [400/626], Loss: 0.501, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [500/626], Loss: 0.438, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [600/626], Loss: 0.480, Training Accuracy: 80.43%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 54.85576370532122 %\n",
      "Epoch [11/60], Step [100/626], Loss: 0.561, Training Accuracy: 75.54%\n",
      "Epoch [11/60], Step [200/626], Loss: 0.407, Training Accuracy: 82.61%\n",
      "Epoch [11/60], Step [300/626], Loss: 0.439, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [400/626], Loss: 0.518, Training Accuracy: 82.61%\n",
      "Epoch [11/60], Step [500/626], Loss: 0.462, Training Accuracy: 80.98%\n",
      "Epoch [11/60], Step [600/626], Loss: 0.525, Training Accuracy: 80.43%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 58.10497972318453 %\n",
      "Epoch [12/60], Step [100/626], Loss: 0.473, Training Accuracy: 80.98%\n",
      "Epoch [12/60], Step [200/626], Loss: 0.483, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [300/626], Loss: 0.511, Training Accuracy: 79.35%\n",
      "Epoch [12/60], Step [400/626], Loss: 0.420, Training Accuracy: 87.50%\n",
      "Epoch [12/60], Step [500/626], Loss: 0.517, Training Accuracy: 75.54%\n",
      "Epoch [12/60], Step [600/626], Loss: 0.435, Training Accuracy: 84.24%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 59.71234833434581 %\n",
      "Epoch [13/60], Step [100/626], Loss: 0.362, Training Accuracy: 86.41%\n",
      "Epoch [13/60], Step [200/626], Loss: 0.477, Training Accuracy: 82.07%\n",
      "Epoch [13/60], Step [300/626], Loss: 0.520, Training Accuracy: 83.15%\n",
      "Epoch [13/60], Step [400/626], Loss: 0.508, Training Accuracy: 82.61%\n",
      "Epoch [13/60], Step [500/626], Loss: 0.482, Training Accuracy: 80.43%\n",
      "Epoch [13/60], Step [600/626], Loss: 0.411, Training Accuracy: 84.24%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 59.80593363652782 %\n",
      "Epoch [14/60], Step [100/626], Loss: 0.406, Training Accuracy: 84.78%\n",
      "Epoch [14/60], Step [200/626], Loss: 0.385, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [300/626], Loss: 0.441, Training Accuracy: 82.07%\n",
      "Epoch [14/60], Step [400/626], Loss: 0.426, Training Accuracy: 85.33%\n",
      "Epoch [14/60], Step [500/626], Loss: 0.439, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [600/626], Loss: 0.380, Training Accuracy: 85.33%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 60.87641814569754 %\n",
      "Epoch [15/60], Step [100/626], Loss: 0.394, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [200/626], Loss: 0.448, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [300/626], Loss: 0.554, Training Accuracy: 85.33%\n",
      "Epoch [15/60], Step [400/626], Loss: 0.454, Training Accuracy: 83.15%\n",
      "Epoch [15/60], Step [500/626], Loss: 0.370, Training Accuracy: 85.87%\n",
      "Epoch [15/60], Step [600/626], Loss: 0.492, Training Accuracy: 80.43%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 59.27397507675637 %\n",
      "Epoch [16/60], Step [100/626], Loss: 0.436, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [200/626], Loss: 0.427, Training Accuracy: 84.78%\n",
      "Epoch [16/60], Step [300/626], Loss: 0.487, Training Accuracy: 80.43%\n",
      "Epoch [16/60], Step [400/626], Loss: 0.395, Training Accuracy: 83.70%\n",
      "Epoch [16/60], Step [500/626], Loss: 0.422, Training Accuracy: 84.24%\n",
      "Epoch [16/60], Step [600/626], Loss: 0.432, Training Accuracy: 85.33%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 63.08634475511846 %\n",
      "Epoch [17/60], Step [100/626], Loss: 0.389, Training Accuracy: 86.41%\n",
      "Epoch [17/60], Step [200/626], Loss: 0.396, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [300/626], Loss: 0.515, Training Accuracy: 82.07%\n",
      "Epoch [17/60], Step [400/626], Loss: 0.491, Training Accuracy: 81.52%\n",
      "Epoch [17/60], Step [500/626], Loss: 0.425, Training Accuracy: 86.41%\n",
      "Epoch [17/60], Step [600/626], Loss: 0.419, Training Accuracy: 88.59%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 60.864925213850626 %\n",
      "Epoch [18/60], Step [100/626], Loss: 0.511, Training Accuracy: 83.15%\n",
      "Epoch [18/60], Step [200/626], Loss: 0.438, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [300/626], Loss: 0.452, Training Accuracy: 84.78%\n",
      "Epoch [18/60], Step [400/626], Loss: 0.377, Training Accuracy: 89.67%\n",
      "Epoch [18/60], Step [500/626], Loss: 0.371, Training Accuracy: 83.70%\n",
      "Epoch [18/60], Step [600/626], Loss: 0.385, Training Accuracy: 82.61%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 63.048582264764306 %\n",
      "Epoch [19/60], Step [100/626], Loss: 0.463, Training Accuracy: 85.33%\n",
      "Epoch [19/60], Step [200/626], Loss: 0.384, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [300/626], Loss: 0.414, Training Accuracy: 85.87%\n",
      "Epoch [19/60], Step [400/626], Loss: 0.335, Training Accuracy: 90.22%\n",
      "Epoch [19/60], Step [500/626], Loss: 0.401, Training Accuracy: 82.61%\n",
      "Epoch [19/60], Step [600/626], Loss: 0.302, Training Accuracy: 88.04%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 62.69558507232338 %\n",
      "Epoch [20/60], Step [100/626], Loss: 0.440, Training Accuracy: 80.98%\n",
      "Epoch [20/60], Step [200/626], Loss: 0.301, Training Accuracy: 89.67%\n",
      "Epoch [20/60], Step [300/626], Loss: 0.398, Training Accuracy: 88.04%\n",
      "Epoch [20/60], Step [400/626], Loss: 0.279, Training Accuracy: 89.13%\n",
      "Epoch [20/60], Step [500/626], Loss: 0.379, Training Accuracy: 86.96%\n",
      "Epoch [20/60], Step [600/626], Loss: 0.325, Training Accuracy: 88.04%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 62.114371090350865 %\n",
      "Epoch [21/60], Step [100/626], Loss: 0.433, Training Accuracy: 85.87%\n",
      "Epoch [21/60], Step [200/626], Loss: 0.401, Training Accuracy: 87.50%\n",
      "Epoch [21/60], Step [300/626], Loss: 0.312, Training Accuracy: 86.41%\n",
      "Epoch [21/60], Step [400/626], Loss: 0.447, Training Accuracy: 83.70%\n",
      "Epoch [21/60], Step [500/626], Loss: 0.405, Training Accuracy: 88.59%\n",
      "Epoch [21/60], Step [600/626], Loss: 0.407, Training Accuracy: 85.87%\n",
      "Testing epoch 21\n",
      "Epoch 21: Test Accuracy 62.90738338778794 %\n",
      "Finished early after 21 epochs!\n",
      "Final Training Accuracy 87.25581395348837 %\n",
      "Final Test Accuracy 62.75304973155795 %\n",
      "Epoch [1/60], Step [100/628], Loss: 1.254, Training Accuracy: 42.39%\n",
      "Epoch [1/60], Step [200/628], Loss: 1.156, Training Accuracy: 54.89%\n",
      "Epoch [1/60], Step [300/628], Loss: 1.001, Training Accuracy: 50.54%\n",
      "Epoch [1/60], Step [400/628], Loss: 0.836, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [500/628], Loss: 0.939, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [600/628], Loss: 0.880, Training Accuracy: 67.39%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 39.455162705970025 %\n",
      "Epoch [2/60], Step [100/628], Loss: 0.840, Training Accuracy: 66.85%\n",
      "Epoch [2/60], Step [200/628], Loss: 0.733, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [300/628], Loss: 0.620, Training Accuracy: 71.20%\n",
      "Epoch [2/60], Step [400/628], Loss: 0.934, Training Accuracy: 70.65%\n",
      "Epoch [2/60], Step [500/628], Loss: 0.853, Training Accuracy: 69.02%\n",
      "Epoch [2/60], Step [600/628], Loss: 0.793, Training Accuracy: 73.37%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 37.25262896414673 %\n",
      "Epoch [3/60], Step [100/628], Loss: 0.711, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [200/628], Loss: 0.734, Training Accuracy: 75.54%\n",
      "Epoch [3/60], Step [300/628], Loss: 0.707, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [400/628], Loss: 0.707, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [500/628], Loss: 0.774, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [600/628], Loss: 0.643, Training Accuracy: 77.72%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 38.653639148795236 %\n",
      "Epoch [4/60], Step [100/628], Loss: 0.718, Training Accuracy: 73.91%\n",
      "Epoch [4/60], Step [200/628], Loss: 0.568, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [300/628], Loss: 0.687, Training Accuracy: 72.28%\n",
      "Epoch [4/60], Step [400/628], Loss: 0.612, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [500/628], Loss: 0.532, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [600/628], Loss: 0.506, Training Accuracy: 78.80%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 40.11923490933179 %\n",
      "Epoch [5/60], Step [100/628], Loss: 0.584, Training Accuracy: 76.09%\n",
      "Epoch [5/60], Step [200/628], Loss: 0.679, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [300/628], Loss: 0.563, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [400/628], Loss: 0.485, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [500/628], Loss: 0.607, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [600/628], Loss: 0.486, Training Accuracy: 82.07%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 40.99859236565372 %\n",
      "Epoch [6/60], Step [100/628], Loss: 0.582, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [200/628], Loss: 0.408, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [300/628], Loss: 0.649, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [400/628], Loss: 0.562, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [500/628], Loss: 0.438, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [600/628], Loss: 0.478, Training Accuracy: 82.61%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 40.11095470729486 %\n",
      "Epoch [7/60], Step [100/628], Loss: 0.467, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [200/628], Loss: 0.518, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [300/628], Loss: 0.497, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [400/628], Loss: 0.695, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [500/628], Loss: 0.533, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [600/628], Loss: 0.470, Training Accuracy: 84.78%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 41.88623002401259 %\n",
      "Epoch [8/60], Step [100/628], Loss: 0.292, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [200/628], Loss: 0.552, Training Accuracy: 77.72%\n",
      "Epoch [8/60], Step [300/628], Loss: 0.482, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [400/628], Loss: 0.514, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [500/628], Loss: 0.535, Training Accuracy: 75.00%\n",
      "Epoch [8/60], Step [600/628], Loss: 0.394, Training Accuracy: 90.76%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 40.65247992051006 %\n",
      "Epoch [9/60], Step [100/628], Loss: 0.332, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [200/628], Loss: 0.511, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [300/628], Loss: 0.579, Training Accuracy: 80.43%\n",
      "Epoch [9/60], Step [400/628], Loss: 0.350, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [500/628], Loss: 0.480, Training Accuracy: 77.17%\n",
      "Epoch [9/60], Step [600/628], Loss: 0.482, Training Accuracy: 85.33%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 39.3524882007121 %\n",
      "Epoch [10/60], Step [100/628], Loss: 0.458, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [200/628], Loss: 0.497, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [300/628], Loss: 0.398, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [400/628], Loss: 0.479, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [500/628], Loss: 0.344, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [600/628], Loss: 0.406, Training Accuracy: 83.70%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 40.77171482984185 %\n",
      "Epoch [11/60], Step [100/628], Loss: 0.510, Training Accuracy: 75.54%\n",
      "Epoch [11/60], Step [200/628], Loss: 0.458, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [300/628], Loss: 0.386, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [400/628], Loss: 0.474, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [500/628], Loss: 0.407, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [600/628], Loss: 0.307, Training Accuracy: 88.59%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 41.714001821644445 %\n",
      "Epoch [12/60], Step [100/628], Loss: 0.486, Training Accuracy: 80.43%\n",
      "Epoch [12/60], Step [200/628], Loss: 0.512, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [300/628], Loss: 0.392, Training Accuracy: 83.15%\n",
      "Epoch [12/60], Step [400/628], Loss: 0.457, Training Accuracy: 84.78%\n",
      "Epoch [12/60], Step [500/628], Loss: 0.340, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [600/628], Loss: 0.404, Training Accuracy: 86.96%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 41.74049846816262 %\n",
      "Finished early after 12 epochs!\n",
      "Final Training Accuracy 84.87368776342095 %\n",
      "Final Test Accuracy 41.78024343793989 %\n",
      "Epoch [1/60], Step [100/660], Loss: 1.196, Training Accuracy: 51.63%\n",
      "Epoch [1/60], Step [200/660], Loss: 1.054, Training Accuracy: 61.96%\n",
      "Epoch [1/60], Step [300/660], Loss: 1.063, Training Accuracy: 55.43%\n",
      "Epoch [1/60], Step [400/660], Loss: 0.945, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [500/660], Loss: 0.984, Training Accuracy: 60.87%\n",
      "Epoch [1/60], Step [600/660], Loss: 0.913, Training Accuracy: 64.67%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 50.95717423133236 %\n",
      "Epoch [2/60], Step [100/660], Loss: 0.862, Training Accuracy: 69.57%\n",
      "Epoch [2/60], Step [200/660], Loss: 0.836, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/660], Loss: 0.848, Training Accuracy: 66.30%\n",
      "Epoch [2/60], Step [400/660], Loss: 0.884, Training Accuracy: 69.02%\n",
      "Epoch [2/60], Step [500/660], Loss: 0.731, Training Accuracy: 69.57%\n",
      "Epoch [2/60], Step [600/660], Loss: 0.705, Training Accuracy: 74.46%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 53.90190336749634 %\n",
      "Epoch [3/60], Step [100/660], Loss: 0.946, Training Accuracy: 60.87%\n",
      "Epoch [3/60], Step [200/660], Loss: 0.733, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [300/660], Loss: 0.687, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [400/660], Loss: 0.661, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [500/660], Loss: 0.709, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [600/660], Loss: 0.832, Training Accuracy: 72.28%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 50.603953147877014 %\n",
      "Epoch [4/60], Step [100/660], Loss: 0.744, Training Accuracy: 73.91%\n",
      "Epoch [4/60], Step [200/660], Loss: 0.778, Training Accuracy: 71.74%\n",
      "Epoch [4/60], Step [300/660], Loss: 0.709, Training Accuracy: 73.91%\n",
      "Epoch [4/60], Step [400/660], Loss: 0.664, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [500/660], Loss: 0.712, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [600/660], Loss: 0.779, Training Accuracy: 73.91%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 57.9703513909224 %\n",
      "Epoch [5/60], Step [100/660], Loss: 0.686, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [200/660], Loss: 0.590, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [300/660], Loss: 0.543, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [400/660], Loss: 0.631, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [500/660], Loss: 0.651, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [600/660], Loss: 0.628, Training Accuracy: 78.26%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 57.96120058565154 %\n",
      "Epoch [6/60], Step [100/660], Loss: 0.642, Training Accuracy: 77.17%\n",
      "Epoch [6/60], Step [200/660], Loss: 0.601, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [300/660], Loss: 0.533, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [400/660], Loss: 0.636, Training Accuracy: 73.91%\n",
      "Epoch [6/60], Step [500/660], Loss: 0.625, Training Accuracy: 77.72%\n",
      "Epoch [6/60], Step [600/660], Loss: 0.576, Training Accuracy: 78.26%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 56.725841874084914 %\n",
      "Epoch [7/60], Step [100/660], Loss: 0.553, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [200/660], Loss: 0.523, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [300/660], Loss: 0.553, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [400/660], Loss: 0.613, Training Accuracy: 79.35%\n",
      "Epoch [7/60], Step [500/660], Loss: 0.572, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [600/660], Loss: 0.519, Training Accuracy: 81.52%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 54.904831625183014 %\n",
      "Epoch [8/60], Step [100/660], Loss: 0.545, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [200/660], Loss: 0.523, Training Accuracy: 78.80%\n",
      "Epoch [8/60], Step [300/660], Loss: 0.591, Training Accuracy: 79.89%\n",
      "Epoch [8/60], Step [400/660], Loss: 0.611, Training Accuracy: 79.35%\n",
      "Epoch [8/60], Step [500/660], Loss: 0.459, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [600/660], Loss: 0.567, Training Accuracy: 79.89%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 58.56698389458273 %\n",
      "Epoch [9/60], Step [100/660], Loss: 0.551, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [200/660], Loss: 0.521, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [300/660], Loss: 0.480, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [400/660], Loss: 0.552, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [500/660], Loss: 0.547, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [600/660], Loss: 0.549, Training Accuracy: 84.24%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 58.265007320644216 %\n",
      "Epoch [10/60], Step [100/660], Loss: 0.568, Training Accuracy: 78.80%\n",
      "Epoch [10/60], Step [200/660], Loss: 0.568, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [300/660], Loss: 0.487, Training Accuracy: 80.98%\n",
      "Epoch [10/60], Step [400/660], Loss: 0.561, Training Accuracy: 79.35%\n",
      "Epoch [10/60], Step [500/660], Loss: 0.598, Training Accuracy: 79.89%\n",
      "Epoch [10/60], Step [600/660], Loss: 0.509, Training Accuracy: 86.41%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 58.39860907759883 %\n",
      "Epoch [11/60], Step [100/660], Loss: 0.440, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [200/660], Loss: 0.423, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [300/660], Loss: 0.472, Training Accuracy: 79.89%\n",
      "Epoch [11/60], Step [400/660], Loss: 0.525, Training Accuracy: 77.72%\n",
      "Epoch [11/60], Step [500/660], Loss: 0.500, Training Accuracy: 82.07%\n",
      "Epoch [11/60], Step [600/660], Loss: 0.499, Training Accuracy: 81.52%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 60.03477306002928 %\n",
      "Epoch [12/60], Step [100/660], Loss: 0.516, Training Accuracy: 83.70%\n",
      "Epoch [12/60], Step [200/660], Loss: 0.529, Training Accuracy: 82.07%\n",
      "Epoch [12/60], Step [300/660], Loss: 0.463, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [400/660], Loss: 0.408, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [500/660], Loss: 0.421, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [600/660], Loss: 0.414, Training Accuracy: 85.87%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 61.48792093704246 %\n",
      "Epoch [13/60], Step [100/660], Loss: 0.536, Training Accuracy: 82.07%\n",
      "Epoch [13/60], Step [200/660], Loss: 0.333, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [300/660], Loss: 0.467, Training Accuracy: 82.61%\n",
      "Epoch [13/60], Step [400/660], Loss: 0.455, Training Accuracy: 85.87%\n",
      "Epoch [13/60], Step [500/660], Loss: 0.460, Training Accuracy: 85.87%\n",
      "Epoch [13/60], Step [600/660], Loss: 0.512, Training Accuracy: 80.43%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 58.13140556368961 %\n",
      "Epoch [14/60], Step [100/660], Loss: 0.463, Training Accuracy: 80.43%\n",
      "Epoch [14/60], Step [200/660], Loss: 0.603, Training Accuracy: 79.89%\n",
      "Epoch [14/60], Step [300/660], Loss: 0.396, Training Accuracy: 83.15%\n",
      "Epoch [14/60], Step [400/660], Loss: 0.436, Training Accuracy: 84.24%\n",
      "Epoch [14/60], Step [500/660], Loss: 0.477, Training Accuracy: 84.24%\n",
      "Epoch [14/60], Step [600/660], Loss: 0.333, Training Accuracy: 88.59%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 60.79612005856515 %\n",
      "Epoch [15/60], Step [100/660], Loss: 0.548, Training Accuracy: 82.61%\n",
      "Epoch [15/60], Step [200/660], Loss: 0.373, Training Accuracy: 86.96%\n",
      "Epoch [15/60], Step [300/660], Loss: 0.540, Training Accuracy: 84.24%\n",
      "Epoch [15/60], Step [400/660], Loss: 0.412, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [500/660], Loss: 0.410, Training Accuracy: 85.33%\n",
      "Epoch [15/60], Step [600/660], Loss: 0.364, Training Accuracy: 86.41%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 57.902635431918 %\n",
      "Epoch [16/60], Step [100/660], Loss: 0.391, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [200/660], Loss: 0.617, Training Accuracy: 78.26%\n",
      "Epoch [16/60], Step [300/660], Loss: 0.405, Training Accuracy: 82.07%\n",
      "Epoch [16/60], Step [400/660], Loss: 0.420, Training Accuracy: 84.78%\n",
      "Epoch [16/60], Step [500/660], Loss: 0.396, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [600/660], Loss: 0.417, Training Accuracy: 84.24%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 59.253294289897504 %\n",
      "Epoch [17/60], Step [100/660], Loss: 0.406, Training Accuracy: 84.24%\n",
      "Epoch [17/60], Step [200/660], Loss: 0.425, Training Accuracy: 83.70%\n",
      "Epoch [17/60], Step [300/660], Loss: 0.375, Training Accuracy: 88.59%\n",
      "Epoch [17/60], Step [400/660], Loss: 0.490, Training Accuracy: 85.33%\n",
      "Epoch [17/60], Step [500/660], Loss: 0.419, Training Accuracy: 85.87%\n",
      "Epoch [17/60], Step [600/660], Loss: 0.434, Training Accuracy: 86.96%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 62.59516837481698 %\n",
      "Epoch [18/60], Step [100/660], Loss: 0.413, Training Accuracy: 81.52%\n",
      "Epoch [18/60], Step [200/660], Loss: 0.361, Training Accuracy: 85.87%\n",
      "Epoch [18/60], Step [300/660], Loss: 0.540, Training Accuracy: 80.98%\n",
      "Epoch [18/60], Step [400/660], Loss: 0.336, Training Accuracy: 89.67%\n",
      "Epoch [18/60], Step [500/660], Loss: 0.356, Training Accuracy: 88.59%\n",
      "Epoch [18/60], Step [600/660], Loss: 0.370, Training Accuracy: 84.78%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 62.30600292825768 %\n",
      "Epoch [19/60], Step [100/660], Loss: 0.446, Training Accuracy: 83.70%\n",
      "Epoch [19/60], Step [200/660], Loss: 0.389, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [300/660], Loss: 0.400, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [400/660], Loss: 0.505, Training Accuracy: 84.78%\n",
      "Epoch [19/60], Step [500/660], Loss: 0.457, Training Accuracy: 86.41%\n",
      "Epoch [19/60], Step [600/660], Loss: 0.494, Training Accuracy: 82.07%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 59.24597364568081 %\n",
      "Epoch [20/60], Step [100/660], Loss: 0.315, Training Accuracy: 89.13%\n",
      "Epoch [20/60], Step [200/660], Loss: 0.405, Training Accuracy: 88.04%\n",
      "Epoch [20/60], Step [300/660], Loss: 0.540, Training Accuracy: 84.78%\n",
      "Epoch [20/60], Step [400/660], Loss: 0.381, Training Accuracy: 88.59%\n",
      "Epoch [20/60], Step [500/660], Loss: 0.390, Training Accuracy: 84.24%\n",
      "Epoch [20/60], Step [600/660], Loss: 0.429, Training Accuracy: 85.33%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 60.973645680819914 %\n",
      "Epoch [21/60], Step [100/660], Loss: 0.348, Training Accuracy: 88.59%\n",
      "Epoch [21/60], Step [200/660], Loss: 0.382, Training Accuracy: 87.50%\n",
      "Epoch [21/60], Step [300/660], Loss: 0.360, Training Accuracy: 89.13%\n",
      "Epoch [21/60], Step [400/660], Loss: 0.314, Training Accuracy: 91.30%\n",
      "Epoch [21/60], Step [500/660], Loss: 0.325, Training Accuracy: 88.04%\n",
      "Epoch [21/60], Step [600/660], Loss: 0.365, Training Accuracy: 86.41%\n",
      "Testing epoch 21\n",
      "Epoch 21: Test Accuracy 61.61786237188872 %\n",
      "Epoch [22/60], Step [100/660], Loss: 0.409, Training Accuracy: 87.50%\n",
      "Epoch [22/60], Step [200/660], Loss: 0.361, Training Accuracy: 88.59%\n",
      "Epoch [22/60], Step [300/660], Loss: 0.364, Training Accuracy: 87.50%\n",
      "Epoch [22/60], Step [400/660], Loss: 0.371, Training Accuracy: 84.78%\n",
      "Epoch [22/60], Step [500/660], Loss: 0.508, Training Accuracy: 84.24%\n",
      "Epoch [22/60], Step [600/660], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Testing epoch 22\n",
      "Epoch 22: Test Accuracy 61.17130307467057 %\n",
      "Finished early after 22 epochs!\n",
      "Final Training Accuracy 86.87300069254361 %\n",
      "Final Test Accuracy 61.189604685212295 %\n",
      "By Spectra accuracy: 0.5882920399437404 +/- 0.07550570041814639\n",
      "By Sample simple accuracy: 0.5933546954135188 +/- 0.13489673590588785\n",
      "By Sample simple log loss: 1.5315586514200137 +/- 1.0418952896732117\n",
      "Centre: UoE\n",
      "Epoch [1/60], Step [100/692], Loss: 1.286, Training Accuracy: 39.13%\n",
      "Epoch [1/60], Step [200/692], Loss: 1.121, Training Accuracy: 45.11%\n",
      "Epoch [1/60], Step [300/692], Loss: 0.923, Training Accuracy: 55.98%\n",
      "Epoch [1/60], Step [400/692], Loss: 0.868, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [500/692], Loss: 0.933, Training Accuracy: 58.70%\n",
      "Epoch [1/60], Step [600/692], Loss: 0.833, Training Accuracy: 66.30%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 65.63650649370658 %\n",
      "Epoch [2/60], Step [100/692], Loss: 0.820, Training Accuracy: 67.93%\n",
      "Epoch [2/60], Step [200/692], Loss: 0.801, Training Accuracy: 66.30%\n",
      "Epoch [2/60], Step [300/692], Loss: 0.827, Training Accuracy: 63.04%\n",
      "Epoch [2/60], Step [400/692], Loss: 0.733, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [500/692], Loss: 0.642, Training Accuracy: 69.02%\n",
      "Epoch [2/60], Step [600/692], Loss: 0.718, Training Accuracy: 66.85%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 68.44505384461323 %\n",
      "Epoch [3/60], Step [100/692], Loss: 0.649, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [200/692], Loss: 0.560, Training Accuracy: 73.37%\n",
      "Epoch [3/60], Step [300/692], Loss: 0.677, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [400/692], Loss: 0.601, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [500/692], Loss: 0.689, Training Accuracy: 70.65%\n",
      "Epoch [3/60], Step [600/692], Loss: 0.599, Training Accuracy: 74.46%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 67.08160655687193 %\n",
      "Epoch [4/60], Step [100/692], Loss: 0.583, Training Accuracy: 70.65%\n",
      "Epoch [4/60], Step [200/692], Loss: 0.610, Training Accuracy: 73.91%\n",
      "Epoch [4/60], Step [300/692], Loss: 0.583, Training Accuracy: 75.00%\n",
      "Epoch [4/60], Step [400/692], Loss: 0.707, Training Accuracy: 70.65%\n",
      "Epoch [4/60], Step [500/692], Loss: 0.640, Training Accuracy: 72.83%\n",
      "Epoch [4/60], Step [600/692], Loss: 0.507, Training Accuracy: 77.17%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 66.53314640496696 %\n",
      "Epoch [5/60], Step [100/692], Loss: 0.514, Training Accuracy: 73.37%\n",
      "Epoch [5/60], Step [200/692], Loss: 0.606, Training Accuracy: 73.37%\n",
      "Epoch [5/60], Step [300/692], Loss: 0.584, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [400/692], Loss: 0.391, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [500/692], Loss: 0.511, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [600/692], Loss: 0.602, Training Accuracy: 77.17%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 66.32054106518356 %\n",
      "Epoch [6/60], Step [100/692], Loss: 0.599, Training Accuracy: 72.83%\n",
      "Epoch [6/60], Step [200/692], Loss: 0.700, Training Accuracy: 75.54%\n",
      "Epoch [6/60], Step [300/692], Loss: 0.572, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [400/692], Loss: 0.522, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [500/692], Loss: 0.623, Training Accuracy: 75.00%\n",
      "Epoch [6/60], Step [600/692], Loss: 0.579, Training Accuracy: 72.28%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 65.76591843966168 %\n",
      "Epoch [7/60], Step [100/692], Loss: 0.466, Training Accuracy: 78.80%\n",
      "Epoch [7/60], Step [200/692], Loss: 0.448, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [300/692], Loss: 0.496, Training Accuracy: 75.54%\n",
      "Epoch [7/60], Step [400/692], Loss: 0.454, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [500/692], Loss: 0.526, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [600/692], Loss: 0.565, Training Accuracy: 77.17%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 67.12012201697762 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 79.93257950196839 %\n",
      "Final Test Accuracy 67.0353880047451 %\n",
      "Epoch [1/60], Step [100/682], Loss: 1.191, Training Accuracy: 62.50%\n",
      "Epoch [1/60], Step [200/682], Loss: 1.006, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [300/682], Loss: 0.822, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [400/682], Loss: 0.644, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [500/682], Loss: 0.744, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [600/682], Loss: 0.670, Training Accuracy: 78.26%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 57.02526426268836 %\n",
      "Epoch [2/60], Step [100/682], Loss: 0.603, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [200/682], Loss: 0.588, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [300/682], Loss: 0.457, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [400/682], Loss: 0.493, Training Accuracy: 87.50%\n",
      "Epoch [2/60], Step [500/682], Loss: 0.499, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [600/682], Loss: 0.517, Training Accuracy: 83.15%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 61.76325061848714 %\n",
      "Epoch [3/60], Step [100/682], Loss: 0.445, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [200/682], Loss: 0.472, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [300/682], Loss: 0.437, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [400/682], Loss: 0.537, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [500/682], Loss: 0.386, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [600/682], Loss: 0.437, Training Accuracy: 87.50%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 62.322512932003896 %\n",
      "Epoch [4/60], Step [100/682], Loss: 0.406, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [200/682], Loss: 0.505, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [300/682], Loss: 0.369, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [400/682], Loss: 0.377, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [500/682], Loss: 0.336, Training Accuracy: 90.22%\n",
      "Epoch [4/60], Step [600/682], Loss: 0.296, Training Accuracy: 91.85%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 63.04670515031112 %\n",
      "Epoch [5/60], Step [100/682], Loss: 0.395, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [200/682], Loss: 0.402, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [300/682], Loss: 0.319, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [400/682], Loss: 0.435, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [500/682], Loss: 0.303, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [600/682], Loss: 0.384, Training Accuracy: 88.59%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 63.82487442836794 %\n",
      "Epoch [6/60], Step [100/682], Loss: 0.301, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [200/682], Loss: 0.302, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [300/682], Loss: 0.372, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [400/682], Loss: 0.334, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [500/682], Loss: 0.411, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [600/682], Loss: 0.394, Training Accuracy: 89.67%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 63.91933428292975 %\n",
      "Epoch [7/60], Step [100/682], Loss: 0.344, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [200/682], Loss: 0.440, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [300/682], Loss: 0.313, Training Accuracy: 94.57%\n",
      "Epoch [7/60], Step [400/682], Loss: 0.454, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [500/682], Loss: 0.342, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [600/682], Loss: 0.287, Training Accuracy: 89.13%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 64.14873678686558 %\n",
      "Epoch [8/60], Step [100/682], Loss: 0.263, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [200/682], Loss: 0.275, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [300/682], Loss: 0.239, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [400/682], Loss: 0.271, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [500/682], Loss: 0.211, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [600/682], Loss: 0.328, Training Accuracy: 89.13%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 63.82487442836794 %\n",
      "Epoch [9/60], Step [100/682], Loss: 0.378, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [200/682], Loss: 0.341, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [300/682], Loss: 0.253, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [400/682], Loss: 0.341, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [500/682], Loss: 0.312, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [600/682], Loss: 0.345, Training Accuracy: 91.85%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 65.37821425893995 %\n",
      "Epoch [10/60], Step [100/682], Loss: 0.222, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [200/682], Loss: 0.255, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [300/682], Loss: 0.236, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [400/682], Loss: 0.239, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [500/682], Loss: 0.201, Training Accuracy: 94.02%\n",
      "Epoch [10/60], Step [600/682], Loss: 0.184, Training Accuracy: 93.48%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 63.59697128720294 %\n",
      "Epoch [11/60], Step [100/682], Loss: 0.284, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [200/682], Loss: 0.206, Training Accuracy: 95.11%\n",
      "Epoch [11/60], Step [300/682], Loss: 0.204, Training Accuracy: 94.02%\n",
      "Epoch [11/60], Step [400/682], Loss: 0.276, Training Accuracy: 94.02%\n",
      "Epoch [11/60], Step [500/682], Loss: 0.269, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [600/682], Loss: 0.243, Training Accuracy: 91.85%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 64.53857110727941 %\n",
      "Epoch [12/60], Step [100/682], Loss: 0.183, Training Accuracy: 94.57%\n",
      "Epoch [12/60], Step [200/682], Loss: 0.224, Training Accuracy: 94.02%\n",
      "Epoch [12/60], Step [300/682], Loss: 0.238, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [400/682], Loss: 0.252, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [500/682], Loss: 0.264, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [600/682], Loss: 0.215, Training Accuracy: 91.85%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 65.56713396806357 %\n",
      "Epoch [13/60], Step [100/682], Loss: 0.196, Training Accuracy: 92.93%\n",
      "Epoch [13/60], Step [200/682], Loss: 0.247, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [300/682], Loss: 0.234, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [400/682], Loss: 0.227, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [500/682], Loss: 0.310, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [600/682], Loss: 0.283, Training Accuracy: 92.39%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 63.77989354524327 %\n",
      "Epoch [14/60], Step [100/682], Loss: 0.215, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [200/682], Loss: 0.183, Training Accuracy: 94.57%\n",
      "Epoch [14/60], Step [300/682], Loss: 0.184, Training Accuracy: 93.48%\n",
      "Epoch [14/60], Step [400/682], Loss: 0.213, Training Accuracy: 94.57%\n",
      "Epoch [14/60], Step [500/682], Loss: 0.242, Training Accuracy: 92.93%\n",
      "Epoch [14/60], Step [600/682], Loss: 0.213, Training Accuracy: 94.57%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 64.72449209086139 %\n",
      "Epoch [15/60], Step [100/682], Loss: 0.310, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [200/682], Loss: 0.192, Training Accuracy: 94.02%\n",
      "Epoch [15/60], Step [300/682], Loss: 0.274, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [400/682], Loss: 0.186, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [500/682], Loss: 0.276, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [600/682], Loss: 0.155, Training Accuracy: 94.57%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 63.890846390284125 %\n",
      "Epoch [16/60], Step [100/682], Loss: 0.221, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [200/682], Loss: 0.187, Training Accuracy: 93.48%\n",
      "Epoch [16/60], Step [300/682], Loss: 0.242, Training Accuracy: 92.93%\n",
      "Epoch [16/60], Step [400/682], Loss: 0.211, Training Accuracy: 92.93%\n",
      "Epoch [16/60], Step [500/682], Loss: 0.374, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [600/682], Loss: 0.264, Training Accuracy: 92.39%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 63.8818502136592 %\n",
      "Epoch [17/60], Step [100/682], Loss: 0.236, Training Accuracy: 92.39%\n",
      "Epoch [17/60], Step [200/682], Loss: 0.152, Training Accuracy: 93.48%\n",
      "Epoch [17/60], Step [300/682], Loss: 0.226, Training Accuracy: 91.85%\n",
      "Epoch [17/60], Step [400/682], Loss: 0.218, Training Accuracy: 92.39%\n",
      "Epoch [17/60], Step [500/682], Loss: 0.163, Training Accuracy: 95.65%\n",
      "Epoch [17/60], Step [600/682], Loss: 0.116, Training Accuracy: 96.74%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 64.33015968213509 %\n",
      "Finished early after 17 epochs!\n",
      "Final Training Accuracy 94.5136481370791 %\n",
      "Final Test Accuracy 64.30916860334358 %\n",
      "Epoch [1/60], Step [100/716], Loss: 1.175, Training Accuracy: 65.76%\n",
      "Epoch [1/60], Step [200/716], Loss: 1.032, Training Accuracy: 64.13%\n",
      "Epoch [1/60], Step [300/716], Loss: 0.832, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [400/716], Loss: 0.902, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [500/716], Loss: 0.879, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [600/716], Loss: 0.884, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [700/716], Loss: 0.844, Training Accuracy: 71.20%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 52.24548426509923 %\n",
      "Epoch [2/60], Step [100/716], Loss: 0.743, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [200/716], Loss: 0.757, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/716], Loss: 0.751, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [400/716], Loss: 0.730, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [500/716], Loss: 0.655, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [600/716], Loss: 0.605, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [700/716], Loss: 0.572, Training Accuracy: 80.98%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 59.92636132483572 %\n",
      "Epoch [3/60], Step [100/716], Loss: 0.715, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [200/716], Loss: 0.674, Training Accuracy: 75.54%\n",
      "Epoch [3/60], Step [300/716], Loss: 0.530, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [400/716], Loss: 0.657, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [500/716], Loss: 0.495, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [600/716], Loss: 0.598, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [700/716], Loss: 0.778, Training Accuracy: 71.74%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 57.235082389459436 %\n",
      "Epoch [4/60], Step [100/716], Loss: 0.431, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [200/716], Loss: 0.443, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [300/716], Loss: 0.563, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/716], Loss: 0.525, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [500/716], Loss: 0.523, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [600/716], Loss: 0.487, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [700/716], Loss: 0.508, Training Accuracy: 83.15%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 61.389228279893004 %\n",
      "Epoch [5/60], Step [100/716], Loss: 0.633, Training Accuracy: 76.09%\n",
      "Epoch [5/60], Step [200/716], Loss: 0.526, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [300/716], Loss: 0.552, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [400/716], Loss: 0.441, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [500/716], Loss: 0.535, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [600/716], Loss: 0.573, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [700/716], Loss: 0.451, Training Accuracy: 83.70%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 61.54443086880428 %\n",
      "Epoch [6/60], Step [100/716], Loss: 0.512, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [200/716], Loss: 0.535, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [300/716], Loss: 0.580, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [400/716], Loss: 0.421, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [500/716], Loss: 0.440, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [600/716], Loss: 0.353, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [700/716], Loss: 0.493, Training Accuracy: 84.78%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 61.80200112274213 %\n",
      "Epoch [7/60], Step [100/716], Loss: 0.425, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [200/716], Loss: 0.577, Training Accuracy: 78.26%\n",
      "Epoch [7/60], Step [300/716], Loss: 0.469, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [400/716], Loss: 0.407, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [500/716], Loss: 0.376, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [600/716], Loss: 0.416, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [700/716], Loss: 0.506, Training Accuracy: 82.61%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 60.93352706138758 %\n",
      "Epoch [8/60], Step [100/716], Loss: 0.430, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [200/716], Loss: 0.530, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [300/716], Loss: 0.450, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [400/716], Loss: 0.399, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [500/716], Loss: 0.415, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [600/716], Loss: 0.448, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [700/716], Loss: 0.448, Training Accuracy: 86.96%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 63.713964930819266 %\n",
      "Epoch [9/60], Step [100/716], Loss: 0.367, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [200/716], Loss: 0.330, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [300/716], Loss: 0.351, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [400/716], Loss: 0.410, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [500/716], Loss: 0.399, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [600/716], Loss: 0.338, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [700/716], Loss: 0.317, Training Accuracy: 88.59%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 63.50757850939471 %\n",
      "Epoch [10/60], Step [100/716], Loss: 0.460, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [200/716], Loss: 0.386, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [300/716], Loss: 0.332, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [400/716], Loss: 0.361, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [500/716], Loss: 0.402, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [600/716], Loss: 0.317, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [700/716], Loss: 0.387, Training Accuracy: 83.15%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 62.67377736683948 %\n",
      "Epoch [11/60], Step [100/716], Loss: 0.376, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [200/716], Loss: 0.295, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [300/716], Loss: 0.361, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [400/716], Loss: 0.293, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [500/716], Loss: 0.405, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [600/716], Loss: 0.267, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [700/716], Loss: 0.314, Training Accuracy: 86.41%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 61.18284185846845 %\n",
      "Epoch [12/60], Step [100/716], Loss: 0.320, Training Accuracy: 87.50%\n",
      "Epoch [12/60], Step [200/716], Loss: 0.392, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [300/716], Loss: 0.378, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [400/716], Loss: 0.309, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [500/716], Loss: 0.304, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [600/716], Loss: 0.400, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [700/716], Loss: 0.411, Training Accuracy: 87.50%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 63.758544397846975 %\n",
      "Epoch [13/60], Step [100/716], Loss: 0.247, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [200/716], Loss: 0.346, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [300/716], Loss: 0.334, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [400/716], Loss: 0.358, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [500/716], Loss: 0.259, Training Accuracy: 93.48%\n",
      "Epoch [13/60], Step [600/716], Loss: 0.278, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [700/716], Loss: 0.329, Training Accuracy: 91.30%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 62.848793052207505 %\n",
      "Epoch [14/60], Step [100/716], Loss: 0.252, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [200/716], Loss: 0.262, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [300/716], Loss: 0.259, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [400/716], Loss: 0.283, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [500/716], Loss: 0.353, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [600/716], Loss: 0.286, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [700/716], Loss: 0.317, Training Accuracy: 88.59%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 62.817422316150974 %\n",
      "Epoch [15/60], Step [100/716], Loss: 0.342, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [200/716], Loss: 0.344, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [300/716], Loss: 0.365, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [400/716], Loss: 0.294, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [500/716], Loss: 0.261, Training Accuracy: 90.76%\n",
      "Epoch [15/60], Step [600/716], Loss: 0.203, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [700/716], Loss: 0.242, Training Accuracy: 91.85%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 59.92636132483572 %\n",
      "Epoch [16/60], Step [100/716], Loss: 0.314, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [200/716], Loss: 0.325, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [300/716], Loss: 0.204, Training Accuracy: 91.85%\n",
      "Epoch [16/60], Step [400/716], Loss: 0.271, Training Accuracy: 92.39%\n",
      "Epoch [16/60], Step [500/716], Loss: 0.275, Training Accuracy: 90.76%\n",
      "Epoch [16/60], Step [600/716], Loss: 0.277, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [700/716], Loss: 0.259, Training Accuracy: 89.67%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 62.81412013340818 %\n",
      "Epoch [17/60], Step [100/716], Loss: 0.363, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [200/716], Loss: 0.246, Training Accuracy: 92.39%\n",
      "Epoch [17/60], Step [300/716], Loss: 0.277, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [400/716], Loss: 0.325, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [500/716], Loss: 0.328, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [600/716], Loss: 0.252, Training Accuracy: 91.85%\n",
      "Epoch [17/60], Step [700/716], Loss: 0.304, Training Accuracy: 87.50%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 61.94729716342502 %\n",
      "Finished early after 17 epochs!\n",
      "Final Training Accuracy 91.47366341448588 %\n",
      "Final Test Accuracy 61.93739061519664 %\n",
      "Epoch [1/60], Step [100/764], Loss: 1.195, Training Accuracy: 59.78%\n",
      "Epoch [1/60], Step [200/764], Loss: 1.003, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [300/764], Loss: 0.899, Training Accuracy: 66.85%\n",
      "Epoch [1/60], Step [400/764], Loss: 0.970, Training Accuracy: 64.13%\n",
      "Epoch [1/60], Step [500/764], Loss: 1.044, Training Accuracy: 65.76%\n",
      "Epoch [1/60], Step [600/764], Loss: 0.821, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [700/764], Loss: 0.753, Training Accuracy: 72.83%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 56.27371273712737 %\n",
      "Epoch [2/60], Step [100/764], Loss: 0.754, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [200/764], Loss: 0.695, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [300/764], Loss: 0.816, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [400/764], Loss: 0.627, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [500/764], Loss: 0.670, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [600/764], Loss: 0.632, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [700/764], Loss: 0.672, Training Accuracy: 79.89%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 59.21215640727836 %\n",
      "Epoch [3/60], Step [100/764], Loss: 0.672, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [200/764], Loss: 0.600, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [300/764], Loss: 0.583, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [400/764], Loss: 0.417, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [500/764], Loss: 0.514, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [600/764], Loss: 0.713, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [700/764], Loss: 0.430, Training Accuracy: 84.24%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 57.51645373596593 %\n",
      "Epoch [4/60], Step [100/764], Loss: 0.586, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [200/764], Loss: 0.532, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [300/764], Loss: 0.505, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [400/764], Loss: 0.506, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [500/764], Loss: 0.460, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [600/764], Loss: 0.606, Training Accuracy: 75.54%\n",
      "Epoch [4/60], Step [700/764], Loss: 0.503, Training Accuracy: 81.52%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 59.938056523422375 %\n",
      "Epoch [5/60], Step [100/764], Loss: 0.369, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [200/764], Loss: 0.422, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [300/764], Loss: 0.410, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [400/764], Loss: 0.290, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [500/764], Loss: 0.293, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [600/764], Loss: 0.430, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [700/764], Loss: 0.544, Training Accuracy: 84.78%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 60.07549361207898 %\n",
      "Epoch [6/60], Step [100/764], Loss: 0.350, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [200/764], Loss: 0.505, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [300/764], Loss: 0.473, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [400/764], Loss: 0.439, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [500/764], Loss: 0.502, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [600/764], Loss: 0.293, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [700/764], Loss: 0.416, Training Accuracy: 88.59%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 61.00271002710027 %\n",
      "Epoch [7/60], Step [100/764], Loss: 0.302, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [200/764], Loss: 0.389, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [300/764], Loss: 0.521, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [400/764], Loss: 0.381, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [500/764], Loss: 0.259, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [600/764], Loss: 0.502, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [700/764], Loss: 0.327, Training Accuracy: 90.22%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 59.52187379016647 %\n",
      "Epoch [8/60], Step [100/764], Loss: 0.438, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [200/764], Loss: 0.327, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [300/764], Loss: 0.292, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [400/764], Loss: 0.303, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [500/764], Loss: 0.334, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [600/764], Loss: 0.383, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [700/764], Loss: 0.330, Training Accuracy: 87.50%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 60.83430120015486 %\n",
      "Epoch [9/60], Step [100/764], Loss: 0.308, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [200/764], Loss: 0.307, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [300/764], Loss: 0.285, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [400/764], Loss: 0.274, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [500/764], Loss: 0.357, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [600/764], Loss: 0.296, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [700/764], Loss: 0.362, Training Accuracy: 89.13%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 61.382113821138205 %\n",
      "Epoch [10/60], Step [100/764], Loss: 0.274, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [200/764], Loss: 0.310, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [300/764], Loss: 0.410, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [400/764], Loss: 0.254, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [500/764], Loss: 0.372, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [600/764], Loss: 0.246, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [700/764], Loss: 0.297, Training Accuracy: 89.67%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 61.651180797522265 %\n",
      "Epoch [11/60], Step [100/764], Loss: 0.235, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [200/764], Loss: 0.405, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [300/764], Loss: 0.308, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [400/764], Loss: 0.286, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [500/764], Loss: 0.381, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [600/764], Loss: 0.254, Training Accuracy: 92.93%\n",
      "Epoch [11/60], Step [700/764], Loss: 0.376, Training Accuracy: 85.87%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 59.72319008904374 %\n",
      "Epoch [12/60], Step [100/764], Loss: 0.359, Training Accuracy: 83.70%\n",
      "Epoch [12/60], Step [200/764], Loss: 0.266, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [300/764], Loss: 0.250, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [400/764], Loss: 0.321, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [500/764], Loss: 0.323, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [600/764], Loss: 0.253, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [700/764], Loss: 0.394, Training Accuracy: 85.87%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 62.47580332946187 %\n",
      "Epoch [13/60], Step [100/764], Loss: 0.318, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [200/764], Loss: 0.299, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [300/764], Loss: 0.295, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [400/764], Loss: 0.286, Training Accuracy: 91.30%\n",
      "Epoch [13/60], Step [500/764], Loss: 0.370, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [600/764], Loss: 0.288, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [700/764], Loss: 0.396, Training Accuracy: 86.96%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 62.036391792489354 %\n",
      "Epoch [14/60], Step [100/764], Loss: 0.297, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [200/764], Loss: 0.352, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [300/764], Loss: 0.214, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [400/764], Loss: 0.286, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [500/764], Loss: 0.307, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [600/764], Loss: 0.281, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [700/764], Loss: 0.185, Training Accuracy: 93.48%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 63.78823073945025 %\n",
      "Epoch [15/60], Step [100/764], Loss: 0.201, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [200/764], Loss: 0.227, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [300/764], Loss: 0.243, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [400/764], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [500/764], Loss: 0.224, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [600/764], Loss: 0.286, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [700/764], Loss: 0.292, Training Accuracy: 91.30%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 62.02864885791715 %\n",
      "Epoch [16/60], Step [100/764], Loss: 0.310, Training Accuracy: 88.59%\n",
      "Epoch [16/60], Step [200/764], Loss: 0.239, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [300/764], Loss: 0.219, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [400/764], Loss: 0.152, Training Accuracy: 90.76%\n",
      "Epoch [16/60], Step [500/764], Loss: 0.211, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [600/764], Loss: 0.216, Training Accuracy: 92.93%\n",
      "Epoch [16/60], Step [700/764], Loss: 0.286, Training Accuracy: 92.93%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 62.15640727835849 %\n",
      "Epoch [17/60], Step [100/764], Loss: 0.228, Training Accuracy: 90.76%\n",
      "Epoch [17/60], Step [200/764], Loss: 0.249, Training Accuracy: 90.76%\n",
      "Epoch [17/60], Step [300/764], Loss: 0.260, Training Accuracy: 92.93%\n",
      "Epoch [17/60], Step [400/764], Loss: 0.257, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [500/764], Loss: 0.275, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [600/764], Loss: 0.200, Training Accuracy: 90.76%\n",
      "Epoch [17/60], Step [700/764], Loss: 0.297, Training Accuracy: 89.13%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 61.74603174603175 %\n",
      "Epoch [18/60], Step [100/764], Loss: 0.217, Training Accuracy: 92.93%\n",
      "Epoch [18/60], Step [200/764], Loss: 0.172, Training Accuracy: 94.02%\n",
      "Epoch [18/60], Step [300/764], Loss: 0.207, Training Accuracy: 92.39%\n",
      "Epoch [18/60], Step [400/764], Loss: 0.180, Training Accuracy: 94.02%\n",
      "Epoch [18/60], Step [500/764], Loss: 0.178, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [600/764], Loss: 0.186, Training Accuracy: 94.02%\n",
      "Epoch [18/60], Step [700/764], Loss: 0.197, Training Accuracy: 90.22%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 61.60859465737515 %\n",
      "Epoch [19/60], Step [100/764], Loss: 0.201, Training Accuracy: 91.85%\n",
      "Epoch [19/60], Step [200/764], Loss: 0.206, Training Accuracy: 92.39%\n",
      "Epoch [19/60], Step [300/764], Loss: 0.173, Training Accuracy: 90.76%\n",
      "Epoch [19/60], Step [400/764], Loss: 0.141, Training Accuracy: 93.48%\n",
      "Epoch [19/60], Step [500/764], Loss: 0.209, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [600/764], Loss: 0.163, Training Accuracy: 94.02%\n",
      "Epoch [19/60], Step [700/764], Loss: 0.154, Training Accuracy: 94.02%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 59.76190476190476 %\n",
      "Finished early after 19 epochs!\n",
      "Final Training Accuracy 91.58280549427087 %\n",
      "Final Test Accuracy 59.959349593495936 %\n",
      "Epoch [1/60], Step [100/618], Loss: 1.242, Training Accuracy: 55.43%\n",
      "Epoch [1/60], Step [200/618], Loss: 1.105, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [300/618], Loss: 1.003, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [400/618], Loss: 0.977, Training Accuracy: 66.85%\n",
      "Epoch [1/60], Step [500/618], Loss: 0.897, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [600/618], Loss: 0.951, Training Accuracy: 65.76%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 67.18545195403469 %\n",
      "Epoch [2/60], Step [100/618], Loss: 0.851, Training Accuracy: 70.65%\n",
      "Epoch [2/60], Step [200/618], Loss: 0.720, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/618], Loss: 0.816, Training Accuracy: 69.02%\n",
      "Epoch [2/60], Step [400/618], Loss: 0.738, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [500/618], Loss: 0.713, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [600/618], Loss: 0.810, Training Accuracy: 74.46%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 69.1375777859788 %\n",
      "Epoch [3/60], Step [100/618], Loss: 0.660, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [200/618], Loss: 0.625, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [300/618], Loss: 0.627, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [400/618], Loss: 0.708, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [500/618], Loss: 0.678, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [600/618], Loss: 0.740, Training Accuracy: 76.63%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 67.8879118361945 %\n",
      "Epoch [4/60], Step [100/618], Loss: 0.475, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [200/618], Loss: 0.553, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [300/618], Loss: 0.692, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/618], Loss: 0.614, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [500/618], Loss: 0.531, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [600/618], Loss: 0.590, Training Accuracy: 77.17%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 67.45523727109607 %\n",
      "Epoch [5/60], Step [100/618], Loss: 0.543, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [200/618], Loss: 0.574, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [300/618], Loss: 0.557, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [400/618], Loss: 0.493, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [500/618], Loss: 0.490, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [600/618], Loss: 0.492, Training Accuracy: 81.52%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 69.02940914470419 %\n",
      "Epoch [6/60], Step [100/618], Loss: 0.511, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [200/618], Loss: 0.518, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [300/618], Loss: 0.500, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [400/618], Loss: 0.439, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [500/618], Loss: 0.490, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [600/618], Loss: 0.455, Training Accuracy: 83.15%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 68.48983851058144 %\n",
      "Epoch [7/60], Step [100/618], Loss: 0.404, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [200/618], Loss: 0.435, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [300/618], Loss: 0.473, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [400/618], Loss: 0.592, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [500/618], Loss: 0.470, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [600/618], Loss: 0.392, Training Accuracy: 90.76%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 67.5366818951146 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 84.95540941464402 %\n",
      "Final Test Accuracy 67.62830709713545 %\n",
      "Epoch [1/60], Step [100/708], Loss: 1.184, Training Accuracy: 54.35%\n",
      "Epoch [1/60], Step [200/708], Loss: 1.052, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [300/708], Loss: 0.881, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [400/708], Loss: 0.843, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [500/708], Loss: 0.808, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [600/708], Loss: 0.752, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [700/708], Loss: 0.721, Training Accuracy: 74.46%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 53.2787547029663 %\n",
      "Epoch [2/60], Step [100/708], Loss: 0.795, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [200/708], Loss: 0.589, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [300/708], Loss: 0.543, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [400/708], Loss: 0.556, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [500/708], Loss: 0.572, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [600/708], Loss: 0.461, Training Accuracy: 88.59%\n",
      "Epoch [2/60], Step [700/708], Loss: 0.651, Training Accuracy: 77.72%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 56.185308982867475 %\n",
      "Epoch [3/60], Step [100/708], Loss: 0.723, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [200/708], Loss: 0.518, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [300/708], Loss: 0.593, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [400/708], Loss: 0.486, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [500/708], Loss: 0.510, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [600/708], Loss: 0.568, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [700/708], Loss: 0.419, Training Accuracy: 86.41%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 57.27849634258586 %\n",
      "Epoch [4/60], Step [100/708], Loss: 0.613, Training Accuracy: 72.28%\n",
      "Epoch [4/60], Step [200/708], Loss: 0.703, Training Accuracy: 76.09%\n",
      "Epoch [4/60], Step [300/708], Loss: 0.506, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [400/708], Loss: 0.454, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [500/708], Loss: 0.577, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [600/708], Loss: 0.435, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [700/708], Loss: 0.521, Training Accuracy: 79.89%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 57.23328327600963 %\n",
      "Epoch [5/60], Step [100/708], Loss: 0.432, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [200/708], Loss: 0.508, Training Accuracy: 76.63%\n",
      "Epoch [5/60], Step [300/708], Loss: 0.442, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [400/708], Loss: 0.483, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [500/708], Loss: 0.497, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [600/708], Loss: 0.416, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [700/708], Loss: 0.483, Training Accuracy: 83.70%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 59.648952833083044 %\n",
      "Epoch [6/60], Step [100/708], Loss: 0.405, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [200/708], Loss: 0.382, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [300/708], Loss: 0.453, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [400/708], Loss: 0.475, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [500/708], Loss: 0.361, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [600/708], Loss: 0.439, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [700/708], Loss: 0.291, Training Accuracy: 91.30%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 56.98138190508486 %\n",
      "Epoch [7/60], Step [100/708], Loss: 0.455, Training Accuracy: 79.89%\n",
      "Epoch [7/60], Step [200/708], Loss: 0.445, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [300/708], Loss: 0.447, Training Accuracy: 78.80%\n",
      "Epoch [7/60], Step [400/708], Loss: 0.369, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [500/708], Loss: 0.421, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [600/708], Loss: 0.412, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [700/708], Loss: 0.385, Training Accuracy: 83.70%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 58.7204702158924 %\n",
      "Epoch [8/60], Step [100/708], Loss: 0.297, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [200/708], Loss: 0.422, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [300/708], Loss: 0.390, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [400/708], Loss: 0.351, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [500/708], Loss: 0.337, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [600/708], Loss: 0.439, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [700/708], Loss: 0.337, Training Accuracy: 88.04%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 60.62103376447222 %\n",
      "Epoch [9/60], Step [100/708], Loss: 0.271, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [200/708], Loss: 0.429, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [300/708], Loss: 0.316, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [400/708], Loss: 0.299, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [500/708], Loss: 0.410, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [600/708], Loss: 0.417, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [700/708], Loss: 0.386, Training Accuracy: 85.33%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 59.25979751005184 %\n",
      "Epoch [10/60], Step [100/708], Loss: 0.326, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [200/708], Loss: 0.419, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [300/708], Loss: 0.389, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [400/708], Loss: 0.318, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [500/708], Loss: 0.302, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [600/708], Loss: 0.325, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [700/708], Loss: 0.339, Training Accuracy: 84.78%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 58.59613428280773 %\n",
      "Epoch [11/60], Step [100/708], Loss: 0.327, Training Accuracy: 82.61%\n",
      "Epoch [11/60], Step [200/708], Loss: 0.289, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [300/708], Loss: 0.299, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [400/708], Loss: 0.314, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [500/708], Loss: 0.405, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [600/708], Loss: 0.305, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [700/708], Loss: 0.280, Training Accuracy: 88.04%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 58.039044712493336 %\n",
      "Epoch [12/60], Step [100/708], Loss: 0.345, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [200/708], Loss: 0.336, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [300/708], Loss: 0.331, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [400/708], Loss: 0.313, Training Accuracy: 82.07%\n",
      "Epoch [12/60], Step [500/708], Loss: 0.354, Training Accuracy: 82.61%\n",
      "Epoch [12/60], Step [600/708], Loss: 0.309, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [700/708], Loss: 0.387, Training Accuracy: 85.33%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 61.16520531576482 %\n",
      "Epoch [13/60], Step [100/708], Loss: 0.291, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [200/708], Loss: 0.289, Training Accuracy: 91.30%\n",
      "Epoch [13/60], Step [300/708], Loss: 0.312, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [400/708], Loss: 0.381, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [500/708], Loss: 0.380, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [600/708], Loss: 0.274, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [700/708], Loss: 0.304, Training Accuracy: 89.67%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 59.59889550937364 %\n",
      "Epoch [14/60], Step [100/708], Loss: 0.378, Training Accuracy: 83.15%\n",
      "Epoch [14/60], Step [200/708], Loss: 0.199, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [300/708], Loss: 0.297, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [400/708], Loss: 0.239, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [500/708], Loss: 0.216, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [600/708], Loss: 0.310, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [700/708], Loss: 0.289, Training Accuracy: 93.48%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 60.74859920231233 %\n",
      "Epoch [15/60], Step [100/708], Loss: 0.262, Training Accuracy: 88.04%\n",
      "Epoch [15/60], Step [200/708], Loss: 0.216, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [300/708], Loss: 0.301, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [400/708], Loss: 0.392, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [500/708], Loss: 0.273, Training Accuracy: 86.41%\n",
      "Epoch [15/60], Step [600/708], Loss: 0.295, Training Accuracy: 86.96%\n",
      "Epoch [15/60], Step [700/708], Loss: 0.295, Training Accuracy: 88.04%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 60.39335367921329 %\n",
      "Epoch [16/60], Step [100/708], Loss: 0.325, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [200/708], Loss: 0.296, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [300/708], Loss: 0.239, Training Accuracy: 88.59%\n",
      "Epoch [16/60], Step [400/708], Loss: 0.320, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [500/708], Loss: 0.273, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [600/708], Loss: 0.224, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [700/708], Loss: 0.368, Training Accuracy: 83.70%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 60.24318170808507 %\n",
      "Epoch [17/60], Step [100/708], Loss: 0.318, Training Accuracy: 83.70%\n",
      "Epoch [17/60], Step [200/708], Loss: 0.344, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [300/708], Loss: 0.276, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [400/708], Loss: 0.228, Training Accuracy: 90.76%\n",
      "Epoch [17/60], Step [500/708], Loss: 0.318, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [600/708], Loss: 0.224, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [700/708], Loss: 0.264, Training Accuracy: 87.50%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 59.78782153756721 %\n",
      "Finished early after 17 epochs!\n",
      "Final Training Accuracy 88.89596977910182 %\n",
      "Final Test Accuracy 59.70869867105879 %\n",
      "Epoch [1/60], Step [100/756], Loss: 1.095, Training Accuracy: 60.33%\n",
      "Epoch [1/60], Step [200/756], Loss: 0.925, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [300/756], Loss: 0.923, Training Accuracy: 63.04%\n",
      "Epoch [1/60], Step [400/756], Loss: 0.930, Training Accuracy: 64.13%\n",
      "Epoch [1/60], Step [500/756], Loss: 0.740, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [600/756], Loss: 0.894, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [700/756], Loss: 0.838, Training Accuracy: 72.83%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 66.24428084577583 %\n",
      "Epoch [2/60], Step [100/756], Loss: 0.877, Training Accuracy: 71.20%\n",
      "Epoch [2/60], Step [200/756], Loss: 0.696, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [300/756], Loss: 0.740, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [400/756], Loss: 0.587, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [500/756], Loss: 0.660, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [600/756], Loss: 0.555, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [700/756], Loss: 0.691, Training Accuracy: 82.07%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 67.87294534088983 %\n",
      "Epoch [3/60], Step [100/756], Loss: 0.624, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [200/756], Loss: 0.646, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [300/756], Loss: 0.580, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [400/756], Loss: 0.493, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [500/756], Loss: 0.557, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [600/756], Loss: 0.503, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [700/756], Loss: 0.516, Training Accuracy: 83.70%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 69.41123307789347 %\n",
      "Epoch [4/60], Step [100/756], Loss: 0.474, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [200/756], Loss: 0.512, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [300/756], Loss: 0.535, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [400/756], Loss: 0.510, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [500/756], Loss: 0.567, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [600/756], Loss: 0.521, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [700/756], Loss: 0.396, Training Accuracy: 88.04%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 70.80265858296775 %\n",
      "Epoch [5/60], Step [100/756], Loss: 0.465, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [200/756], Loss: 0.457, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [300/756], Loss: 0.504, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [400/756], Loss: 0.390, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [500/756], Loss: 0.543, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [600/756], Loss: 0.528, Training Accuracy: 78.80%\n",
      "Epoch [5/60], Step [700/756], Loss: 0.405, Training Accuracy: 86.41%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 70.72357891962116 %\n",
      "Epoch [6/60], Step [100/756], Loss: 0.395, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [200/756], Loss: 0.408, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [300/756], Loss: 0.425, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [400/756], Loss: 0.420, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [500/756], Loss: 0.405, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/756], Loss: 0.409, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [700/756], Loss: 0.463, Training Accuracy: 84.78%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 69.62022933102371 %\n",
      "Epoch [7/60], Step [100/756], Loss: 0.342, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [200/756], Loss: 0.411, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [300/756], Loss: 0.392, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [400/756], Loss: 0.450, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [500/756], Loss: 0.329, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [600/756], Loss: 0.408, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [700/756], Loss: 0.457, Training Accuracy: 84.24%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 70.3281806028883 %\n",
      "Epoch [8/60], Step [100/756], Loss: 0.402, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [200/756], Loss: 0.395, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [300/756], Loss: 0.385, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [400/756], Loss: 0.461, Training Accuracy: 80.43%\n",
      "Epoch [8/60], Step [500/756], Loss: 0.317, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [600/756], Loss: 0.423, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [700/756], Loss: 0.394, Training Accuracy: 83.70%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 69.74826307168006 %\n",
      "Epoch [9/60], Step [100/756], Loss: 0.290, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [200/756], Loss: 0.369, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [300/756], Loss: 0.438, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [400/756], Loss: 0.267, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [500/756], Loss: 0.294, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [600/756], Loss: 0.373, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [700/756], Loss: 0.363, Training Accuracy: 88.04%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 70.22085820263221 %\n",
      "Finished early after 9 epochs!\n",
      "Final Training Accuracy 84.94380083274005 %\n",
      "Final Test Accuracy 70.50705126998173 %\n",
      "Epoch [1/60], Step [100/667], Loss: 1.158, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [200/667], Loss: 0.972, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [300/667], Loss: 0.856, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [400/667], Loss: 0.783, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [500/667], Loss: 0.701, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [600/667], Loss: 0.915, Training Accuracy: 71.20%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 48.77036228344873 %\n",
      "Epoch [2/60], Step [100/667], Loss: 0.776, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [200/667], Loss: 0.797, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [300/667], Loss: 0.658, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [400/667], Loss: 0.622, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [500/667], Loss: 0.664, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [600/667], Loss: 0.616, Training Accuracy: 79.89%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 57.29738271037434 %\n",
      "Epoch [3/60], Step [100/667], Loss: 0.643, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [200/667], Loss: 0.606, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [300/667], Loss: 0.583, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [400/667], Loss: 0.565, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [500/667], Loss: 0.587, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [600/667], Loss: 0.510, Training Accuracy: 85.87%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 56.8793633464533 %\n",
      "Epoch [4/60], Step [100/667], Loss: 0.563, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [200/667], Loss: 0.618, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [300/667], Loss: 0.476, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [400/667], Loss: 0.784, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [500/667], Loss: 0.518, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [600/667], Loss: 0.537, Training Accuracy: 81.52%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 59.41189990519149 %\n",
      "Epoch [5/60], Step [100/667], Loss: 0.517, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [200/667], Loss: 0.520, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [300/667], Loss: 0.529, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [400/667], Loss: 0.515, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [500/667], Loss: 0.476, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [600/667], Loss: 0.473, Training Accuracy: 87.50%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 59.57996954635561 %\n",
      "Epoch [6/60], Step [100/667], Loss: 0.641, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [200/667], Loss: 0.393, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [300/667], Loss: 0.453, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [400/667], Loss: 0.489, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [500/667], Loss: 0.392, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [600/667], Loss: 0.407, Training Accuracy: 83.70%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 60.049702646019476 %\n",
      "Epoch [7/60], Step [100/667], Loss: 0.336, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [200/667], Loss: 0.476, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [300/667], Loss: 0.547, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [400/667], Loss: 0.405, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [500/667], Loss: 0.415, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [600/667], Loss: 0.633, Training Accuracy: 83.15%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 61.89128623552733 %\n",
      "Epoch [8/60], Step [100/667], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [200/667], Loss: 0.481, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [300/667], Loss: 0.337, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [400/667], Loss: 0.418, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [500/667], Loss: 0.302, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [600/667], Loss: 0.360, Training Accuracy: 86.96%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 60.96905794811388 %\n",
      "Epoch [9/60], Step [100/667], Loss: 0.357, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [200/667], Loss: 0.451, Training Accuracy: 81.52%\n",
      "Epoch [9/60], Step [300/667], Loss: 0.449, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [400/667], Loss: 0.380, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [500/667], Loss: 0.324, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [600/667], Loss: 0.366, Training Accuracy: 85.87%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 60.785186887695005 %\n",
      "Epoch [10/60], Step [100/667], Loss: 0.276, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [200/667], Loss: 0.372, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [300/667], Loss: 0.391, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [400/667], Loss: 0.309, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [500/667], Loss: 0.271, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [600/667], Loss: 0.282, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 61.558019938518115 %\n",
      "Epoch [11/60], Step [100/667], Loss: 0.303, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [200/667], Loss: 0.299, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [300/667], Loss: 0.421, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [400/667], Loss: 0.318, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [500/667], Loss: 0.301, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [600/667], Loss: 0.316, Training Accuracy: 90.22%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 62.75605481655989 %\n",
      "Epoch [12/60], Step [100/667], Loss: 0.357, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [200/667], Loss: 0.403, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [300/667], Loss: 0.274, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [400/667], Loss: 0.299, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [500/667], Loss: 0.363, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [600/667], Loss: 0.336, Training Accuracy: 83.70%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 62.81638750825983 %\n",
      "Epoch [13/60], Step [100/667], Loss: 0.303, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [200/667], Loss: 0.343, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [300/667], Loss: 0.319, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [400/667], Loss: 0.333, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [500/667], Loss: 0.324, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [600/667], Loss: 0.260, Training Accuracy: 90.76%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 62.28344873157698 %\n",
      "Epoch [14/60], Step [100/667], Loss: 0.281, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [200/667], Loss: 0.230, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [300/667], Loss: 0.247, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [400/667], Loss: 0.322, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [500/667], Loss: 0.279, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [600/667], Loss: 0.415, Training Accuracy: 85.33%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 62.171402304134226 %\n",
      "Epoch [15/60], Step [100/667], Loss: 0.304, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [200/667], Loss: 0.302, Training Accuracy: 84.78%\n",
      "Epoch [15/60], Step [300/667], Loss: 0.342, Training Accuracy: 86.96%\n",
      "Epoch [15/60], Step [400/667], Loss: 0.307, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [500/667], Loss: 0.249, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [600/667], Loss: 0.238, Training Accuracy: 92.93%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 64.09773896055391 %\n",
      "Epoch [16/60], Step [100/667], Loss: 0.259, Training Accuracy: 92.39%\n",
      "Epoch [16/60], Step [200/667], Loss: 0.226, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [300/667], Loss: 0.473, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [400/667], Loss: 0.210, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [500/667], Loss: 0.228, Training Accuracy: 94.02%\n",
      "Epoch [16/60], Step [600/667], Loss: 0.192, Training Accuracy: 90.22%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 63.060591260378665 %\n",
      "Epoch [17/60], Step [100/667], Loss: 0.260, Training Accuracy: 89.13%\n",
      "Epoch [17/60], Step [200/667], Loss: 0.205, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [300/667], Loss: 0.229, Training Accuracy: 89.67%\n",
      "Epoch [17/60], Step [400/667], Loss: 0.239, Training Accuracy: 89.13%\n",
      "Epoch [17/60], Step [500/667], Loss: 0.304, Training Accuracy: 89.67%\n",
      "Epoch [17/60], Step [600/667], Loss: 0.273, Training Accuracy: 86.96%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 63.188439107076164 %\n",
      "Epoch [18/60], Step [100/667], Loss: 0.286, Training Accuracy: 85.87%\n",
      "Epoch [18/60], Step [200/667], Loss: 0.231, Training Accuracy: 92.93%\n",
      "Epoch [18/60], Step [300/667], Loss: 0.308, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [400/667], Loss: 0.242, Training Accuracy: 91.30%\n",
      "Epoch [18/60], Step [500/667], Loss: 0.211, Training Accuracy: 89.67%\n",
      "Epoch [18/60], Step [600/667], Loss: 0.277, Training Accuracy: 89.67%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 63.33496135834746 %\n",
      "Epoch [19/60], Step [100/667], Loss: 0.269, Training Accuracy: 90.76%\n",
      "Epoch [19/60], Step [200/667], Loss: 0.310, Training Accuracy: 88.04%\n",
      "Epoch [19/60], Step [300/667], Loss: 0.223, Training Accuracy: 91.85%\n",
      "Epoch [19/60], Step [400/667], Loss: 0.217, Training Accuracy: 87.50%\n",
      "Epoch [19/60], Step [500/667], Loss: 0.211, Training Accuracy: 91.30%\n",
      "Epoch [19/60], Step [600/667], Loss: 0.219, Training Accuracy: 90.76%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 63.671100640675725 %\n",
      "Epoch [20/60], Step [100/667], Loss: 0.278, Training Accuracy: 88.04%\n",
      "Epoch [20/60], Step [200/667], Loss: 0.194, Training Accuracy: 93.48%\n",
      "Epoch [20/60], Step [300/667], Loss: 0.224, Training Accuracy: 88.59%\n",
      "Epoch [20/60], Step [400/667], Loss: 0.114, Training Accuracy: 96.74%\n",
      "Epoch [20/60], Step [500/667], Loss: 0.284, Training Accuracy: 89.67%\n",
      "Epoch [20/60], Step [600/667], Loss: 0.219, Training Accuracy: 91.30%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 64.518631309794 %\n",
      "Epoch [21/60], Step [100/667], Loss: 0.265, Training Accuracy: 89.67%\n",
      "Epoch [21/60], Step [200/667], Loss: 0.243, Training Accuracy: 90.22%\n",
      "Epoch [21/60], Step [300/667], Loss: 0.187, Training Accuracy: 91.30%\n",
      "Epoch [21/60], Step [400/667], Loss: 0.188, Training Accuracy: 92.93%\n",
      "Epoch [21/60], Step [500/667], Loss: 0.232, Training Accuracy: 89.67%\n",
      "Epoch [21/60], Step [600/667], Loss: 0.321, Training Accuracy: 89.13%\n",
      "Testing epoch 21\n",
      "Epoch 21: Test Accuracy 63.65386272876145 %\n",
      "Epoch [22/60], Step [100/667], Loss: 0.260, Training Accuracy: 93.48%\n",
      "Epoch [22/60], Step [200/667], Loss: 0.230, Training Accuracy: 89.67%\n",
      "Epoch [22/60], Step [300/667], Loss: 0.231, Training Accuracy: 90.76%\n",
      "Epoch [22/60], Step [400/667], Loss: 0.225, Training Accuracy: 92.39%\n",
      "Epoch [22/60], Step [500/667], Loss: 0.256, Training Accuracy: 90.76%\n",
      "Epoch [22/60], Step [600/667], Loss: 0.263, Training Accuracy: 86.41%\n",
      "Testing epoch 22\n",
      "Epoch 22: Test Accuracy 64.95388858562933 %\n",
      "Epoch [23/60], Step [100/667], Loss: 0.283, Training Accuracy: 89.13%\n",
      "Epoch [23/60], Step [200/667], Loss: 0.280, Training Accuracy: 92.93%\n",
      "Epoch [23/60], Step [300/667], Loss: 0.175, Training Accuracy: 91.85%\n",
      "Epoch [23/60], Step [400/667], Loss: 0.246, Training Accuracy: 91.85%\n",
      "Epoch [23/60], Step [500/667], Loss: 0.280, Training Accuracy: 90.22%\n",
      "Epoch [23/60], Step [600/667], Loss: 0.355, Training Accuracy: 88.04%\n",
      "Testing epoch 23\n",
      "Epoch 23: Test Accuracy 61.77780331542506 %\n",
      "Epoch [24/60], Step [100/667], Loss: 0.198, Training Accuracy: 91.85%\n",
      "Epoch [24/60], Step [200/667], Loss: 0.227, Training Accuracy: 93.48%\n",
      "Epoch [24/60], Step [300/667], Loss: 0.272, Training Accuracy: 93.48%\n",
      "Epoch [24/60], Step [400/667], Loss: 0.224, Training Accuracy: 95.65%\n",
      "Epoch [24/60], Step [500/667], Loss: 0.205, Training Accuracy: 93.48%\n",
      "Epoch [24/60], Step [600/667], Loss: 0.218, Training Accuracy: 92.93%\n",
      "Testing epoch 24\n",
      "Epoch 24: Test Accuracy 64.52581377309163 %\n",
      "Epoch [25/60], Step [100/667], Loss: 0.202, Training Accuracy: 90.76%\n",
      "Epoch [25/60], Step [200/667], Loss: 0.161, Training Accuracy: 90.76%\n",
      "Epoch [25/60], Step [300/667], Loss: 0.236, Training Accuracy: 90.22%\n",
      "Epoch [25/60], Step [400/667], Loss: 0.137, Training Accuracy: 92.93%\n",
      "Epoch [25/60], Step [500/667], Loss: 0.233, Training Accuracy: 89.13%\n",
      "Epoch [25/60], Step [600/667], Loss: 0.181, Training Accuracy: 90.76%\n",
      "Testing epoch 25\n",
      "Epoch 25: Test Accuracy 62.556382336886266 %\n",
      "Epoch [26/60], Step [100/667], Loss: 0.152, Training Accuracy: 95.65%\n",
      "Epoch [26/60], Step [200/667], Loss: 0.165, Training Accuracy: 92.93%\n",
      "Epoch [26/60], Step [300/667], Loss: 0.166, Training Accuracy: 91.30%\n",
      "Epoch [26/60], Step [400/667], Loss: 0.237, Training Accuracy: 91.30%\n",
      "Epoch [26/60], Step [500/667], Loss: 0.252, Training Accuracy: 89.67%\n",
      "Epoch [26/60], Step [600/667], Loss: 0.262, Training Accuracy: 90.22%\n",
      "Testing epoch 26\n",
      "Epoch 26: Test Accuracy 63.658172206740026 %\n",
      "Epoch [27/60], Step [100/667], Loss: 0.258, Training Accuracy: 91.30%\n",
      "Epoch [27/60], Step [200/667], Loss: 0.139, Training Accuracy: 94.02%\n",
      "Epoch [27/60], Step [300/667], Loss: 0.263, Training Accuracy: 89.67%\n",
      "Epoch [27/60], Step [400/667], Loss: 0.169, Training Accuracy: 91.30%\n",
      "Epoch [27/60], Step [500/667], Loss: 0.127, Training Accuracy: 95.11%\n",
      "Epoch [27/60], Step [600/667], Loss: 0.194, Training Accuracy: 91.30%\n",
      "Testing epoch 27\n",
      "Epoch 27: Test Accuracy 64.39365644841556 %\n",
      "Finished early after 27 epochs!\n",
      "Final Training Accuracy 92.90283625444695 %\n",
      "Final Test Accuracy 64.2442612118252 %\n",
      "Epoch [1/60], Step [100/667], Loss: 1.196, Training Accuracy: 53.80%\n",
      "Epoch [1/60], Step [200/667], Loss: 0.908, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [300/667], Loss: 0.833, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [400/667], Loss: 0.732, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [500/667], Loss: 0.674, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [600/667], Loss: 0.592, Training Accuracy: 78.80%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 42.75757793937648 %\n",
      "Epoch [2/60], Step [100/667], Loss: 0.585, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [200/667], Loss: 0.526, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [300/667], Loss: 0.468, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [400/667], Loss: 0.473, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [500/667], Loss: 0.543, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [600/667], Loss: 0.456, Training Accuracy: 83.70%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 43.95996832025344 %\n",
      "Epoch [3/60], Step [100/667], Loss: 0.396, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [200/667], Loss: 0.493, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [300/667], Loss: 0.471, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [400/667], Loss: 0.534, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [500/667], Loss: 0.478, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [600/667], Loss: 0.346, Training Accuracy: 83.70%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 44.01324789401685 %\n",
      "Epoch [4/60], Step [100/667], Loss: 0.411, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [200/667], Loss: 0.364, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [300/667], Loss: 0.400, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [400/667], Loss: 0.347, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [500/667], Loss: 0.488, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [600/667], Loss: 0.278, Training Accuracy: 88.04%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 44.92044063647491 %\n",
      "Epoch [5/60], Step [100/667], Loss: 0.350, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [200/667], Loss: 0.347, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [300/667], Loss: 0.354, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [400/667], Loss: 0.308, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [500/667], Loss: 0.444, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [600/667], Loss: 0.335, Training Accuracy: 89.13%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 46.67146662826697 %\n",
      "Epoch [6/60], Step [100/667], Loss: 0.364, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [200/667], Loss: 0.273, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [300/667], Loss: 0.336, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [400/667], Loss: 0.345, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [500/667], Loss: 0.364, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [600/667], Loss: 0.298, Training Accuracy: 89.13%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 45.175318597451216 %\n",
      "Epoch [7/60], Step [100/667], Loss: 0.322, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [200/667], Loss: 0.307, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [300/667], Loss: 0.340, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [400/667], Loss: 0.322, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [500/667], Loss: 0.327, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [600/667], Loss: 0.265, Training Accuracy: 90.22%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 45.686514507883935 %\n",
      "Epoch [8/60], Step [100/667], Loss: 0.302, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [200/667], Loss: 0.268, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [300/667], Loss: 0.349, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [400/667], Loss: 0.292, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [500/667], Loss: 0.228, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [600/667], Loss: 0.164, Training Accuracy: 94.02%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 46.3546691626467 %\n",
      "Epoch [9/60], Step [100/667], Loss: 0.199, Training Accuracy: 94.02%\n",
      "Epoch [9/60], Step [200/667], Loss: 0.310, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [300/667], Loss: 0.252, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [400/667], Loss: 0.271, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [500/667], Loss: 0.203, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [600/667], Loss: 0.278, Training Accuracy: 85.87%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 45.640434876520985 %\n",
      "Epoch [10/60], Step [100/667], Loss: 0.267, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [200/667], Loss: 0.326, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [300/667], Loss: 0.229, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [400/667], Loss: 0.263, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [500/667], Loss: 0.227, Training Accuracy: 91.85%\n",
      "Epoch [10/60], Step [600/667], Loss: 0.274, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 46.10267117863057 %\n",
      "Finished early after 10 epochs!\n",
      "Final Training Accuracy 90.313302098187 %\n",
      "Final Test Accuracy 46.13147094823241 %\n",
      "Epoch [1/60], Step [100/735], Loss: 1.236, Training Accuracy: 59.24%\n",
      "Epoch [1/60], Step [200/735], Loss: 1.155, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [300/735], Loss: 0.888, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [400/735], Loss: 0.898, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [500/735], Loss: 0.662, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [600/735], Loss: 0.869, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [700/735], Loss: 0.713, Training Accuracy: 72.83%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 58.31170653012555 %\n",
      "Epoch [2/60], Step [100/735], Loss: 0.632, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [200/735], Loss: 0.716, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [300/735], Loss: 0.763, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [400/735], Loss: 0.665, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [500/735], Loss: 0.576, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [600/735], Loss: 0.588, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [700/735], Loss: 0.484, Training Accuracy: 83.15%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 60.39314021182577 %\n",
      "Epoch [3/60], Step [100/735], Loss: 0.511, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [200/735], Loss: 0.488, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [300/735], Loss: 0.613, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [400/735], Loss: 0.540, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [500/735], Loss: 0.555, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [600/735], Loss: 0.451, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [700/735], Loss: 0.441, Training Accuracy: 86.41%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 59.88812513151435 %\n",
      "Epoch [4/60], Step [100/735], Loss: 0.531, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [200/735], Loss: 0.528, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [300/735], Loss: 0.346, Training Accuracy: 91.30%\n",
      "Epoch [4/60], Step [400/735], Loss: 0.414, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [500/735], Loss: 0.478, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [600/735], Loss: 0.371, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [700/735], Loss: 0.435, Training Accuracy: 84.78%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 59.547941362137905 %\n",
      "Epoch [5/60], Step [100/735], Loss: 0.449, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [200/735], Loss: 0.374, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [300/735], Loss: 0.364, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [400/735], Loss: 0.338, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [500/735], Loss: 0.448, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [600/735], Loss: 0.452, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [700/735], Loss: 0.336, Training Accuracy: 89.13%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 59.591779476748265 %\n",
      "Epoch [6/60], Step [100/735], Loss: 0.372, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [200/735], Loss: 0.322, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [300/735], Loss: 0.370, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [400/735], Loss: 0.364, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [500/735], Loss: 0.363, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [600/735], Loss: 0.298, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [700/735], Loss: 0.372, Training Accuracy: 86.41%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 61.66093848635757 %\n",
      "Epoch [7/60], Step [100/735], Loss: 0.322, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [200/735], Loss: 0.344, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [300/735], Loss: 0.412, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [400/735], Loss: 0.360, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [500/735], Loss: 0.426, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [600/735], Loss: 0.494, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [700/735], Loss: 0.368, Training Accuracy: 85.87%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 60.903415865890445 %\n",
      "Epoch [8/60], Step [100/735], Loss: 0.312, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [200/735], Loss: 0.304, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [300/735], Loss: 0.374, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [400/735], Loss: 0.375, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [500/735], Loss: 0.324, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [600/735], Loss: 0.321, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [700/735], Loss: 0.266, Training Accuracy: 89.67%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 63.72483692221365 %\n",
      "Epoch [9/60], Step [100/735], Loss: 0.376, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [200/735], Loss: 0.342, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [300/735], Loss: 0.309, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [400/735], Loss: 0.305, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [500/735], Loss: 0.386, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [600/735], Loss: 0.402, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [700/735], Loss: 0.302, Training Accuracy: 92.39%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 64.24212667461597 %\n",
      "Epoch [10/60], Step [100/735], Loss: 0.272, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [200/735], Loss: 0.351, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [300/735], Loss: 0.288, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [400/735], Loss: 0.343, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [500/735], Loss: 0.290, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [600/735], Loss: 0.297, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [700/735], Loss: 0.347, Training Accuracy: 88.04%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 65.77646068597882 %\n",
      "Epoch [11/60], Step [100/735], Loss: 0.221, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [200/735], Loss: 0.298, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [300/735], Loss: 0.315, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [400/735], Loss: 0.344, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [500/735], Loss: 0.259, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [600/735], Loss: 0.365, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [700/735], Loss: 0.233, Training Accuracy: 91.30%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 64.44553552640808 %\n",
      "Epoch [12/60], Step [100/735], Loss: 0.281, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [200/735], Loss: 0.327, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [300/735], Loss: 0.225, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [400/735], Loss: 0.273, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [500/735], Loss: 0.230, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [600/735], Loss: 0.270, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [700/735], Loss: 0.253, Training Accuracy: 91.30%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 63.823034298940875 %\n",
      "Epoch [13/60], Step [100/735], Loss: 0.195, Training Accuracy: 93.48%\n",
      "Epoch [13/60], Step [200/735], Loss: 0.272, Training Accuracy: 92.39%\n",
      "Epoch [13/60], Step [300/735], Loss: 0.299, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [400/735], Loss: 0.268, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [500/735], Loss: 0.213, Training Accuracy: 93.48%\n",
      "Epoch [13/60], Step [600/735], Loss: 0.244, Training Accuracy: 94.02%\n",
      "Epoch [13/60], Step [700/735], Loss: 0.273, Training Accuracy: 91.85%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 63.105842743915275 %\n",
      "Epoch [14/60], Step [100/735], Loss: 0.242, Training Accuracy: 94.02%\n",
      "Epoch [14/60], Step [200/735], Loss: 0.314, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [300/735], Loss: 0.279, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [400/735], Loss: 0.247, Training Accuracy: 94.02%\n",
      "Epoch [14/60], Step [500/735], Loss: 0.237, Training Accuracy: 94.02%\n",
      "Epoch [14/60], Step [600/735], Loss: 0.279, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [700/735], Loss: 0.234, Training Accuracy: 91.85%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 61.61359332257839 %\n",
      "Epoch [15/60], Step [100/735], Loss: 0.261, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [200/735], Loss: 0.194, Training Accuracy: 95.11%\n",
      "Epoch [15/60], Step [300/735], Loss: 0.204, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [400/735], Loss: 0.331, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [500/735], Loss: 0.249, Training Accuracy: 92.93%\n",
      "Epoch [15/60], Step [600/735], Loss: 0.189, Training Accuracy: 94.02%\n",
      "Epoch [15/60], Step [700/735], Loss: 0.268, Training Accuracy: 90.22%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 63.595076102966964 %\n",
      "Finished early after 15 epochs!\n",
      "Final Training Accuracy 91.96696807802164 %\n",
      "Final Test Accuracy 63.67398470926562 %\n",
      "Epoch [1/60], Step [100/656], Loss: 1.072, Training Accuracy: 52.72%\n",
      "Epoch [1/60], Step [200/656], Loss: 1.132, Training Accuracy: 44.57%\n",
      "Epoch [1/60], Step [300/656], Loss: 1.081, Training Accuracy: 66.85%\n",
      "Epoch [1/60], Step [400/656], Loss: 0.928, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [500/656], Loss: 0.729, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [600/656], Loss: 0.750, Training Accuracy: 75.00%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 60.54988106897999 %\n",
      "Epoch [2/60], Step [100/656], Loss: 0.822, Training Accuracy: 66.85%\n",
      "Epoch [2/60], Step [200/656], Loss: 0.723, Training Accuracy: 72.83%\n",
      "Epoch [2/60], Step [300/656], Loss: 0.726, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [400/656], Loss: 0.576, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [500/656], Loss: 0.714, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [600/656], Loss: 0.560, Training Accuracy: 81.52%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 64.79641807751504 %\n",
      "Epoch [3/60], Step [100/656], Loss: 0.568, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [200/656], Loss: 0.669, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [300/656], Loss: 0.496, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [400/656], Loss: 0.533, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [500/656], Loss: 0.608, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [600/656], Loss: 0.588, Training Accuracy: 79.35%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 66.6839233244718 %\n",
      "Epoch [4/60], Step [100/656], Loss: 0.604, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [200/656], Loss: 0.574, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [300/656], Loss: 0.478, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/656], Loss: 0.552, Training Accuracy: 76.63%\n",
      "Epoch [4/60], Step [500/656], Loss: 0.554, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [600/656], Loss: 0.576, Training Accuracy: 75.00%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 66.98894641108157 %\n",
      "Epoch [5/60], Step [100/656], Loss: 0.549, Training Accuracy: 73.91%\n",
      "Epoch [5/60], Step [200/656], Loss: 0.497, Training Accuracy: 78.80%\n",
      "Epoch [5/60], Step [300/656], Loss: 0.633, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [400/656], Loss: 0.427, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [500/656], Loss: 0.455, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [600/656], Loss: 0.381, Training Accuracy: 85.87%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 68.58122289072338 %\n",
      "Epoch [6/60], Step [100/656], Loss: 0.473, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [200/656], Loss: 0.567, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [300/656], Loss: 0.527, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [400/656], Loss: 0.452, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [500/656], Loss: 0.505, Training Accuracy: 79.89%\n",
      "Epoch [6/60], Step [600/656], Loss: 0.456, Training Accuracy: 79.89%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 66.35231565691899 %\n",
      "Epoch [7/60], Step [100/656], Loss: 0.452, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [200/656], Loss: 0.448, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [300/656], Loss: 0.367, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [400/656], Loss: 0.430, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [500/656], Loss: 0.417, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [600/656], Loss: 0.450, Training Accuracy: 83.70%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 67.6535609346579 %\n",
      "Epoch [8/60], Step [100/656], Loss: 0.520, Training Accuracy: 80.43%\n",
      "Epoch [8/60], Step [200/656], Loss: 0.390, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [300/656], Loss: 0.427, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [400/656], Loss: 0.360, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [500/656], Loss: 0.314, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [600/656], Loss: 0.425, Training Accuracy: 85.87%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 66.95396669931439 %\n",
      "Epoch [9/60], Step [100/656], Loss: 0.382, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [200/656], Loss: 0.372, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [300/656], Loss: 0.385, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [400/656], Loss: 0.411, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [500/656], Loss: 0.374, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [600/656], Loss: 0.451, Training Accuracy: 84.24%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 67.59199664194767 %\n",
      "Epoch [10/60], Step [100/656], Loss: 0.405, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [200/656], Loss: 0.375, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [300/656], Loss: 0.333, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [400/656], Loss: 0.325, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [500/656], Loss: 0.372, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [600/656], Loss: 0.440, Training Accuracy: 84.24%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 67.77668952007836 %\n",
      "Finished early after 10 epochs!\n",
      "Final Training Accuracy 86.02236951118476 %\n",
      "Final Test Accuracy 67.76969357772492 %\n",
      "Epoch [1/60], Step [100/699], Loss: 1.168, Training Accuracy: 48.91%\n",
      "Epoch [1/60], Step [200/699], Loss: 0.937, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [300/699], Loss: 0.881, Training Accuracy: 63.59%\n",
      "Epoch [1/60], Step [400/699], Loss: 0.799, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [500/699], Loss: 0.833, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [600/699], Loss: 0.686, Training Accuracy: 72.28%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 48.61006407840181 %\n",
      "Epoch [2/60], Step [100/699], Loss: 0.513, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [200/699], Loss: 0.687, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [300/699], Loss: 0.734, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [400/699], Loss: 0.660, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [500/699], Loss: 0.668, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [600/699], Loss: 0.567, Training Accuracy: 75.54%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 49.92775474305817 %\n",
      "Epoch [3/60], Step [100/699], Loss: 0.555, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [200/699], Loss: 0.528, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [300/699], Loss: 0.573, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [400/699], Loss: 0.491, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [500/699], Loss: 0.404, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [600/699], Loss: 0.587, Training Accuracy: 78.26%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 52.154793315743184 %\n",
      "Epoch [4/60], Step [100/699], Loss: 0.452, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [200/699], Loss: 0.669, Training Accuracy: 69.57%\n",
      "Epoch [4/60], Step [300/699], Loss: 0.440, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/699], Loss: 0.453, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [500/699], Loss: 0.493, Training Accuracy: 73.37%\n",
      "Epoch [4/60], Step [600/699], Loss: 0.490, Training Accuracy: 80.98%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 56.464379947229546 %\n",
      "Epoch [5/60], Step [100/699], Loss: 0.436, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [200/699], Loss: 0.483, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [300/699], Loss: 0.367, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [400/699], Loss: 0.407, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [500/699], Loss: 0.391, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [600/699], Loss: 0.444, Training Accuracy: 78.80%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 55.152971478828995 %\n",
      "Epoch [6/60], Step [100/699], Loss: 0.552, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [200/699], Loss: 0.456, Training Accuracy: 77.17%\n",
      "Epoch [6/60], Step [300/699], Loss: 0.443, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [400/699], Loss: 0.455, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [500/699], Loss: 0.449, Training Accuracy: 79.35%\n",
      "Epoch [6/60], Step [600/699], Loss: 0.487, Training Accuracy: 79.89%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 53.235331071742685 %\n",
      "Epoch [7/60], Step [100/699], Loss: 0.323, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [200/699], Loss: 0.358, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [300/699], Loss: 0.330, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [400/699], Loss: 0.393, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [500/699], Loss: 0.444, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [600/699], Loss: 0.512, Training Accuracy: 79.89%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 52.592976504586005 %\n",
      "Epoch [8/60], Step [100/699], Loss: 0.397, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [200/699], Loss: 0.363, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [300/699], Loss: 0.495, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [400/699], Loss: 0.305, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [500/699], Loss: 0.404, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [600/699], Loss: 0.343, Training Accuracy: 89.67%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 55.24563387360221 %\n",
      "Epoch [9/60], Step [100/699], Loss: 0.284, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [200/699], Loss: 0.347, Training Accuracy: 82.07%\n",
      "Epoch [9/60], Step [300/699], Loss: 0.387, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [400/699], Loss: 0.413, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [500/699], Loss: 0.368, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [600/699], Loss: 0.383, Training Accuracy: 83.70%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 56.52877245885162 %\n",
      "Epoch [10/60], Step [100/699], Loss: 0.370, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [200/699], Loss: 0.310, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [300/699], Loss: 0.360, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [400/699], Loss: 0.390, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [500/699], Loss: 0.387, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [600/699], Loss: 0.381, Training Accuracy: 80.98%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 57.36430456087448 %\n",
      "Epoch [11/60], Step [100/699], Loss: 0.378, Training Accuracy: 81.52%\n",
      "Epoch [11/60], Step [200/699], Loss: 0.354, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [300/699], Loss: 0.267, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [400/699], Loss: 0.426, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [500/699], Loss: 0.310, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [600/699], Loss: 0.270, Training Accuracy: 85.87%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 57.38315114964192 %\n",
      "Epoch [12/60], Step [100/699], Loss: 0.285, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [200/699], Loss: 0.334, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [300/699], Loss: 0.298, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [400/699], Loss: 0.262, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [500/699], Loss: 0.340, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [600/699], Loss: 0.264, Training Accuracy: 88.59%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 56.368576454328434 %\n",
      "Epoch [13/60], Step [100/699], Loss: 0.292, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [200/699], Loss: 0.309, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [300/699], Loss: 0.280, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [400/699], Loss: 0.216, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [500/699], Loss: 0.464, Training Accuracy: 82.61%\n",
      "Epoch [13/60], Step [600/699], Loss: 0.270, Training Accuracy: 91.30%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 56.74707877874104 %\n",
      "Epoch [14/60], Step [100/699], Loss: 0.288, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [200/699], Loss: 0.316, Training Accuracy: 85.33%\n",
      "Epoch [14/60], Step [300/699], Loss: 0.383, Training Accuracy: 82.07%\n",
      "Epoch [14/60], Step [400/699], Loss: 0.232, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [500/699], Loss: 0.349, Training Accuracy: 84.24%\n",
      "Epoch [14/60], Step [600/699], Loss: 0.278, Training Accuracy: 85.87%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 56.82717678100264 %\n",
      "Epoch [15/60], Step [100/699], Loss: 0.282, Training Accuracy: 88.04%\n",
      "Epoch [15/60], Step [200/699], Loss: 0.362, Training Accuracy: 85.33%\n",
      "Epoch [15/60], Step [300/699], Loss: 0.321, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [400/699], Loss: 0.254, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [500/699], Loss: 0.290, Training Accuracy: 84.24%\n",
      "Epoch [15/60], Step [600/699], Loss: 0.246, Training Accuracy: 89.13%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 57.7993466515894 %\n",
      "Epoch [16/60], Step [100/699], Loss: 0.271, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [200/699], Loss: 0.242, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [300/699], Loss: 0.223, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [400/699], Loss: 0.307, Training Accuracy: 85.87%\n",
      "Epoch [16/60], Step [500/699], Loss: 0.275, Training Accuracy: 86.96%\n",
      "Epoch [16/60], Step [600/699], Loss: 0.339, Training Accuracy: 86.41%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 56.58060057796206 %\n",
      "Epoch [17/60], Step [100/699], Loss: 0.227, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [200/699], Loss: 0.235, Training Accuracy: 91.85%\n",
      "Epoch [17/60], Step [300/699], Loss: 0.243, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [400/699], Loss: 0.267, Training Accuracy: 85.87%\n",
      "Epoch [17/60], Step [500/699], Loss: 0.234, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [600/699], Loss: 0.214, Training Accuracy: 90.76%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 57.75851237592663 %\n",
      "Epoch [18/60], Step [100/699], Loss: 0.254, Training Accuracy: 86.41%\n",
      "Epoch [18/60], Step [200/699], Loss: 0.286, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [300/699], Loss: 0.213, Training Accuracy: 89.67%\n",
      "Epoch [18/60], Step [400/699], Loss: 0.264, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [500/699], Loss: 0.223, Training Accuracy: 87.50%\n",
      "Epoch [18/60], Step [600/699], Loss: 0.250, Training Accuracy: 91.30%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 55.78904384972987 %\n",
      "Epoch [19/60], Step [100/699], Loss: 0.292, Training Accuracy: 88.04%\n",
      "Epoch [19/60], Step [200/699], Loss: 0.209, Training Accuracy: 91.30%\n",
      "Epoch [19/60], Step [300/699], Loss: 0.174, Training Accuracy: 90.76%\n",
      "Epoch [19/60], Step [400/699], Loss: 0.314, Training Accuracy: 86.96%\n",
      "Epoch [19/60], Step [500/699], Loss: 0.261, Training Accuracy: 89.13%\n",
      "Epoch [19/60], Step [600/699], Loss: 0.235, Training Accuracy: 85.33%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 56.39998743560749 %\n",
      "Epoch [20/60], Step [100/699], Loss: 0.209, Training Accuracy: 91.30%\n",
      "Epoch [20/60], Step [200/699], Loss: 0.331, Training Accuracy: 86.41%\n",
      "Epoch [20/60], Step [300/699], Loss: 0.246, Training Accuracy: 87.50%\n",
      "Epoch [20/60], Step [400/699], Loss: 0.290, Training Accuracy: 88.04%\n",
      "Epoch [20/60], Step [500/699], Loss: 0.217, Training Accuracy: 91.30%\n",
      "Epoch [20/60], Step [600/699], Loss: 0.290, Training Accuracy: 86.96%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 56.30575449177032 %\n",
      "Finished early after 20 epochs!\n",
      "Final Training Accuracy 89.18893679279056 %\n",
      "Final Test Accuracy 56.33559492398542 %\n",
      "Epoch [1/60], Step [100/702], Loss: 1.157, Training Accuracy: 53.26%\n",
      "Epoch [1/60], Step [200/702], Loss: 0.987, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [300/702], Loss: 0.897, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [400/702], Loss: 0.908, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [500/702], Loss: 0.824, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [600/702], Loss: 0.834, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [700/702], Loss: 0.725, Training Accuracy: 76.09%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 57.20622247568355 %\n",
      "Epoch [2/60], Step [100/702], Loss: 0.651, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [200/702], Loss: 0.737, Training Accuracy: 70.11%\n",
      "Epoch [2/60], Step [300/702], Loss: 0.620, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [400/702], Loss: 0.614, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [500/702], Loss: 0.674, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [600/702], Loss: 0.565, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [700/702], Loss: 0.611, Training Accuracy: 80.98%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 59.62519405633178 %\n",
      "Epoch [3/60], Step [100/702], Loss: 0.554, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [200/702], Loss: 0.643, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [300/702], Loss: 0.610, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [400/702], Loss: 0.542, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [500/702], Loss: 0.460, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [600/702], Loss: 0.639, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [700/702], Loss: 0.579, Training Accuracy: 82.61%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 60.47904191616767 %\n",
      "Epoch [4/60], Step [100/702], Loss: 0.472, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [200/702], Loss: 0.529, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [300/702], Loss: 0.460, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [400/702], Loss: 0.682, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [500/702], Loss: 0.537, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [600/702], Loss: 0.452, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [700/702], Loss: 0.464, Training Accuracy: 84.24%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 66.82507999873269 %\n",
      "Epoch [5/60], Step [100/702], Loss: 0.456, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [200/702], Loss: 0.499, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [300/702], Loss: 0.587, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [400/702], Loss: 0.422, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [500/702], Loss: 0.509, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [600/702], Loss: 0.492, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [700/702], Loss: 0.494, Training Accuracy: 82.61%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 64.48373095079681 %\n",
      "Epoch [6/60], Step [100/702], Loss: 0.453, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [200/702], Loss: 0.401, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [300/702], Loss: 0.501, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [400/702], Loss: 0.436, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [500/702], Loss: 0.648, Training Accuracy: 77.72%\n",
      "Epoch [6/60], Step [600/702], Loss: 0.460, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [700/702], Loss: 0.445, Training Accuracy: 84.24%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 68.20644425434844 %\n",
      "Epoch [7/60], Step [100/702], Loss: 0.345, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [200/702], Loss: 0.417, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [300/702], Loss: 0.458, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [400/702], Loss: 0.483, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [500/702], Loss: 0.416, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [600/702], Loss: 0.383, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [700/702], Loss: 0.393, Training Accuracy: 84.78%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 67.12448119633748 %\n",
      "Epoch [8/60], Step [100/702], Loss: 0.340, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [200/702], Loss: 0.390, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [300/702], Loss: 0.384, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [400/702], Loss: 0.337, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [500/702], Loss: 0.279, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [600/702], Loss: 0.435, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [700/702], Loss: 0.448, Training Accuracy: 84.78%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 65.33599467731204 %\n",
      "Epoch [9/60], Step [100/702], Loss: 0.332, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [200/702], Loss: 0.300, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [300/702], Loss: 0.339, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [400/702], Loss: 0.406, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [500/702], Loss: 0.334, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [600/702], Loss: 0.430, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [700/702], Loss: 0.478, Training Accuracy: 83.15%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 66.20251560371321 %\n",
      "Epoch [10/60], Step [100/702], Loss: 0.285, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [200/702], Loss: 0.449, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [300/702], Loss: 0.418, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [400/702], Loss: 0.301, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [500/702], Loss: 0.404, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [600/702], Loss: 0.276, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [700/702], Loss: 0.433, Training Accuracy: 83.70%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 66.49716440135602 %\n",
      "Epoch [11/60], Step [100/702], Loss: 0.397, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [200/702], Loss: 0.344, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [300/702], Loss: 0.374, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [400/702], Loss: 0.325, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [500/702], Loss: 0.370, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [600/702], Loss: 0.416, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [700/702], Loss: 0.305, Training Accuracy: 87.50%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 67.20210372904984 %\n",
      "Finished early after 11 epochs!\n",
      "Final Training Accuracy 88.31483834971017 %\n",
      "Final Test Accuracy 67.14665906282673 %\n",
      "Epoch [1/60], Step [100/663], Loss: 1.113, Training Accuracy: 59.78%\n",
      "Epoch [1/60], Step [200/663], Loss: 0.968, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [300/663], Loss: 0.871, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [400/663], Loss: 0.840, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [500/663], Loss: 0.709, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [600/663], Loss: 0.724, Training Accuracy: 75.54%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 50.105341077326045 %\n",
      "Epoch [2/60], Step [100/663], Loss: 0.620, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [200/663], Loss: 0.614, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [300/663], Loss: 0.577, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [400/663], Loss: 0.665, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [500/663], Loss: 0.863, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [600/663], Loss: 0.563, Training Accuracy: 83.70%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 51.25270470333675 %\n",
      "Epoch [3/60], Step [100/663], Loss: 0.552, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [200/663], Loss: 0.614, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [300/663], Loss: 0.467, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [400/663], Loss: 0.732, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [500/663], Loss: 0.500, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [600/663], Loss: 0.587, Training Accuracy: 80.98%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 49.94021182097711 %\n",
      "Epoch [4/60], Step [100/663], Loss: 0.424, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [200/663], Loss: 0.446, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [300/663], Loss: 0.539, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [400/663], Loss: 0.438, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [500/663], Loss: 0.468, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [600/663], Loss: 0.394, Training Accuracy: 85.33%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 49.67401207151805 %\n",
      "Epoch [5/60], Step [100/663], Loss: 0.517, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [200/663], Loss: 0.517, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [300/663], Loss: 0.426, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [400/663], Loss: 0.381, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [500/663], Loss: 0.562, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [600/663], Loss: 0.397, Training Accuracy: 85.87%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 50.81425805716889 %\n",
      "Epoch [6/60], Step [100/663], Loss: 0.445, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [200/663], Loss: 0.412, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [300/663], Loss: 0.422, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [400/663], Loss: 0.361, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [500/663], Loss: 0.416, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [600/663], Loss: 0.378, Training Accuracy: 86.41%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 50.6192347113085 %\n",
      "Epoch [7/60], Step [100/663], Loss: 0.422, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [200/663], Loss: 0.366, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [300/663], Loss: 0.490, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [400/663], Loss: 0.466, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [500/663], Loss: 0.411, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [600/663], Loss: 0.392, Training Accuracy: 89.13%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 50.88401093269559 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 86.23956299929463 %\n",
      "Final Test Accuracy 50.952340280150324 %\n",
      "Epoch [1/60], Step [100/725], Loss: 1.197, Training Accuracy: 58.70%\n",
      "Epoch [1/60], Step [200/725], Loss: 1.021, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [300/725], Loss: 0.886, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [400/725], Loss: 0.846, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [500/725], Loss: 0.744, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [600/725], Loss: 0.630, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [700/725], Loss: 0.698, Training Accuracy: 72.83%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 70.0778964555412 %\n",
      "Epoch [2/60], Step [100/725], Loss: 0.641, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [200/725], Loss: 0.578, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [300/725], Loss: 0.639, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [400/725], Loss: 0.570, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [500/725], Loss: 0.662, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [600/725], Loss: 0.571, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [700/725], Loss: 0.570, Training Accuracy: 75.54%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 72.10864684672427 %\n",
      "Epoch [3/60], Step [100/725], Loss: 0.478, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [200/725], Loss: 0.517, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [300/725], Loss: 0.517, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [400/725], Loss: 0.461, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [500/725], Loss: 0.578, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [600/725], Loss: 0.429, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [700/725], Loss: 0.367, Training Accuracy: 84.24%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 72.87400503435609 %\n",
      "Epoch [4/60], Step [100/725], Loss: 0.533, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [200/725], Loss: 0.375, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [300/725], Loss: 0.400, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [400/725], Loss: 0.471, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [500/725], Loss: 0.387, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [600/725], Loss: 0.421, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [700/725], Loss: 0.388, Training Accuracy: 86.41%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 71.56439213551943 %\n",
      "Epoch [5/60], Step [100/725], Loss: 0.414, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [200/725], Loss: 0.414, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [300/725], Loss: 0.394, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [400/725], Loss: 0.393, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [500/725], Loss: 0.436, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [600/725], Loss: 0.418, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [700/725], Loss: 0.378, Training Accuracy: 84.24%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 71.70215660929314 %\n",
      "Epoch [6/60], Step [100/725], Loss: 0.366, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [200/725], Loss: 0.398, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [300/725], Loss: 0.452, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [400/725], Loss: 0.321, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [500/725], Loss: 0.444, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [600/725], Loss: 0.338, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [700/725], Loss: 0.368, Training Accuracy: 86.96%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 70.42485883393428 %\n",
      "Epoch [7/60], Step [100/725], Loss: 0.280, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [200/725], Loss: 0.459, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [300/725], Loss: 0.371, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [400/725], Loss: 0.379, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [500/725], Loss: 0.307, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [600/725], Loss: 0.357, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [700/725], Loss: 0.269, Training Accuracy: 89.67%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 70.11021157901898 %\n",
      "Epoch [8/60], Step [100/725], Loss: 0.358, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [200/725], Loss: 0.232, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [300/725], Loss: 0.325, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [400/725], Loss: 0.295, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [500/725], Loss: 0.376, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [600/725], Loss: 0.178, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [700/725], Loss: 0.235, Training Accuracy: 89.67%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 72.14436356214708 %\n",
      "Finished early after 8 epochs!\n",
      "Final Training Accuracy 88.62896816470976 %\n",
      "Final Test Accuracy 72.08313490713654 %\n",
      "By Spectra accuracy: 0.6009552474016278 +/- 0.07554426763780263\n",
      "By Sample simple accuracy: 0.6116859290233904 +/- 0.1313188959622262\n",
      "By Sample simple log loss: 1.5761604660320618 +/- 1.0341153105180743\n"
     ]
    }
   ],
   "source": [
    "# 5x3 CV, All data together, one centre at a time centre\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "n_repeats = 5\n",
    "folds = 3\n",
    "\n",
    "outer_accuracy = []\n",
    "outer_log_loss = []\n",
    "simple_sample_accuracy = []\n",
    "simple_sample_logloss = []\n",
    "\n",
    "\n",
    "for cen in np.unique(SMART_centre):\n",
    "    centre_x = SMART_x[SMART_centre == cen]\n",
    "    centre_y = SMART_y[SMART_centre == cen]\n",
    "    centre_patient = SMART_patient[SMART_centre == cen]\n",
    "    centre_sample = SMART_uniquemapID[SMART_centre == cen]\n",
    "    print(\"Centre: {}\".format(list(Centre.keys())[list(Centre.values()).index(cen)]))\n",
    "    #MapandSpectraCount(centre_y, centre_patient, Raw_labels)\n",
    "    counter = 0\n",
    "    while counter < n_repeats*folds:\n",
    "        sgkf = StratifiedGroupKFold(folds, shuffle = True)\n",
    "        for train_index, test_index in sgkf.split(centre_x, centre_y, groups=centre_patient): #(note switching of test/train indices here as we are leaving out 2 centres)\n",
    "\n",
    "            X_train, X_test = centre_x[train_index], centre_x[test_index]\n",
    "            y_train, y_test = centre_y[train_index], centre_y[test_index]\n",
    "            group_train, group_test = centre_sample[train_index], centre_sample[test_index]\n",
    "\n",
    "            convnet, con_mat, test_ll, test_acc, train_ll, train_acc, test_outputs, test_predicted  = train_cnn(ConvNet, X_train, y_train, X_test, y_test, learning_rate = learning_rate, num_epochs = n_epochs, batch_size = batch_size, early_stop = 5, weightedLoss = True, learning_curves = False)\n",
    "\n",
    "            outer_accuracy.append(test_acc)\n",
    "            outer_log_loss.append(test_ll)\n",
    "\n",
    "            y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_whole(convnet, group_test, X_test, y_test, SMART_y, proportioned = False )\n",
    "\n",
    "            acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "            ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y) )\n",
    "            simple_sample_accuracy.append(acc)\n",
    "            simple_sample_logloss.append(ll)\n",
    "\n",
    "\n",
    "            counter = counter+1\n",
    "    print(\"By Spectra accuracy: {} +/- {}\".format(np.mean(outer_accuracy),np.std(outer_accuracy)))\n",
    "    print(\"By Sample simple accuracy: {} +/- {}\".format(np.mean(simple_sample_accuracy),np.std(simple_sample_accuracy)))\n",
    "    print(\"By Sample simple log loss: {} +/- {}\".format(np.mean(simple_sample_logloss),np.std(simple_sample_logloss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3826551,
     "status": "ok",
     "timestamp": 1680545411458,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "9TbOZh7uJl90",
    "outputId": "74ebdc1e-b559-4455-b3d8-ade6c1f2d4ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Step [100/2007], Loss: 0.954, Training Accuracy: 66.85%\n",
      "Epoch [1/60], Step [200/2007], Loss: 0.837, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [300/2007], Loss: 0.844, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [400/2007], Loss: 0.815, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [500/2007], Loss: 0.716, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [600/2007], Loss: 0.647, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [700/2007], Loss: 0.719, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [800/2007], Loss: 0.737, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [900/2007], Loss: 0.708, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [1000/2007], Loss: 0.639, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [1100/2007], Loss: 0.550, Training Accuracy: 82.61%\n",
      "Epoch [1/60], Step [1200/2007], Loss: 0.626, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1300/2007], Loss: 0.583, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [1400/2007], Loss: 0.586, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [1500/2007], Loss: 0.623, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [1600/2007], Loss: 0.681, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [1700/2007], Loss: 0.503, Training Accuracy: 83.15%\n",
      "Epoch [1/60], Step [1800/2007], Loss: 0.581, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [1900/2007], Loss: 0.521, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [2000/2007], Loss: 0.564, Training Accuracy: 80.43%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 54.031411972878594 %\n",
      "Epoch [2/60], Step [100/2007], Loss: 0.519, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [200/2007], Loss: 0.611, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [300/2007], Loss: 0.616, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [400/2007], Loss: 0.469, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [500/2007], Loss: 0.621, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [600/2007], Loss: 0.538, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [700/2007], Loss: 0.431, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [800/2007], Loss: 0.518, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [900/2007], Loss: 0.497, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1000/2007], Loss: 0.581, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1100/2007], Loss: 0.495, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [1200/2007], Loss: 0.560, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1300/2007], Loss: 0.467, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [1400/2007], Loss: 0.611, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [1500/2007], Loss: 0.482, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [1600/2007], Loss: 0.467, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1700/2007], Loss: 0.477, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1800/2007], Loss: 0.495, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1900/2007], Loss: 0.431, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [2000/2007], Loss: 0.441, Training Accuracy: 81.52%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 53.2662083796579 %\n",
      "Epoch [3/60], Step [100/2007], Loss: 0.523, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [200/2007], Loss: 0.444, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [300/2007], Loss: 0.421, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [400/2007], Loss: 0.486, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [500/2007], Loss: 0.420, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [600/2007], Loss: 0.352, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [700/2007], Loss: 0.395, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [800/2007], Loss: 0.464, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [900/2007], Loss: 0.450, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1000/2007], Loss: 0.456, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [1100/2007], Loss: 0.387, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [1200/2007], Loss: 0.398, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1300/2007], Loss: 0.333, Training Accuracy: 89.67%\n",
      "Epoch [3/60], Step [1400/2007], Loss: 0.428, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1500/2007], Loss: 0.386, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1600/2007], Loss: 0.485, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1700/2007], Loss: 0.431, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1800/2007], Loss: 0.341, Training Accuracy: 89.67%\n",
      "Epoch [3/60], Step [1900/2007], Loss: 0.463, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [2000/2007], Loss: 0.542, Training Accuracy: 75.54%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 54.34824592996247 %\n",
      "Epoch [4/60], Step [100/2007], Loss: 0.415, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [200/2007], Loss: 0.380, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [300/2007], Loss: 0.495, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/2007], Loss: 0.431, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [500/2007], Loss: 0.402, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [600/2007], Loss: 0.361, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [700/2007], Loss: 0.411, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [800/2007], Loss: 0.443, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [900/2007], Loss: 0.298, Training Accuracy: 90.22%\n",
      "Epoch [4/60], Step [1000/2007], Loss: 0.363, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1100/2007], Loss: 0.415, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1200/2007], Loss: 0.443, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1300/2007], Loss: 0.388, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1400/2007], Loss: 0.474, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [1500/2007], Loss: 0.348, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [1600/2007], Loss: 0.243, Training Accuracy: 93.48%\n",
      "Epoch [4/60], Step [1700/2007], Loss: 0.471, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1800/2007], Loss: 0.480, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [1900/2007], Loss: 0.360, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [2000/2007], Loss: 0.412, Training Accuracy: 85.87%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 54.461512764702505 %\n",
      "Epoch [5/60], Step [100/2007], Loss: 0.437, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [200/2007], Loss: 0.456, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [300/2007], Loss: 0.497, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [400/2007], Loss: 0.398, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [500/2007], Loss: 0.315, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [600/2007], Loss: 0.461, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [700/2007], Loss: 0.391, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [800/2007], Loss: 0.370, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [900/2007], Loss: 0.575, Training Accuracy: 76.63%\n",
      "Epoch [5/60], Step [1000/2007], Loss: 0.398, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1100/2007], Loss: 0.479, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [1200/2007], Loss: 0.424, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1300/2007], Loss: 0.410, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1400/2007], Loss: 0.402, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1500/2007], Loss: 0.388, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [1600/2007], Loss: 0.319, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [1700/2007], Loss: 0.401, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1800/2007], Loss: 0.318, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [1900/2007], Loss: 0.489, Training Accuracy: 78.80%\n",
      "Epoch [5/60], Step [2000/2007], Loss: 0.357, Training Accuracy: 86.96%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 54.56434025983516 %\n",
      "Epoch [6/60], Step [100/2007], Loss: 0.330, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [200/2007], Loss: 0.407, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [300/2007], Loss: 0.445, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [400/2007], Loss: 0.421, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [500/2007], Loss: 0.385, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/2007], Loss: 0.439, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [700/2007], Loss: 0.378, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [800/2007], Loss: 0.367, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [900/2007], Loss: 0.417, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1000/2007], Loss: 0.362, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1100/2007], Loss: 0.311, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [1200/2007], Loss: 0.391, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1300/2007], Loss: 0.361, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1400/2007], Loss: 0.430, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1500/2007], Loss: 0.431, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [1600/2007], Loss: 0.373, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1700/2007], Loss: 0.346, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1800/2007], Loss: 0.267, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1900/2007], Loss: 0.414, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [2000/2007], Loss: 0.392, Training Accuracy: 83.70%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 55.8494229655032 %\n",
      "Epoch [7/60], Step [100/2007], Loss: 0.332, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [200/2007], Loss: 0.351, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [300/2007], Loss: 0.487, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [400/2007], Loss: 0.396, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [500/2007], Loss: 0.349, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [600/2007], Loss: 0.410, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [700/2007], Loss: 0.434, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [800/2007], Loss: 0.310, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [900/2007], Loss: 0.315, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [1000/2007], Loss: 0.379, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1100/2007], Loss: 0.308, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1200/2007], Loss: 0.248, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1300/2007], Loss: 0.394, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [1400/2007], Loss: 0.361, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [1500/2007], Loss: 0.387, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [1600/2007], Loss: 0.406, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [1700/2007], Loss: 0.394, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [1800/2007], Loss: 0.391, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1900/2007], Loss: 0.377, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [2000/2007], Loss: 0.366, Training Accuracy: 86.41%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 54.02097263327123 %\n",
      "Epoch [8/60], Step [100/2007], Loss: 0.284, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [200/2007], Loss: 0.277, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [300/2007], Loss: 0.305, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [400/2007], Loss: 0.333, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [500/2007], Loss: 0.356, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [600/2007], Loss: 0.346, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [700/2007], Loss: 0.268, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [800/2007], Loss: 0.276, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [900/2007], Loss: 0.289, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1000/2007], Loss: 0.317, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1100/2007], Loss: 0.386, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1200/2007], Loss: 0.319, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1300/2007], Loss: 0.389, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [1400/2007], Loss: 0.258, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1500/2007], Loss: 0.363, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1600/2007], Loss: 0.286, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [1700/2007], Loss: 0.373, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1800/2007], Loss: 0.303, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1900/2007], Loss: 0.367, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [2000/2007], Loss: 0.346, Training Accuracy: 85.33%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 55.89326819185418 %\n",
      "Epoch [9/60], Step [100/2007], Loss: 0.323, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [200/2007], Loss: 0.291, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [300/2007], Loss: 0.295, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [400/2007], Loss: 0.254, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [500/2007], Loss: 0.309, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [600/2007], Loss: 0.323, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [700/2007], Loss: 0.323, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [800/2007], Loss: 0.316, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [900/2007], Loss: 0.245, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [1000/2007], Loss: 0.312, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1100/2007], Loss: 0.311, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [1200/2007], Loss: 0.376, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [1300/2007], Loss: 0.318, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [1400/2007], Loss: 0.327, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1500/2007], Loss: 0.343, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [1600/2007], Loss: 0.306, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1700/2007], Loss: 0.336, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [1800/2007], Loss: 0.301, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1900/2007], Loss: 0.330, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [2000/2007], Loss: 0.293, Training Accuracy: 88.04%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 54.86760307542945 %\n",
      "Epoch [10/60], Step [100/2007], Loss: 0.308, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [200/2007], Loss: 0.362, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [300/2007], Loss: 0.316, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [400/2007], Loss: 0.429, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [500/2007], Loss: 0.323, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [600/2007], Loss: 0.365, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [700/2007], Loss: 0.274, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [800/2007], Loss: 0.306, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [900/2007], Loss: 0.224, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [1000/2007], Loss: 0.324, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1100/2007], Loss: 0.418, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [1200/2007], Loss: 0.243, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [1300/2007], Loss: 0.281, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [1400/2007], Loss: 0.335, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [1500/2007], Loss: 0.307, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [1600/2007], Loss: 0.309, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1700/2007], Loss: 0.250, Training Accuracy: 91.85%\n",
      "Epoch [10/60], Step [1800/2007], Loss: 0.389, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [1900/2007], Loss: 0.330, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [2000/2007], Loss: 0.302, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 55.517973932969 %\n",
      "Epoch [11/60], Step [100/2007], Loss: 0.237, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [200/2007], Loss: 0.229, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [300/2007], Loss: 0.321, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [400/2007], Loss: 0.246, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [500/2007], Loss: 0.277, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [600/2007], Loss: 0.284, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [700/2007], Loss: 0.333, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [800/2007], Loss: 0.311, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [900/2007], Loss: 0.257, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [1000/2007], Loss: 0.290, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1100/2007], Loss: 0.249, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [1200/2007], Loss: 0.364, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1300/2007], Loss: 0.299, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1400/2007], Loss: 0.218, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [1500/2007], Loss: 0.295, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [1600/2007], Loss: 0.304, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1700/2007], Loss: 0.390, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [1800/2007], Loss: 0.264, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [1900/2007], Loss: 0.335, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [2000/2007], Loss: 0.238, Training Accuracy: 90.76%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 55.24916093807906 %\n",
      "Epoch [12/60], Step [100/2007], Loss: 0.292, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [200/2007], Loss: 0.261, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [300/2007], Loss: 0.301, Training Accuracy: 87.50%\n",
      "Epoch [12/60], Step [400/2007], Loss: 0.336, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [500/2007], Loss: 0.301, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [600/2007], Loss: 0.285, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [700/2007], Loss: 0.269, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [800/2007], Loss: 0.255, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [900/2007], Loss: 0.288, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1000/2007], Loss: 0.321, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [1100/2007], Loss: 0.276, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [1200/2007], Loss: 0.323, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [1300/2007], Loss: 0.226, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [1400/2007], Loss: 0.277, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1500/2007], Loss: 0.222, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [1600/2007], Loss: 0.246, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [1700/2007], Loss: 0.315, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1800/2007], Loss: 0.251, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [1900/2007], Loss: 0.222, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [2000/2007], Loss: 0.303, Training Accuracy: 88.59%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 56.416279106183744 %\n",
      "Epoch [13/60], Step [100/2007], Loss: 0.223, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [200/2007], Loss: 0.325, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [300/2007], Loss: 0.308, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [400/2007], Loss: 0.310, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [500/2007], Loss: 0.266, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [600/2007], Loss: 0.320, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [700/2007], Loss: 0.263, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [800/2007], Loss: 0.250, Training Accuracy: 92.39%\n",
      "Epoch [13/60], Step [900/2007], Loss: 0.310, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [1000/2007], Loss: 0.322, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [1100/2007], Loss: 0.256, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [1200/2007], Loss: 0.285, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [1300/2007], Loss: 0.269, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [1400/2007], Loss: 0.293, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [1500/2007], Loss: 0.214, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [1600/2007], Loss: 0.326, Training Accuracy: 83.70%\n",
      "Epoch [13/60], Step [1700/2007], Loss: 0.210, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [1800/2007], Loss: 0.269, Training Accuracy: 92.39%\n",
      "Epoch [13/60], Step [1900/2007], Loss: 0.297, Training Accuracy: 85.87%\n",
      "Epoch [13/60], Step [2000/2007], Loss: 0.309, Training Accuracy: 88.59%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 57.18304860034554 %\n",
      "Epoch [14/60], Step [100/2007], Loss: 0.314, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [200/2007], Loss: 0.297, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [300/2007], Loss: 0.260, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [400/2007], Loss: 0.294, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [500/2007], Loss: 0.360, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [600/2007], Loss: 0.319, Training Accuracy: 85.33%\n",
      "Epoch [14/60], Step [700/2007], Loss: 0.224, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [800/2007], Loss: 0.289, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [900/2007], Loss: 0.328, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [1000/2007], Loss: 0.243, Training Accuracy: 91.30%\n",
      "Epoch [14/60], Step [1100/2007], Loss: 0.287, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [1200/2007], Loss: 0.225, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [1300/2007], Loss: 0.343, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [1400/2007], Loss: 0.337, Training Accuracy: 86.96%\n",
      "Epoch [14/60], Step [1500/2007], Loss: 0.289, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [1600/2007], Loss: 0.261, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [1700/2007], Loss: 0.277, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [1800/2007], Loss: 0.275, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [1900/2007], Loss: 0.279, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [2000/2007], Loss: 0.389, Training Accuracy: 83.15%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 55.57069259798625 %\n",
      "Epoch [15/60], Step [100/2007], Loss: 0.225, Training Accuracy: 91.85%\n",
      "Epoch [15/60], Step [200/2007], Loss: 0.352, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [300/2007], Loss: 0.232, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [400/2007], Loss: 0.357, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [500/2007], Loss: 0.241, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [600/2007], Loss: 0.289, Training Accuracy: 91.85%\n",
      "Epoch [15/60], Step [700/2007], Loss: 0.291, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [800/2007], Loss: 0.207, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [900/2007], Loss: 0.282, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [1000/2007], Loss: 0.246, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [1100/2007], Loss: 0.338, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [1200/2007], Loss: 0.255, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [1300/2007], Loss: 0.242, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [1400/2007], Loss: 0.260, Training Accuracy: 91.85%\n",
      "Epoch [15/60], Step [1500/2007], Loss: 0.304, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [1600/2007], Loss: 0.172, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [1700/2007], Loss: 0.314, Training Accuracy: 87.50%\n",
      "Epoch [15/60], Step [1800/2007], Loss: 0.360, Training Accuracy: 83.70%\n",
      "Epoch [15/60], Step [1900/2007], Loss: 0.207, Training Accuracy: 91.85%\n",
      "Epoch [15/60], Step [2000/2007], Loss: 0.261, Training Accuracy: 89.67%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 54.662992019124864 %\n",
      "Epoch [16/60], Step [100/2007], Loss: 0.280, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [200/2007], Loss: 0.237, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [300/2007], Loss: 0.303, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [400/2007], Loss: 0.267, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [500/2007], Loss: 0.303, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [600/2007], Loss: 0.240, Training Accuracy: 88.59%\n",
      "Epoch [16/60], Step [700/2007], Loss: 0.331, Training Accuracy: 85.87%\n",
      "Epoch [16/60], Step [800/2007], Loss: 0.286, Training Accuracy: 91.85%\n",
      "Epoch [16/60], Step [900/2007], Loss: 0.257, Training Accuracy: 90.76%\n",
      "Epoch [16/60], Step [1000/2007], Loss: 0.268, Training Accuracy: 92.39%\n",
      "Epoch [16/60], Step [1100/2007], Loss: 0.203, Training Accuracy: 91.85%\n",
      "Epoch [16/60], Step [1200/2007], Loss: 0.358, Training Accuracy: 87.50%\n",
      "Epoch [16/60], Step [1300/2007], Loss: 0.226, Training Accuracy: 91.85%\n",
      "Epoch [16/60], Step [1400/2007], Loss: 0.226, Training Accuracy: 92.93%\n",
      "Epoch [16/60], Step [1500/2007], Loss: 0.247, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [1600/2007], Loss: 0.324, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [1700/2007], Loss: 0.284, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [1800/2007], Loss: 0.297, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [1900/2007], Loss: 0.231, Training Accuracy: 90.76%\n",
      "Epoch [16/60], Step [2000/2007], Loss: 0.248, Training Accuracy: 91.85%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 55.14006983918197 %\n",
      "Epoch [17/60], Step [100/2007], Loss: 0.231, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [200/2007], Loss: 0.324, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [300/2007], Loss: 0.203, Training Accuracy: 94.57%\n",
      "Epoch [17/60], Step [400/2007], Loss: 0.282, Training Accuracy: 92.39%\n",
      "Epoch [17/60], Step [500/2007], Loss: 0.282, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [600/2007], Loss: 0.227, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [700/2007], Loss: 0.323, Training Accuracy: 89.67%\n",
      "Epoch [17/60], Step [800/2007], Loss: 0.259, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [900/2007], Loss: 0.255, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [1000/2007], Loss: 0.267, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [1100/2007], Loss: 0.188, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [1200/2007], Loss: 0.250, Training Accuracy: 92.39%\n",
      "Epoch [17/60], Step [1300/2007], Loss: 0.305, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [1400/2007], Loss: 0.231, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [1500/2007], Loss: 0.217, Training Accuracy: 92.93%\n",
      "Epoch [17/60], Step [1600/2007], Loss: 0.291, Training Accuracy: 88.59%\n",
      "Epoch [17/60], Step [1700/2007], Loss: 0.231, Training Accuracy: 91.85%\n",
      "Epoch [17/60], Step [1800/2007], Loss: 0.197, Training Accuracy: 92.39%\n",
      "Epoch [17/60], Step [1900/2007], Loss: 0.281, Training Accuracy: 88.59%\n",
      "Epoch [17/60], Step [2000/2007], Loss: 0.247, Training Accuracy: 92.93%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 54.355553467687635 %\n",
      "Epoch [18/60], Step [100/2007], Loss: 0.249, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [200/2007], Loss: 0.218, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [300/2007], Loss: 0.223, Training Accuracy: 91.30%\n",
      "Epoch [18/60], Step [400/2007], Loss: 0.265, Training Accuracy: 90.76%\n",
      "Epoch [18/60], Step [500/2007], Loss: 0.336, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [600/2007], Loss: 0.314, Training Accuracy: 88.59%\n",
      "Epoch [18/60], Step [700/2007], Loss: 0.201, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [800/2007], Loss: 0.298, Training Accuracy: 87.50%\n",
      "Epoch [18/60], Step [900/2007], Loss: 0.212, Training Accuracy: 93.48%\n",
      "Epoch [18/60], Step [1000/2007], Loss: 0.224, Training Accuracy: 90.76%\n",
      "Epoch [18/60], Step [1100/2007], Loss: 0.263, Training Accuracy: 90.76%\n",
      "Epoch [18/60], Step [1200/2007], Loss: 0.261, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [1300/2007], Loss: 0.224, Training Accuracy: 90.76%\n",
      "Epoch [18/60], Step [1400/2007], Loss: 0.264, Training Accuracy: 91.30%\n",
      "Epoch [18/60], Step [1500/2007], Loss: 0.275, Training Accuracy: 88.59%\n",
      "Epoch [18/60], Step [1600/2007], Loss: 0.240, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [1700/2007], Loss: 0.192, Training Accuracy: 93.48%\n",
      "Epoch [18/60], Step [1800/2007], Loss: 0.295, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [1900/2007], Loss: 0.264, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [2000/2007], Loss: 0.228, Training Accuracy: 91.85%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 55.112405589222426 %\n",
      "Finished early after 18 epochs!\n",
      "Final Training Accuracy 91.05056890355047 %\n",
      "Final Test Accuracy 55.083175438321774 %\n",
      "Epoch [1/60], Step [100/1964], Loss: 0.950, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [200/1964], Loss: 0.907, Training Accuracy: 65.22%\n",
      "Epoch [1/60], Step [300/1964], Loss: 0.770, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [400/1964], Loss: 0.763, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [500/1964], Loss: 0.685, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [600/1964], Loss: 0.792, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [700/1964], Loss: 0.586, Training Accuracy: 78.80%\n",
      "Epoch [1/60], Step [800/1964], Loss: 0.449, Training Accuracy: 85.33%\n",
      "Epoch [1/60], Step [900/1964], Loss: 0.446, Training Accuracy: 83.15%\n",
      "Epoch [1/60], Step [1000/1964], Loss: 0.562, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [1100/1964], Loss: 0.532, Training Accuracy: 78.80%\n",
      "Epoch [1/60], Step [1200/1964], Loss: 0.607, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [1300/1964], Loss: 0.585, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [1400/1964], Loss: 0.539, Training Accuracy: 82.61%\n",
      "Epoch [1/60], Step [1500/1964], Loss: 0.537, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [1600/1964], Loss: 0.573, Training Accuracy: 78.80%\n",
      "Epoch [1/60], Step [1700/1964], Loss: 0.569, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [1800/1964], Loss: 0.554, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1900/1964], Loss: 0.438, Training Accuracy: 83.70%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 58.83370088211708 %\n",
      "Epoch [2/60], Step [100/1964], Loss: 0.486, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [200/1964], Loss: 0.441, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [300/1964], Loss: 0.445, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [400/1964], Loss: 0.532, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [500/1964], Loss: 0.485, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [600/1964], Loss: 0.342, Training Accuracy: 89.13%\n",
      "Epoch [2/60], Step [700/1964], Loss: 0.524, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [800/1964], Loss: 0.436, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [900/1964], Loss: 0.403, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1000/1964], Loss: 0.388, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [1100/1964], Loss: 0.370, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [1200/1964], Loss: 0.486, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [1300/1964], Loss: 0.337, Training Accuracy: 88.04%\n",
      "Epoch [2/60], Step [1400/1964], Loss: 0.446, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1500/1964], Loss: 0.398, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [1600/1964], Loss: 0.411, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1700/1964], Loss: 0.440, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1800/1964], Loss: 0.413, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [1900/1964], Loss: 0.403, Training Accuracy: 83.70%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 59.82257417802727 %\n",
      "Epoch [3/60], Step [100/1964], Loss: 0.416, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [200/1964], Loss: 0.381, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [300/1964], Loss: 0.304, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [400/1964], Loss: 0.360, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [500/1964], Loss: 0.338, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [600/1964], Loss: 0.374, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [700/1964], Loss: 0.317, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [800/1964], Loss: 0.296, Training Accuracy: 90.22%\n",
      "Epoch [3/60], Step [900/1964], Loss: 0.388, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [1000/1964], Loss: 0.333, Training Accuracy: 90.76%\n",
      "Epoch [3/60], Step [1100/1964], Loss: 0.351, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1200/1964], Loss: 0.431, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1300/1964], Loss: 0.352, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [1400/1964], Loss: 0.362, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [1500/1964], Loss: 0.328, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [1600/1964], Loss: 0.314, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1700/1964], Loss: 0.317, Training Accuracy: 89.13%\n",
      "Epoch [3/60], Step [1800/1964], Loss: 0.315, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1900/1964], Loss: 0.363, Training Accuracy: 86.96%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 59.310344827586206 %\n",
      "Epoch [4/60], Step [100/1964], Loss: 0.303, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [200/1964], Loss: 0.370, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [300/1964], Loss: 0.389, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [400/1964], Loss: 0.306, Training Accuracy: 89.67%\n",
      "Epoch [4/60], Step [500/1964], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [600/1964], Loss: 0.355, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [700/1964], Loss: 0.284, Training Accuracy: 92.39%\n",
      "Epoch [4/60], Step [800/1964], Loss: 0.409, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [900/1964], Loss: 0.264, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1000/1964], Loss: 0.403, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1100/1964], Loss: 0.236, Training Accuracy: 93.48%\n",
      "Epoch [4/60], Step [1200/1964], Loss: 0.327, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [1300/1964], Loss: 0.342, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [1400/1964], Loss: 0.283, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1500/1964], Loss: 0.285, Training Accuracy: 90.76%\n",
      "Epoch [4/60], Step [1600/1964], Loss: 0.341, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1700/1964], Loss: 0.251, Training Accuracy: 92.39%\n",
      "Epoch [4/60], Step [1800/1964], Loss: 0.368, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1900/1964], Loss: 0.312, Training Accuracy: 88.04%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 60.481154771451486 %\n",
      "Epoch [5/60], Step [100/1964], Loss: 0.256, Training Accuracy: 93.48%\n",
      "Epoch [5/60], Step [200/1964], Loss: 0.242, Training Accuracy: 94.02%\n",
      "Epoch [5/60], Step [300/1964], Loss: 0.346, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [400/1964], Loss: 0.304, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [500/1964], Loss: 0.334, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [600/1964], Loss: 0.289, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [700/1964], Loss: 0.281, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [800/1964], Loss: 0.261, Training Accuracy: 91.30%\n",
      "Epoch [5/60], Step [900/1964], Loss: 0.260, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [1000/1964], Loss: 0.283, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [1100/1964], Loss: 0.293, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1200/1964], Loss: 0.294, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1300/1964], Loss: 0.281, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [1400/1964], Loss: 0.376, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1500/1964], Loss: 0.250, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [1600/1964], Loss: 0.259, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [1700/1964], Loss: 0.325, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1800/1964], Loss: 0.291, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [1900/1964], Loss: 0.294, Training Accuracy: 88.04%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 58.25180433039294 %\n",
      "Epoch [6/60], Step [100/1964], Loss: 0.234, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [200/1964], Loss: 0.301, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [300/1964], Loss: 0.283, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [400/1964], Loss: 0.219, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [500/1964], Loss: 0.264, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [600/1964], Loss: 0.269, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [700/1964], Loss: 0.237, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [800/1964], Loss: 0.314, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [900/1964], Loss: 0.241, Training Accuracy: 92.39%\n",
      "Epoch [6/60], Step [1000/1964], Loss: 0.371, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1100/1964], Loss: 0.280, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [1200/1964], Loss: 0.257, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [1300/1964], Loss: 0.303, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [1400/1964], Loss: 0.199, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [1500/1964], Loss: 0.257, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [1600/1964], Loss: 0.203, Training Accuracy: 93.48%\n",
      "Epoch [6/60], Step [1700/1964], Loss: 0.303, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1800/1964], Loss: 0.284, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [1900/1964], Loss: 0.260, Training Accuracy: 91.85%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 59.8967522052927 %\n",
      "Epoch [7/60], Step [100/1964], Loss: 0.128, Training Accuracy: 96.20%\n",
      "Epoch [7/60], Step [200/1964], Loss: 0.316, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [300/1964], Loss: 0.273, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [400/1964], Loss: 0.272, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [500/1964], Loss: 0.240, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [600/1964], Loss: 0.293, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [700/1964], Loss: 0.333, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [800/1964], Loss: 0.275, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [900/1964], Loss: 0.267, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1000/1964], Loss: 0.236, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [1100/1964], Loss: 0.237, Training Accuracy: 93.48%\n",
      "Epoch [7/60], Step [1200/1964], Loss: 0.208, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [1300/1964], Loss: 0.268, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1400/1964], Loss: 0.258, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1500/1964], Loss: 0.261, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [1600/1964], Loss: 0.277, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1700/1964], Loss: 0.175, Training Accuracy: 95.11%\n",
      "Epoch [7/60], Step [1800/1964], Loss: 0.196, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [1900/1964], Loss: 0.188, Training Accuracy: 94.02%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 59.43163592622294 %\n",
      "Epoch [8/60], Step [100/1964], Loss: 0.214, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [200/1964], Loss: 0.178, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [300/1964], Loss: 0.233, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [400/1964], Loss: 0.227, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [500/1964], Loss: 0.329, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [600/1964], Loss: 0.262, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [700/1964], Loss: 0.226, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [800/1964], Loss: 0.305, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [900/1964], Loss: 0.237, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [1000/1964], Loss: 0.225, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [1100/1964], Loss: 0.195, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [1200/1964], Loss: 0.290, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [1300/1964], Loss: 0.228, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [1400/1964], Loss: 0.203, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [1500/1964], Loss: 0.242, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [1600/1964], Loss: 0.197, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1700/1964], Loss: 0.218, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [1800/1964], Loss: 0.190, Training Accuracy: 94.57%\n",
      "Epoch [8/60], Step [1900/1964], Loss: 0.209, Training Accuracy: 91.30%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 61.97724538893345 %\n",
      "Epoch [9/60], Step [100/1964], Loss: 0.284, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [200/1964], Loss: 0.303, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [300/1964], Loss: 0.316, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [400/1964], Loss: 0.295, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [500/1964], Loss: 0.246, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [600/1964], Loss: 0.330, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [700/1964], Loss: 0.269, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [800/1964], Loss: 0.283, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [900/1964], Loss: 0.262, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1000/1964], Loss: 0.300, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1100/1964], Loss: 0.170, Training Accuracy: 94.02%\n",
      "Epoch [9/60], Step [1200/1964], Loss: 0.335, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1300/1964], Loss: 0.263, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1400/1964], Loss: 0.169, Training Accuracy: 94.57%\n",
      "Epoch [9/60], Step [1500/1964], Loss: 0.205, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [1600/1964], Loss: 0.238, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [1700/1964], Loss: 0.212, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1800/1964], Loss: 0.226, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [1900/1964], Loss: 0.196, Training Accuracy: 92.93%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 59.87820769847634 %\n",
      "Epoch [10/60], Step [100/1964], Loss: 0.181, Training Accuracy: 94.57%\n",
      "Epoch [10/60], Step [200/1964], Loss: 0.232, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [300/1964], Loss: 0.224, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [400/1964], Loss: 0.271, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [500/1964], Loss: 0.243, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [600/1964], Loss: 0.222, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [700/1964], Loss: 0.171, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [800/1964], Loss: 0.395, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [900/1964], Loss: 0.284, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1000/1964], Loss: 0.302, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1100/1964], Loss: 0.258, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1200/1964], Loss: 0.231, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [1300/1964], Loss: 0.197, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [1400/1964], Loss: 0.222, Training Accuracy: 91.85%\n",
      "Epoch [10/60], Step [1500/1964], Loss: 0.172, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [1600/1964], Loss: 0.270, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1700/1964], Loss: 0.191, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [1800/1964], Loss: 0.258, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1900/1964], Loss: 0.227, Training Accuracy: 91.30%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 58.48636728147554 %\n",
      "Epoch [11/60], Step [100/1964], Loss: 0.155, Training Accuracy: 92.93%\n",
      "Epoch [11/60], Step [200/1964], Loss: 0.201, Training Accuracy: 92.93%\n",
      "Epoch [11/60], Step [300/1964], Loss: 0.177, Training Accuracy: 92.93%\n",
      "Epoch [11/60], Step [400/1964], Loss: 0.144, Training Accuracy: 94.57%\n",
      "Epoch [11/60], Step [500/1964], Loss: 0.197, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [600/1964], Loss: 0.215, Training Accuracy: 94.02%\n",
      "Epoch [11/60], Step [700/1964], Loss: 0.169, Training Accuracy: 94.57%\n",
      "Epoch [11/60], Step [800/1964], Loss: 0.164, Training Accuracy: 95.11%\n",
      "Epoch [11/60], Step [900/1964], Loss: 0.136, Training Accuracy: 95.65%\n",
      "Epoch [11/60], Step [1000/1964], Loss: 0.230, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [1100/1964], Loss: 0.207, Training Accuracy: 94.02%\n",
      "Epoch [11/60], Step [1200/1964], Loss: 0.233, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [1300/1964], Loss: 0.173, Training Accuracy: 93.48%\n",
      "Epoch [11/60], Step [1400/1964], Loss: 0.202, Training Accuracy: 93.48%\n",
      "Epoch [11/60], Step [1500/1964], Loss: 0.200, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [1600/1964], Loss: 0.181, Training Accuracy: 94.02%\n",
      "Epoch [11/60], Step [1700/1964], Loss: 0.229, Training Accuracy: 92.93%\n",
      "Epoch [11/60], Step [1800/1964], Loss: 0.269, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [1900/1964], Loss: 0.185, Training Accuracy: 94.57%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 58.81615878107458 %\n",
      "Epoch [12/60], Step [100/1964], Loss: 0.195, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [200/1964], Loss: 0.179, Training Accuracy: 94.02%\n",
      "Epoch [12/60], Step [300/1964], Loss: 0.287, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [400/1964], Loss: 0.205, Training Accuracy: 93.48%\n",
      "Epoch [12/60], Step [500/1964], Loss: 0.185, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [600/1964], Loss: 0.206, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [700/1964], Loss: 0.216, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [800/1964], Loss: 0.242, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [900/1964], Loss: 0.254, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1000/1964], Loss: 0.191, Training Accuracy: 93.48%\n",
      "Epoch [12/60], Step [1100/1964], Loss: 0.181, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [1200/1964], Loss: 0.244, Training Accuracy: 87.50%\n",
      "Epoch [12/60], Step [1300/1964], Loss: 0.161, Training Accuracy: 95.11%\n",
      "Epoch [12/60], Step [1400/1964], Loss: 0.160, Training Accuracy: 95.65%\n",
      "Epoch [12/60], Step [1500/1964], Loss: 0.282, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1600/1964], Loss: 0.242, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [1700/1964], Loss: 0.251, Training Accuracy: 93.48%\n",
      "Epoch [12/60], Step [1800/1964], Loss: 0.200, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [1900/1964], Loss: 0.217, Training Accuracy: 91.30%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 60.270148356054534 %\n",
      "Epoch [13/60], Step [100/1964], Loss: 0.159, Training Accuracy: 95.65%\n",
      "Epoch [13/60], Step [200/1964], Loss: 0.185, Training Accuracy: 92.93%\n",
      "Epoch [13/60], Step [300/1964], Loss: 0.175, Training Accuracy: 94.02%\n",
      "Epoch [13/60], Step [400/1964], Loss: 0.191, Training Accuracy: 92.93%\n",
      "Epoch [13/60], Step [500/1964], Loss: 0.165, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [600/1964], Loss: 0.210, Training Accuracy: 94.02%\n",
      "Epoch [13/60], Step [700/1964], Loss: 0.279, Training Accuracy: 93.48%\n",
      "Epoch [13/60], Step [800/1964], Loss: 0.222, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [900/1964], Loss: 0.195, Training Accuracy: 94.02%\n",
      "Epoch [13/60], Step [1000/1964], Loss: 0.240, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [1100/1964], Loss: 0.178, Training Accuracy: 95.11%\n",
      "Epoch [13/60], Step [1200/1964], Loss: 0.250, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [1300/1964], Loss: 0.255, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [1400/1964], Loss: 0.160, Training Accuracy: 94.02%\n",
      "Epoch [13/60], Step [1500/1964], Loss: 0.285, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [1600/1964], Loss: 0.166, Training Accuracy: 94.57%\n",
      "Epoch [13/60], Step [1700/1964], Loss: 0.162, Training Accuracy: 95.65%\n",
      "Epoch [13/60], Step [1800/1964], Loss: 0.195, Training Accuracy: 92.39%\n",
      "Epoch [13/60], Step [1900/1964], Loss: 0.151, Training Accuracy: 95.11%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 58.780072173215714 %\n",
      "Finished early after 13 epochs!\n",
      "Final Training Accuracy 92.97778466682536 %\n",
      "Final Test Accuracy 58.76152766639936 %\n",
      "Epoch [1/60], Step [100/2126], Loss: 0.994, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [200/2126], Loss: 0.906, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [300/2126], Loss: 0.788, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [400/2126], Loss: 0.782, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [500/2126], Loss: 0.790, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [600/2126], Loss: 0.723, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [700/2126], Loss: 0.799, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [800/2126], Loss: 0.754, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [900/2126], Loss: 0.800, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [1000/2126], Loss: 0.598, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1100/2126], Loss: 0.738, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [1200/2126], Loss: 0.712, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [1300/2126], Loss: 0.713, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [1400/2126], Loss: 0.826, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [1500/2126], Loss: 0.528, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1600/2126], Loss: 0.723, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [1700/2126], Loss: 0.616, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1800/2126], Loss: 0.634, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1900/2126], Loss: 0.655, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [2000/2126], Loss: 0.566, Training Accuracy: 80.98%\n",
      "Epoch [1/60], Step [2100/2126], Loss: 0.668, Training Accuracy: 76.09%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 64.96384775396437 %\n",
      "Epoch [2/60], Step [100/2126], Loss: 0.614, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [200/2126], Loss: 0.582, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [300/2126], Loss: 0.676, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [400/2126], Loss: 0.591, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [500/2126], Loss: 0.522, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [600/2126], Loss: 0.554, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [700/2126], Loss: 0.607, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [800/2126], Loss: 0.572, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [900/2126], Loss: 0.690, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [1000/2126], Loss: 0.495, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [1100/2126], Loss: 0.442, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [1200/2126], Loss: 0.565, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [1300/2126], Loss: 0.607, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1400/2126], Loss: 0.596, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [1500/2126], Loss: 0.614, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [1600/2126], Loss: 0.614, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [1700/2126], Loss: 0.551, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1800/2126], Loss: 0.630, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [1900/2126], Loss: 0.596, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [2000/2126], Loss: 0.524, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [2100/2126], Loss: 0.535, Training Accuracy: 82.07%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 66.64741689697641 %\n",
      "Epoch [3/60], Step [100/2126], Loss: 0.498, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [200/2126], Loss: 0.455, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [300/2126], Loss: 0.563, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [400/2126], Loss: 0.469, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [500/2126], Loss: 0.469, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [600/2126], Loss: 0.513, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [700/2126], Loss: 0.485, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [800/2126], Loss: 0.503, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [900/2126], Loss: 0.525, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [1000/2126], Loss: 0.490, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1100/2126], Loss: 0.515, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [1200/2126], Loss: 0.466, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1300/2126], Loss: 0.565, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [1400/2126], Loss: 0.479, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1500/2126], Loss: 0.496, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1600/2126], Loss: 0.566, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [1700/2126], Loss: 0.647, Training Accuracy: 71.74%\n",
      "Epoch [3/60], Step [1800/2126], Loss: 0.513, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [1900/2126], Loss: 0.540, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [2000/2126], Loss: 0.498, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [2100/2126], Loss: 0.444, Training Accuracy: 84.24%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 66.59320325988956 %\n",
      "Epoch [4/60], Step [100/2126], Loss: 0.539, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [200/2126], Loss: 0.533, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [300/2126], Loss: 0.539, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [400/2126], Loss: 0.436, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [500/2126], Loss: 0.499, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [600/2126], Loss: 0.541, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [700/2126], Loss: 0.474, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [800/2126], Loss: 0.403, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [900/2126], Loss: 0.552, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [1000/2126], Loss: 0.461, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [1100/2126], Loss: 0.371, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [1200/2126], Loss: 0.455, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1300/2126], Loss: 0.456, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [1400/2126], Loss: 0.487, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [1500/2126], Loss: 0.448, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [1600/2126], Loss: 0.427, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1700/2126], Loss: 0.463, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [1800/2126], Loss: 0.541, Training Accuracy: 77.17%\n",
      "Epoch [4/60], Step [1900/2126], Loss: 0.454, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [2000/2126], Loss: 0.411, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [2100/2126], Loss: 0.466, Training Accuracy: 84.24%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 68.5908579308069 %\n",
      "Epoch [5/60], Step [100/2126], Loss: 0.410, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [200/2126], Loss: 0.500, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [300/2126], Loss: 0.561, Training Accuracy: 78.26%\n",
      "Epoch [5/60], Step [400/2126], Loss: 0.440, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [500/2126], Loss: 0.485, Training Accuracy: 78.80%\n",
      "Epoch [5/60], Step [600/2126], Loss: 0.449, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [700/2126], Loss: 0.394, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [800/2126], Loss: 0.425, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [900/2126], Loss: 0.469, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [1000/2126], Loss: 0.522, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [1100/2126], Loss: 0.465, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [1200/2126], Loss: 0.447, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1300/2126], Loss: 0.520, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [1400/2126], Loss: 0.526, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [1500/2126], Loss: 0.592, Training Accuracy: 77.17%\n",
      "Epoch [5/60], Step [1600/2126], Loss: 0.484, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1700/2126], Loss: 0.461, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1800/2126], Loss: 0.418, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1900/2126], Loss: 0.553, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [2000/2126], Loss: 0.428, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [2100/2126], Loss: 0.356, Training Accuracy: 86.96%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 68.48537704995316 %\n",
      "Epoch [6/60], Step [100/2126], Loss: 0.454, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [200/2126], Loss: 0.426, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [300/2126], Loss: 0.377, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [400/2126], Loss: 0.428, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [500/2126], Loss: 0.417, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/2126], Loss: 0.289, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [700/2126], Loss: 0.357, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [800/2126], Loss: 0.382, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [900/2126], Loss: 0.413, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1000/2126], Loss: 0.396, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1100/2126], Loss: 0.448, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [1200/2126], Loss: 0.334, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1300/2126], Loss: 0.490, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [1400/2126], Loss: 0.444, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [1500/2126], Loss: 0.350, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1600/2126], Loss: 0.451, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1700/2126], Loss: 0.456, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [1800/2126], Loss: 0.414, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [1900/2126], Loss: 0.420, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [2000/2126], Loss: 0.465, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [2100/2126], Loss: 0.369, Training Accuracy: 83.70%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 68.5201444911284 %\n",
      "Epoch [7/60], Step [100/2126], Loss: 0.423, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [200/2126], Loss: 0.399, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [300/2126], Loss: 0.496, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [400/2126], Loss: 0.357, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [500/2126], Loss: 0.390, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [600/2126], Loss: 0.453, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [700/2126], Loss: 0.431, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [800/2126], Loss: 0.420, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [900/2126], Loss: 0.392, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [1000/2126], Loss: 0.317, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1100/2126], Loss: 0.380, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [1200/2126], Loss: 0.365, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [1300/2126], Loss: 0.402, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [1400/2126], Loss: 0.441, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [1500/2126], Loss: 0.501, Training Accuracy: 77.72%\n",
      "Epoch [7/60], Step [1600/2126], Loss: 0.426, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [1700/2126], Loss: 0.351, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [1800/2126], Loss: 0.393, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [1900/2126], Loss: 0.468, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [2000/2126], Loss: 0.293, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [2100/2126], Loss: 0.388, Training Accuracy: 85.33%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 66.64564906098445 %\n",
      "Epoch [8/60], Step [100/2126], Loss: 0.334, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [200/2126], Loss: 0.400, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [300/2126], Loss: 0.352, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [400/2126], Loss: 0.347, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [500/2126], Loss: 0.338, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [600/2126], Loss: 0.398, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [700/2126], Loss: 0.265, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [800/2126], Loss: 0.415, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [900/2126], Loss: 0.439, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [1000/2126], Loss: 0.482, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [1100/2126], Loss: 0.456, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [1200/2126], Loss: 0.432, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [1300/2126], Loss: 0.380, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [1400/2126], Loss: 0.364, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1500/2126], Loss: 0.461, Training Accuracy: 80.43%\n",
      "Epoch [8/60], Step [1600/2126], Loss: 0.368, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1700/2126], Loss: 0.271, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [1800/2126], Loss: 0.463, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [1900/2126], Loss: 0.288, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [2000/2126], Loss: 0.385, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [2100/2126], Loss: 0.319, Training Accuracy: 88.59%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 67.2143029717323 %\n",
      "Epoch [9/60], Step [100/2126], Loss: 0.333, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [200/2126], Loss: 0.355, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [300/2126], Loss: 0.358, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [400/2126], Loss: 0.531, Training Accuracy: 79.89%\n",
      "Epoch [9/60], Step [500/2126], Loss: 0.348, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [600/2126], Loss: 0.346, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [700/2126], Loss: 0.313, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [800/2126], Loss: 0.332, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [900/2126], Loss: 0.395, Training Accuracy: 82.61%\n",
      "Epoch [9/60], Step [1000/2126], Loss: 0.375, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1100/2126], Loss: 0.380, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [1200/2126], Loss: 0.417, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [1300/2126], Loss: 0.440, Training Accuracy: 80.98%\n",
      "Epoch [9/60], Step [1400/2126], Loss: 0.284, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1500/2126], Loss: 0.392, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [1600/2126], Loss: 0.447, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [1700/2126], Loss: 0.349, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [1800/2126], Loss: 0.330, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1900/2126], Loss: 0.403, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [2000/2126], Loss: 0.355, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [2100/2126], Loss: 0.345, Training Accuracy: 86.96%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 67.83009917559914 %\n",
      "Finished early after 9 epochs!\n",
      "Final Training Accuracy 86.72063369496014 %\n",
      "Final Test Accuracy 67.76822491588047 %\n",
      "Epoch [1/60], Step [100/2180], Loss: 1.001, Training Accuracy: 59.24%\n",
      "Epoch [1/60], Step [200/2180], Loss: 0.746, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [300/2180], Loss: 0.816, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [400/2180], Loss: 0.665, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [500/2180], Loss: 0.811, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [600/2180], Loss: 0.655, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [700/2180], Loss: 0.654, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [800/2180], Loss: 0.666, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [900/2180], Loss: 0.623, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [1000/2180], Loss: 0.546, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1100/2180], Loss: 0.628, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1200/2180], Loss: 0.511, Training Accuracy: 82.07%\n",
      "Epoch [1/60], Step [1300/2180], Loss: 0.594, Training Accuracy: 81.52%\n",
      "Epoch [1/60], Step [1400/2180], Loss: 0.603, Training Accuracy: 82.61%\n",
      "Epoch [1/60], Step [1500/2180], Loss: 0.519, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [1600/2180], Loss: 0.565, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [1700/2180], Loss: 0.447, Training Accuracy: 83.70%\n",
      "Epoch [1/60], Step [1800/2180], Loss: 0.456, Training Accuracy: 83.15%\n",
      "Epoch [1/60], Step [1900/2180], Loss: 0.510, Training Accuracy: 83.70%\n",
      "Epoch [1/60], Step [2000/2180], Loss: 0.549, Training Accuracy: 78.80%\n",
      "Epoch [1/60], Step [2100/2180], Loss: 0.506, Training Accuracy: 79.89%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 57.20999799739661 %\n",
      "Epoch [2/60], Step [100/2180], Loss: 0.521, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [200/2180], Loss: 0.637, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [300/2180], Loss: 0.467, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [400/2180], Loss: 0.463, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [500/2180], Loss: 0.391, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [600/2180], Loss: 0.409, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [700/2180], Loss: 0.485, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [800/2180], Loss: 0.415, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [900/2180], Loss: 0.478, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1000/2180], Loss: 0.429, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [1100/2180], Loss: 0.479, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [1200/2180], Loss: 0.438, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1300/2180], Loss: 0.361, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [1400/2180], Loss: 0.388, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [1500/2180], Loss: 0.458, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [1600/2180], Loss: 0.516, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [1700/2180], Loss: 0.441, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [1800/2180], Loss: 0.459, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [1900/2180], Loss: 0.386, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [2000/2180], Loss: 0.461, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [2100/2180], Loss: 0.458, Training Accuracy: 82.07%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 58.45974767197356 %\n",
      "Epoch [3/60], Step [100/2180], Loss: 0.506, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [200/2180], Loss: 0.413, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [300/2180], Loss: 0.409, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [400/2180], Loss: 0.429, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [500/2180], Loss: 0.552, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [600/2180], Loss: 0.435, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [700/2180], Loss: 0.322, Training Accuracy: 89.13%\n",
      "Epoch [3/60], Step [800/2180], Loss: 0.450, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [900/2180], Loss: 0.401, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1000/2180], Loss: 0.426, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [1100/2180], Loss: 0.326, Training Accuracy: 89.13%\n",
      "Epoch [3/60], Step [1200/2180], Loss: 0.389, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [1300/2180], Loss: 0.407, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [1400/2180], Loss: 0.463, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1500/2180], Loss: 0.392, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1600/2180], Loss: 0.329, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1700/2180], Loss: 0.391, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [1800/2180], Loss: 0.407, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [1900/2180], Loss: 0.475, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [2000/2180], Loss: 0.381, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [2100/2180], Loss: 0.414, Training Accuracy: 84.78%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 58.66876940022029 %\n",
      "Epoch [4/60], Step [100/2180], Loss: 0.358, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [200/2180], Loss: 0.456, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [300/2180], Loss: 0.352, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [400/2180], Loss: 0.406, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [500/2180], Loss: 0.429, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [600/2180], Loss: 0.412, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [700/2180], Loss: 0.424, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [800/2180], Loss: 0.321, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [900/2180], Loss: 0.333, Training Accuracy: 89.67%\n",
      "Epoch [4/60], Step [1000/2180], Loss: 0.311, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1100/2180], Loss: 0.416, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1200/2180], Loss: 0.436, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1300/2180], Loss: 0.339, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1400/2180], Loss: 0.416, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1500/2180], Loss: 0.408, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [1600/2180], Loss: 0.457, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1700/2180], Loss: 0.391, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [1800/2180], Loss: 0.471, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1900/2180], Loss: 0.445, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [2000/2180], Loss: 0.391, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [2100/2180], Loss: 0.338, Training Accuracy: 90.76%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 60.514919395213774 %\n",
      "Epoch [5/60], Step [100/2180], Loss: 0.447, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [200/2180], Loss: 0.322, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [300/2180], Loss: 0.353, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [400/2180], Loss: 0.428, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [500/2180], Loss: 0.449, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [600/2180], Loss: 0.373, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [700/2180], Loss: 0.374, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [800/2180], Loss: 0.297, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [900/2180], Loss: 0.326, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1000/2180], Loss: 0.401, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [1100/2180], Loss: 0.302, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [1200/2180], Loss: 0.421, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1300/2180], Loss: 0.388, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [1400/2180], Loss: 0.370, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [1500/2180], Loss: 0.350, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1600/2180], Loss: 0.390, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1700/2180], Loss: 0.378, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [1800/2180], Loss: 0.335, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1900/2180], Loss: 0.407, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [2000/2180], Loss: 0.349, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [2100/2180], Loss: 0.342, Training Accuracy: 87.50%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 58.72634424752178 %\n",
      "Epoch [6/60], Step [100/2180], Loss: 0.397, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [200/2180], Loss: 0.340, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [300/2180], Loss: 0.326, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [400/2180], Loss: 0.240, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [500/2180], Loss: 0.382, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [600/2180], Loss: 0.299, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [700/2180], Loss: 0.372, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [800/2180], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [900/2180], Loss: 0.324, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1000/2180], Loss: 0.416, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1100/2180], Loss: 0.301, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [1200/2180], Loss: 0.344, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1300/2180], Loss: 0.283, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1400/2180], Loss: 0.255, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [1500/2180], Loss: 0.334, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [1600/2180], Loss: 0.303, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [1700/2180], Loss: 0.349, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1800/2180], Loss: 0.341, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1900/2180], Loss: 0.346, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [2000/2180], Loss: 0.265, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [2100/2180], Loss: 0.353, Training Accuracy: 86.96%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 60.884775207770105 %\n",
      "Epoch [7/60], Step [100/2180], Loss: 0.249, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [200/2180], Loss: 0.366, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [300/2180], Loss: 0.383, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [400/2180], Loss: 0.323, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [500/2180], Loss: 0.309, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [600/2180], Loss: 0.236, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [700/2180], Loss: 0.247, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [800/2180], Loss: 0.343, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [900/2180], Loss: 0.326, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1000/2180], Loss: 0.282, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1100/2180], Loss: 0.242, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1200/2180], Loss: 0.279, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1300/2180], Loss: 0.339, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [1400/2180], Loss: 0.278, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [1500/2180], Loss: 0.301, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1600/2180], Loss: 0.192, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [1700/2180], Loss: 0.254, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1800/2180], Loss: 0.399, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1900/2180], Loss: 0.261, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [2000/2180], Loss: 0.202, Training Accuracy: 94.57%\n",
      "Epoch [7/60], Step [2100/2180], Loss: 0.262, Training Accuracy: 90.22%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 59.402848703314305 %\n",
      "Epoch [8/60], Step [100/2180], Loss: 0.248, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [200/2180], Loss: 0.280, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [300/2180], Loss: 0.413, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [400/2180], Loss: 0.285, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [500/2180], Loss: 0.390, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [600/2180], Loss: 0.251, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [700/2180], Loss: 0.292, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [800/2180], Loss: 0.256, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [900/2180], Loss: 0.293, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1000/2180], Loss: 0.278, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [1100/2180], Loss: 0.238, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1200/2180], Loss: 0.238, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1300/2180], Loss: 0.264, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1400/2180], Loss: 0.259, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1500/2180], Loss: 0.197, Training Accuracy: 94.57%\n",
      "Epoch [8/60], Step [1600/2180], Loss: 0.296, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1700/2180], Loss: 0.430, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [1800/2180], Loss: 0.226, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1900/2180], Loss: 0.305, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [2000/2180], Loss: 0.380, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [2100/2180], Loss: 0.343, Training Accuracy: 84.78%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 59.383448483027934 %\n",
      "Epoch [9/60], Step [100/2180], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [200/2180], Loss: 0.309, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [300/2180], Loss: 0.232, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [400/2180], Loss: 0.315, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [500/2180], Loss: 0.248, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [600/2180], Loss: 0.278, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [700/2180], Loss: 0.301, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [800/2180], Loss: 0.223, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [900/2180], Loss: 0.296, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1000/2180], Loss: 0.301, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1100/2180], Loss: 0.286, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1200/2180], Loss: 0.275, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1300/2180], Loss: 0.238, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [1400/2180], Loss: 0.230, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1500/2180], Loss: 0.243, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [1600/2180], Loss: 0.344, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [1700/2180], Loss: 0.283, Training Accuracy: 93.48%\n",
      "Epoch [9/60], Step [1800/2180], Loss: 0.239, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1900/2180], Loss: 0.321, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [2000/2180], Loss: 0.289, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [2100/2180], Loss: 0.228, Training Accuracy: 92.93%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 60.437944327625914 %\n",
      "Epoch [10/60], Step [100/2180], Loss: 0.268, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [200/2180], Loss: 0.280, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [300/2180], Loss: 0.268, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [400/2180], Loss: 0.355, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [500/2180], Loss: 0.255, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [600/2180], Loss: 0.167, Training Accuracy: 95.11%\n",
      "Epoch [10/60], Step [700/2180], Loss: 0.262, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [800/2180], Loss: 0.406, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [900/2180], Loss: 0.253, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1000/2180], Loss: 0.229, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [1100/2180], Loss: 0.207, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [1200/2180], Loss: 0.217, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [1300/2180], Loss: 0.264, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1400/2180], Loss: 0.286, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [1500/2180], Loss: 0.323, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1600/2180], Loss: 0.373, Training Accuracy: 82.61%\n",
      "Epoch [10/60], Step [1700/2180], Loss: 0.259, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1800/2180], Loss: 0.260, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1900/2180], Loss: 0.263, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [2000/2180], Loss: 0.288, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [2100/2180], Loss: 0.292, Training Accuracy: 91.30%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 60.7389606488435 %\n",
      "Epoch [11/60], Step [100/2180], Loss: 0.260, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [200/2180], Loss: 0.186, Training Accuracy: 95.65%\n",
      "Epoch [11/60], Step [300/2180], Loss: 0.201, Training Accuracy: 93.48%\n",
      "Epoch [11/60], Step [400/2180], Loss: 0.366, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [500/2180], Loss: 0.251, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [600/2180], Loss: 0.299, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [700/2180], Loss: 0.292, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [800/2180], Loss: 0.298, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [900/2180], Loss: 0.288, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [1000/2180], Loss: 0.304, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1100/2180], Loss: 0.274, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [1200/2180], Loss: 0.221, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [1300/2180], Loss: 0.287, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [1400/2180], Loss: 0.293, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1500/2180], Loss: 0.217, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [1600/2180], Loss: 0.233, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [1700/2180], Loss: 0.273, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [1800/2180], Loss: 0.322, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [1900/2180], Loss: 0.277, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [2000/2180], Loss: 0.313, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [2100/2180], Loss: 0.307, Training Accuracy: 90.22%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 60.03742365074597 %\n",
      "Finished early after 11 epochs!\n",
      "Final Training Accuracy 90.1847834218598 %\n",
      "Final Test Accuracy 59.86344748172624 %\n",
      "Epoch [1/60], Step [100/1885], Loss: 0.877, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [200/1885], Loss: 0.874, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [300/1885], Loss: 0.840, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [400/1885], Loss: 0.732, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [500/1885], Loss: 0.768, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [600/1885], Loss: 0.726, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [700/1885], Loss: 0.616, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [800/1885], Loss: 0.714, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [900/1885], Loss: 0.673, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1000/1885], Loss: 0.666, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [1100/1885], Loss: 0.626, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1200/1885], Loss: 0.597, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1300/1885], Loss: 0.535, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1400/1885], Loss: 0.535, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [1500/1885], Loss: 0.607, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [1600/1885], Loss: 0.529, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [1700/1885], Loss: 0.558, Training Accuracy: 82.07%\n",
      "Epoch [1/60], Step [1800/1885], Loss: 0.619, Training Accuracy: 76.09%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 65.62259126900707 %\n",
      "Epoch [2/60], Step [100/1885], Loss: 0.552, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [200/1885], Loss: 0.554, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [300/1885], Loss: 0.634, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [400/1885], Loss: 0.542, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [500/1885], Loss: 0.558, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [600/1885], Loss: 0.499, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [700/1885], Loss: 0.574, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [800/1885], Loss: 0.474, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [900/1885], Loss: 0.459, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1000/1885], Loss: 0.503, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [1100/1885], Loss: 0.553, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [1200/1885], Loss: 0.454, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1300/1885], Loss: 0.590, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1400/1885], Loss: 0.409, Training Accuracy: 87.50%\n",
      "Epoch [2/60], Step [1500/1885], Loss: 0.429, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1600/1885], Loss: 0.397, Training Accuracy: 86.96%\n",
      "Epoch [2/60], Step [1700/1885], Loss: 0.477, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1800/1885], Loss: 0.473, Training Accuracy: 80.43%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 65.50860719874805 %\n",
      "Epoch [3/60], Step [100/1885], Loss: 0.530, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [200/1885], Loss: 0.533, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [300/1885], Loss: 0.340, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [400/1885], Loss: 0.445, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [500/1885], Loss: 0.452, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [600/1885], Loss: 0.506, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [700/1885], Loss: 0.484, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [800/1885], Loss: 0.526, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [900/1885], Loss: 0.435, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [1000/1885], Loss: 0.351, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [1100/1885], Loss: 0.465, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1200/1885], Loss: 0.491, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [1300/1885], Loss: 0.443, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1400/1885], Loss: 0.382, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1500/1885], Loss: 0.400, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [1600/1885], Loss: 0.509, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [1700/1885], Loss: 0.411, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1800/1885], Loss: 0.407, Training Accuracy: 85.87%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 65.56700067736435 %\n",
      "Epoch [4/60], Step [100/1885], Loss: 0.497, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [200/1885], Loss: 0.491, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [300/1885], Loss: 0.381, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [400/1885], Loss: 0.360, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [500/1885], Loss: 0.381, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [600/1885], Loss: 0.425, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [700/1885], Loss: 0.322, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [800/1885], Loss: 0.462, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [900/1885], Loss: 0.437, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [1000/1885], Loss: 0.438, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1100/1885], Loss: 0.399, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1200/1885], Loss: 0.398, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1300/1885], Loss: 0.430, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1400/1885], Loss: 0.416, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [1500/1885], Loss: 0.453, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1600/1885], Loss: 0.395, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1700/1885], Loss: 0.369, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1800/1885], Loss: 0.378, Training Accuracy: 85.33%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 64.5757129843739 %\n",
      "Epoch [5/60], Step [100/1885], Loss: 0.426, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [200/1885], Loss: 0.428, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [300/1885], Loss: 0.406, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [400/1885], Loss: 0.423, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [500/1885], Loss: 0.469, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [600/1885], Loss: 0.255, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [700/1885], Loss: 0.366, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [800/1885], Loss: 0.455, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [900/1885], Loss: 0.302, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1000/1885], Loss: 0.215, Training Accuracy: 95.11%\n",
      "Epoch [5/60], Step [1100/1885], Loss: 0.378, Training Accuracy: 91.30%\n",
      "Epoch [5/60], Step [1200/1885], Loss: 0.337, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1300/1885], Loss: 0.378, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1400/1885], Loss: 0.348, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1500/1885], Loss: 0.406, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1600/1885], Loss: 0.269, Training Accuracy: 91.30%\n",
      "Epoch [5/60], Step [1700/1885], Loss: 0.441, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [1800/1885], Loss: 0.360, Training Accuracy: 88.04%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 64.35989068740803 %\n",
      "Epoch [6/60], Step [100/1885], Loss: 0.352, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [200/1885], Loss: 0.377, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [300/1885], Loss: 0.377, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [400/1885], Loss: 0.315, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [500/1885], Loss: 0.359, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [600/1885], Loss: 0.380, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [700/1885], Loss: 0.342, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [800/1885], Loss: 0.311, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [900/1885], Loss: 0.320, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [1000/1885], Loss: 0.402, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1100/1885], Loss: 0.343, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1200/1885], Loss: 0.346, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [1300/1885], Loss: 0.432, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1400/1885], Loss: 0.305, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [1500/1885], Loss: 0.312, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1600/1885], Loss: 0.361, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1700/1885], Loss: 0.367, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1800/1885], Loss: 0.374, Training Accuracy: 85.87%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 63.74979562282485 %\n",
      "Finished early after 6 epochs!\n",
      "Final Training Accuracy 87.62260733639617 %\n",
      "Final Test Accuracy 63.802116179665056 %\n",
      "Epoch [1/60], Step [100/2032], Loss: 0.988, Training Accuracy: 65.76%\n",
      "Epoch [1/60], Step [200/2032], Loss: 0.810, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [300/2032], Loss: 0.666, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [400/2032], Loss: 0.817, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [500/2032], Loss: 0.803, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [600/2032], Loss: 0.690, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [700/2032], Loss: 0.604, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [800/2032], Loss: 0.713, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [900/2032], Loss: 0.636, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [1000/2032], Loss: 0.649, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1100/2032], Loss: 0.607, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [1200/2032], Loss: 0.501, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1300/2032], Loss: 0.640, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [1400/2032], Loss: 0.572, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1500/2032], Loss: 0.506, Training Accuracy: 80.98%\n",
      "Epoch [1/60], Step [1600/2032], Loss: 0.668, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [1700/2032], Loss: 0.550, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1800/2032], Loss: 0.637, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [1900/2032], Loss: 0.664, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [2000/2032], Loss: 0.509, Training Accuracy: 83.70%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 64.42108641579074 %\n",
      "Epoch [2/60], Step [100/2032], Loss: 0.537, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [200/2032], Loss: 0.601, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [300/2032], Loss: 0.574, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [400/2032], Loss: 0.475, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [500/2032], Loss: 0.475, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [600/2032], Loss: 0.453, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [700/2032], Loss: 0.452, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [800/2032], Loss: 0.497, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [900/2032], Loss: 0.490, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [1000/2032], Loss: 0.577, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [1100/2032], Loss: 0.480, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [1200/2032], Loss: 0.485, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [1300/2032], Loss: 0.496, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [1400/2032], Loss: 0.570, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [1500/2032], Loss: 0.494, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1600/2032], Loss: 0.458, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [1700/2032], Loss: 0.516, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1800/2032], Loss: 0.535, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1900/2032], Loss: 0.606, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [2000/2032], Loss: 0.574, Training Accuracy: 77.72%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 65.92634197223782 %\n",
      "Epoch [3/60], Step [100/2032], Loss: 0.601, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [200/2032], Loss: 0.503, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [300/2032], Loss: 0.531, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [400/2032], Loss: 0.573, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [500/2032], Loss: 0.409, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [600/2032], Loss: 0.512, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [700/2032], Loss: 0.388, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [800/2032], Loss: 0.505, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [900/2032], Loss: 0.433, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1000/2032], Loss: 0.482, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [1100/2032], Loss: 0.432, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [1200/2032], Loss: 0.530, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [1300/2032], Loss: 0.429, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1400/2032], Loss: 0.462, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1500/2032], Loss: 0.454, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1600/2032], Loss: 0.366, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1700/2032], Loss: 0.494, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1800/2032], Loss: 0.479, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [1900/2032], Loss: 0.415, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [2000/2032], Loss: 0.405, Training Accuracy: 84.78%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 65.15445719329215 %\n",
      "Epoch [4/60], Step [100/2032], Loss: 0.365, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [200/2032], Loss: 0.528, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [300/2032], Loss: 0.436, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [400/2032], Loss: 0.498, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [500/2032], Loss: 0.382, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [600/2032], Loss: 0.446, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [700/2032], Loss: 0.366, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [800/2032], Loss: 0.483, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [900/2032], Loss: 0.432, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [1000/2032], Loss: 0.480, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [1100/2032], Loss: 0.383, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1200/2032], Loss: 0.506, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [1300/2032], Loss: 0.362, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [1400/2032], Loss: 0.509, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [1500/2032], Loss: 0.358, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1600/2032], Loss: 0.405, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [1700/2032], Loss: 0.419, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1800/2032], Loss: 0.423, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1900/2032], Loss: 0.412, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [2000/2032], Loss: 0.400, Training Accuracy: 84.24%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 66.53507716173206 %\n",
      "Epoch [5/60], Step [100/2032], Loss: 0.388, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [200/2032], Loss: 0.394, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [300/2032], Loss: 0.327, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [400/2032], Loss: 0.407, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [500/2032], Loss: 0.444, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [600/2032], Loss: 0.440, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [700/2032], Loss: 0.418, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [800/2032], Loss: 0.524, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [900/2032], Loss: 0.467, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [1000/2032], Loss: 0.380, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1100/2032], Loss: 0.389, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1200/2032], Loss: 0.376, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1300/2032], Loss: 0.416, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1400/2032], Loss: 0.494, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1500/2032], Loss: 0.445, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [1600/2032], Loss: 0.446, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [1700/2032], Loss: 0.434, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1800/2032], Loss: 0.356, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1900/2032], Loss: 0.390, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [2000/2032], Loss: 0.455, Training Accuracy: 85.33%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 66.882773008104 %\n",
      "Epoch [6/60], Step [100/2032], Loss: 0.450, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [200/2032], Loss: 0.362, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [300/2032], Loss: 0.388, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [400/2032], Loss: 0.381, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [500/2032], Loss: 0.384, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [600/2032], Loss: 0.386, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [700/2032], Loss: 0.337, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [800/2032], Loss: 0.389, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [900/2032], Loss: 0.364, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1000/2032], Loss: 0.359, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [1100/2032], Loss: 0.391, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [1200/2032], Loss: 0.365, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1300/2032], Loss: 0.336, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [1400/2032], Loss: 0.327, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1500/2032], Loss: 0.375, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1600/2032], Loss: 0.447, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [1700/2032], Loss: 0.449, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [1800/2032], Loss: 0.492, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [1900/2032], Loss: 0.336, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [2000/2032], Loss: 0.393, Training Accuracy: 83.70%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 67.63647061970099 %\n",
      "Epoch [7/60], Step [100/2032], Loss: 0.338, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [200/2032], Loss: 0.358, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [300/2032], Loss: 0.358, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [400/2032], Loss: 0.344, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [500/2032], Loss: 0.323, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [600/2032], Loss: 0.370, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [700/2032], Loss: 0.335, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [800/2032], Loss: 0.386, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [900/2032], Loss: 0.448, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [1000/2032], Loss: 0.401, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1100/2032], Loss: 0.350, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [1200/2032], Loss: 0.427, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [1300/2032], Loss: 0.375, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1400/2032], Loss: 0.337, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [1500/2032], Loss: 0.383, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1600/2032], Loss: 0.311, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [1700/2032], Loss: 0.312, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1800/2032], Loss: 0.299, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1900/2032], Loss: 0.425, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [2000/2032], Loss: 0.335, Training Accuracy: 89.67%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 65.63588221134559 %\n",
      "Epoch [8/60], Step [100/2032], Loss: 0.380, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [200/2032], Loss: 0.451, Training Accuracy: 81.52%\n",
      "Epoch [8/60], Step [300/2032], Loss: 0.353, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [400/2032], Loss: 0.441, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [500/2032], Loss: 0.423, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [600/2032], Loss: 0.391, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [700/2032], Loss: 0.446, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [800/2032], Loss: 0.326, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [900/2032], Loss: 0.345, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [1000/2032], Loss: 0.396, Training Accuracy: 81.52%\n",
      "Epoch [8/60], Step [1100/2032], Loss: 0.386, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [1200/2032], Loss: 0.319, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1300/2032], Loss: 0.433, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [1400/2032], Loss: 0.331, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [1500/2032], Loss: 0.351, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1600/2032], Loss: 0.361, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [1700/2032], Loss: 0.318, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [1800/2032], Loss: 0.309, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [1900/2032], Loss: 0.457, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [2000/2032], Loss: 0.314, Training Accuracy: 88.04%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 66.72015833533928 %\n",
      "Epoch [9/60], Step [100/2032], Loss: 0.358, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [200/2032], Loss: 0.402, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [300/2032], Loss: 0.294, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [400/2032], Loss: 0.258, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [500/2032], Loss: 0.318, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [600/2032], Loss: 0.362, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [700/2032], Loss: 0.362, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [800/2032], Loss: 0.322, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [900/2032], Loss: 0.348, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [1000/2032], Loss: 0.316, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1100/2032], Loss: 0.280, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1200/2032], Loss: 0.300, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1300/2032], Loss: 0.326, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1400/2032], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1500/2032], Loss: 0.327, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [1600/2032], Loss: 0.380, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [1700/2032], Loss: 0.287, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1800/2032], Loss: 0.253, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [1900/2032], Loss: 0.267, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [2000/2032], Loss: 0.338, Training Accuracy: 86.96%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 66.72336783545963 %\n",
      "Epoch [10/60], Step [100/2032], Loss: 0.295, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [200/2032], Loss: 0.306, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [300/2032], Loss: 0.320, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [400/2032], Loss: 0.394, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [500/2032], Loss: 0.443, Training Accuracy: 82.07%\n",
      "Epoch [10/60], Step [600/2032], Loss: 0.247, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [700/2032], Loss: 0.387, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [800/2032], Loss: 0.318, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [900/2032], Loss: 0.339, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [1000/2032], Loss: 0.318, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [1100/2032], Loss: 0.295, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1200/2032], Loss: 0.355, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1300/2032], Loss: 0.303, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1400/2032], Loss: 0.364, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [1500/2032], Loss: 0.343, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [1600/2032], Loss: 0.337, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [1700/2032], Loss: 0.338, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1800/2032], Loss: 0.310, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1900/2032], Loss: 0.352, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [2000/2032], Loss: 0.283, Training Accuracy: 89.67%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 67.76378079114178 %\n",
      "Epoch [11/60], Step [100/2032], Loss: 0.335, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [200/2032], Loss: 0.316, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [300/2032], Loss: 0.258, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [400/2032], Loss: 0.359, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [500/2032], Loss: 0.341, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [600/2032], Loss: 0.266, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [700/2032], Loss: 0.302, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [800/2032], Loss: 0.322, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [900/2032], Loss: 0.312, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1000/2032], Loss: 0.372, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [1100/2032], Loss: 0.371, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1200/2032], Loss: 0.288, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1300/2032], Loss: 0.289, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [1400/2032], Loss: 0.348, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [1500/2032], Loss: 0.331, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1600/2032], Loss: 0.298, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1700/2032], Loss: 0.256, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [1800/2032], Loss: 0.404, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [1900/2032], Loss: 0.428, Training Accuracy: 83.70%\n",
      "Epoch [11/60], Step [2000/2032], Loss: 0.249, Training Accuracy: 90.76%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 67.51343978175399 %\n",
      "Epoch [12/60], Step [100/2032], Loss: 0.277, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [200/2032], Loss: 0.275, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [300/2032], Loss: 0.283, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [400/2032], Loss: 0.290, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [500/2032], Loss: 0.379, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [600/2032], Loss: 0.315, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [700/2032], Loss: 0.249, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [800/2032], Loss: 0.357, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [900/2032], Loss: 0.285, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [1000/2032], Loss: 0.275, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [1100/2032], Loss: 0.291, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1200/2032], Loss: 0.287, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1300/2032], Loss: 0.260, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1400/2032], Loss: 0.284, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [1500/2032], Loss: 0.320, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1600/2032], Loss: 0.326, Training Accuracy: 83.70%\n",
      "Epoch [12/60], Step [1700/2032], Loss: 0.330, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [1800/2032], Loss: 0.283, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [1900/2032], Loss: 0.303, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [2000/2032], Loss: 0.305, Training Accuracy: 85.87%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 67.01329267966514 %\n",
      "Epoch [13/60], Step [100/2032], Loss: 0.304, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [200/2032], Loss: 0.311, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [300/2032], Loss: 0.276, Training Accuracy: 91.30%\n",
      "Epoch [13/60], Step [400/2032], Loss: 0.350, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [500/2032], Loss: 0.352, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [600/2032], Loss: 0.238, Training Accuracy: 93.48%\n",
      "Epoch [13/60], Step [700/2032], Loss: 0.241, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [800/2032], Loss: 0.283, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [900/2032], Loss: 0.336, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [1000/2032], Loss: 0.337, Training Accuracy: 84.24%\n",
      "Epoch [13/60], Step [1100/2032], Loss: 0.324, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [1200/2032], Loss: 0.310, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [1300/2032], Loss: 0.209, Training Accuracy: 91.30%\n",
      "Epoch [13/60], Step [1400/2032], Loss: 0.349, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [1500/2032], Loss: 0.367, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [1600/2032], Loss: 0.320, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [1700/2032], Loss: 0.321, Training Accuracy: 88.59%\n",
      "Epoch [13/60], Step [1800/2032], Loss: 0.251, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [1900/2032], Loss: 0.287, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [2000/2032], Loss: 0.303, Training Accuracy: 90.76%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 68.02909946775789 %\n",
      "Epoch [14/60], Step [100/2032], Loss: 0.289, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [200/2032], Loss: 0.374, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [300/2032], Loss: 0.339, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [400/2032], Loss: 0.300, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [500/2032], Loss: 0.315, Training Accuracy: 85.87%\n",
      "Epoch [14/60], Step [600/2032], Loss: 0.224, Training Accuracy: 94.02%\n",
      "Epoch [14/60], Step [700/2032], Loss: 0.267, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [800/2032], Loss: 0.365, Training Accuracy: 85.33%\n",
      "Epoch [14/60], Step [900/2032], Loss: 0.209, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [1000/2032], Loss: 0.257, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [1100/2032], Loss: 0.260, Training Accuracy: 88.04%\n",
      "Epoch [14/60], Step [1200/2032], Loss: 0.275, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [1300/2032], Loss: 0.371, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [1400/2032], Loss: 0.369, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [1500/2032], Loss: 0.320, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [1600/2032], Loss: 0.324, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [1700/2032], Loss: 0.217, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [1800/2032], Loss: 0.309, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [1900/2032], Loss: 0.269, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [2000/2032], Loss: 0.361, Training Accuracy: 85.87%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 66.28099173553719 %\n",
      "Epoch [15/60], Step [100/2032], Loss: 0.221, Training Accuracy: 91.85%\n",
      "Epoch [15/60], Step [200/2032], Loss: 0.295, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [300/2032], Loss: 0.260, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [400/2032], Loss: 0.291, Training Accuracy: 90.76%\n",
      "Epoch [15/60], Step [500/2032], Loss: 0.365, Training Accuracy: 85.87%\n",
      "Epoch [15/60], Step [600/2032], Loss: 0.253, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [700/2032], Loss: 0.320, Training Accuracy: 85.87%\n",
      "Epoch [15/60], Step [800/2032], Loss: 0.257, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [900/2032], Loss: 0.383, Training Accuracy: 86.96%\n",
      "Epoch [15/60], Step [1000/2032], Loss: 0.287, Training Accuracy: 90.76%\n",
      "Epoch [15/60], Step [1100/2032], Loss: 0.273, Training Accuracy: 91.30%\n",
      "Epoch [15/60], Step [1200/2032], Loss: 0.288, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [1300/2032], Loss: 0.277, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [1400/2032], Loss: 0.340, Training Accuracy: 86.41%\n",
      "Epoch [15/60], Step [1500/2032], Loss: 0.321, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [1600/2032], Loss: 0.251, Training Accuracy: 92.93%\n",
      "Epoch [15/60], Step [1700/2032], Loss: 0.233, Training Accuracy: 92.93%\n",
      "Epoch [15/60], Step [1800/2032], Loss: 0.176, Training Accuracy: 92.93%\n",
      "Epoch [15/60], Step [1900/2032], Loss: 0.185, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [2000/2032], Loss: 0.239, Training Accuracy: 91.85%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 68.01947096739683 %\n",
      "Epoch [16/60], Step [100/2032], Loss: 0.309, Training Accuracy: 88.59%\n",
      "Epoch [16/60], Step [200/2032], Loss: 0.310, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [300/2032], Loss: 0.332, Training Accuracy: 89.67%\n",
      "Epoch [16/60], Step [400/2032], Loss: 0.201, Training Accuracy: 92.93%\n",
      "Epoch [16/60], Step [500/2032], Loss: 0.291, Training Accuracy: 90.76%\n",
      "Epoch [16/60], Step [600/2032], Loss: 0.270, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [700/2032], Loss: 0.311, Training Accuracy: 88.04%\n",
      "Epoch [16/60], Step [800/2032], Loss: 0.316, Training Accuracy: 85.33%\n",
      "Epoch [16/60], Step [900/2032], Loss: 0.220, Training Accuracy: 93.48%\n",
      "Epoch [16/60], Step [1000/2032], Loss: 0.260, Training Accuracy: 88.59%\n",
      "Epoch [16/60], Step [1100/2032], Loss: 0.325, Training Accuracy: 86.41%\n",
      "Epoch [16/60], Step [1200/2032], Loss: 0.245, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [1300/2032], Loss: 0.251, Training Accuracy: 90.22%\n",
      "Epoch [16/60], Step [1400/2032], Loss: 0.289, Training Accuracy: 89.13%\n",
      "Epoch [16/60], Step [1500/2032], Loss: 0.227, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [1600/2032], Loss: 0.250, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [1700/2032], Loss: 0.329, Training Accuracy: 85.87%\n",
      "Epoch [16/60], Step [1800/2032], Loss: 0.230, Training Accuracy: 91.30%\n",
      "Epoch [16/60], Step [1900/2032], Loss: 0.226, Training Accuracy: 92.93%\n",
      "Epoch [16/60], Step [2000/2032], Loss: 0.210, Training Accuracy: 90.76%\n",
      "Testing epoch 16\n",
      "Epoch 16: Test Accuracy 69.22784776271097 %\n",
      "Epoch [17/60], Step [100/2032], Loss: 0.276, Training Accuracy: 89.67%\n",
      "Epoch [17/60], Step [200/2032], Loss: 0.307, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [300/2032], Loss: 0.239, Training Accuracy: 93.48%\n",
      "Epoch [17/60], Step [400/2032], Loss: 0.255, Training Accuracy: 90.76%\n",
      "Epoch [17/60], Step [500/2032], Loss: 0.238, Training Accuracy: 90.22%\n",
      "Epoch [17/60], Step [600/2032], Loss: 0.304, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [700/2032], Loss: 0.313, Training Accuracy: 85.87%\n",
      "Epoch [17/60], Step [800/2032], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [17/60], Step [900/2032], Loss: 0.223, Training Accuracy: 91.30%\n",
      "Epoch [17/60], Step [1000/2032], Loss: 0.244, Training Accuracy: 90.76%\n",
      "Epoch [17/60], Step [1100/2032], Loss: 0.281, Training Accuracy: 86.96%\n",
      "Epoch [17/60], Step [1200/2032], Loss: 0.251, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [1300/2032], Loss: 0.229, Training Accuracy: 92.39%\n",
      "Epoch [17/60], Step [1400/2032], Loss: 0.245, Training Accuracy: 92.93%\n",
      "Epoch [17/60], Step [1500/2032], Loss: 0.192, Training Accuracy: 94.02%\n",
      "Epoch [17/60], Step [1600/2032], Loss: 0.210, Training Accuracy: 94.02%\n",
      "Epoch [17/60], Step [1700/2032], Loss: 0.295, Training Accuracy: 87.50%\n",
      "Epoch [17/60], Step [1800/2032], Loss: 0.273, Training Accuracy: 89.13%\n",
      "Epoch [17/60], Step [1900/2032], Loss: 0.294, Training Accuracy: 88.04%\n",
      "Epoch [17/60], Step [2000/2032], Loss: 0.279, Training Accuracy: 89.67%\n",
      "Testing epoch 17\n",
      "Epoch 17: Test Accuracy 68.44472973334403 %\n",
      "Epoch [18/60], Step [100/2032], Loss: 0.239, Training Accuracy: 89.13%\n",
      "Epoch [18/60], Step [200/2032], Loss: 0.213, Training Accuracy: 93.48%\n",
      "Epoch [18/60], Step [300/2032], Loss: 0.269, Training Accuracy: 91.30%\n",
      "Epoch [18/60], Step [400/2032], Loss: 0.261, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [500/2032], Loss: 0.262, Training Accuracy: 88.59%\n",
      "Epoch [18/60], Step [600/2032], Loss: 0.241, Training Accuracy: 91.30%\n",
      "Epoch [18/60], Step [700/2032], Loss: 0.345, Training Accuracy: 85.87%\n",
      "Epoch [18/60], Step [800/2032], Loss: 0.207, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [900/2032], Loss: 0.270, Training Accuracy: 90.76%\n",
      "Epoch [18/60], Step [1000/2032], Loss: 0.336, Training Accuracy: 86.96%\n",
      "Epoch [18/60], Step [1100/2032], Loss: 0.212, Training Accuracy: 94.02%\n",
      "Epoch [18/60], Step [1200/2032], Loss: 0.270, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [1300/2032], Loss: 0.264, Training Accuracy: 90.76%\n",
      "Epoch [18/60], Step [1400/2032], Loss: 0.283, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [1500/2032], Loss: 0.252, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [1600/2032], Loss: 0.259, Training Accuracy: 88.04%\n",
      "Epoch [18/60], Step [1700/2032], Loss: 0.178, Training Accuracy: 94.57%\n",
      "Epoch [18/60], Step [1800/2032], Loss: 0.217, Training Accuracy: 91.85%\n",
      "Epoch [18/60], Step [1900/2032], Loss: 0.242, Training Accuracy: 90.22%\n",
      "Epoch [18/60], Step [2000/2032], Loss: 0.183, Training Accuracy: 95.11%\n",
      "Testing epoch 18\n",
      "Epoch 18: Test Accuracy 69.2738505977694 %\n",
      "Epoch [19/60], Step [100/2032], Loss: 0.240, Training Accuracy: 92.39%\n",
      "Epoch [19/60], Step [200/2032], Loss: 0.202, Training Accuracy: 91.30%\n",
      "Epoch [19/60], Step [300/2032], Loss: 0.355, Training Accuracy: 84.24%\n",
      "Epoch [19/60], Step [400/2032], Loss: 0.254, Training Accuracy: 88.59%\n",
      "Epoch [19/60], Step [500/2032], Loss: 0.254, Training Accuracy: 89.13%\n",
      "Epoch [19/60], Step [600/2032], Loss: 0.212, Training Accuracy: 91.85%\n",
      "Epoch [19/60], Step [700/2032], Loss: 0.293, Training Accuracy: 89.67%\n",
      "Epoch [19/60], Step [800/2032], Loss: 0.371, Training Accuracy: 86.96%\n",
      "Epoch [19/60], Step [900/2032], Loss: 0.307, Training Accuracy: 88.59%\n",
      "Epoch [19/60], Step [1000/2032], Loss: 0.242, Training Accuracy: 90.22%\n",
      "Epoch [19/60], Step [1100/2032], Loss: 0.243, Training Accuracy: 89.67%\n",
      "Epoch [19/60], Step [1200/2032], Loss: 0.206, Training Accuracy: 92.93%\n",
      "Epoch [19/60], Step [1300/2032], Loss: 0.150, Training Accuracy: 96.20%\n",
      "Epoch [19/60], Step [1400/2032], Loss: 0.257, Training Accuracy: 90.76%\n",
      "Epoch [19/60], Step [1500/2032], Loss: 0.272, Training Accuracy: 89.67%\n",
      "Epoch [19/60], Step [1600/2032], Loss: 0.238, Training Accuracy: 90.76%\n",
      "Epoch [19/60], Step [1700/2032], Loss: 0.292, Training Accuracy: 88.59%\n",
      "Epoch [19/60], Step [1800/2032], Loss: 0.262, Training Accuracy: 91.30%\n",
      "Epoch [19/60], Step [1900/2032], Loss: 0.208, Training Accuracy: 92.93%\n",
      "Epoch [19/60], Step [2000/2032], Loss: 0.234, Training Accuracy: 92.39%\n",
      "Testing epoch 19\n",
      "Epoch 19: Test Accuracy 68.77423841236727 %\n",
      "Epoch [20/60], Step [100/2032], Loss: 0.295, Training Accuracy: 89.13%\n",
      "Epoch [20/60], Step [200/2032], Loss: 0.261, Training Accuracy: 91.85%\n",
      "Epoch [20/60], Step [300/2032], Loss: 0.241, Training Accuracy: 89.13%\n",
      "Epoch [20/60], Step [400/2032], Loss: 0.311, Training Accuracy: 88.04%\n",
      "Epoch [20/60], Step [500/2032], Loss: 0.206, Training Accuracy: 91.85%\n",
      "Epoch [20/60], Step [600/2032], Loss: 0.244, Training Accuracy: 90.76%\n",
      "Epoch [20/60], Step [700/2032], Loss: 0.246, Training Accuracy: 91.30%\n",
      "Epoch [20/60], Step [800/2032], Loss: 0.258, Training Accuracy: 91.85%\n",
      "Epoch [20/60], Step [900/2032], Loss: 0.293, Training Accuracy: 90.76%\n",
      "Epoch [20/60], Step [1000/2032], Loss: 0.248, Training Accuracy: 91.85%\n",
      "Epoch [20/60], Step [1100/2032], Loss: 0.206, Training Accuracy: 93.48%\n",
      "Epoch [20/60], Step [1200/2032], Loss: 0.225, Training Accuracy: 92.93%\n",
      "Epoch [20/60], Step [1300/2032], Loss: 0.296, Training Accuracy: 89.67%\n",
      "Epoch [20/60], Step [1400/2032], Loss: 0.233, Training Accuracy: 92.39%\n",
      "Epoch [20/60], Step [1500/2032], Loss: 0.270, Training Accuracy: 91.85%\n",
      "Epoch [20/60], Step [1600/2032], Loss: 0.397, Training Accuracy: 84.78%\n",
      "Epoch [20/60], Step [1700/2032], Loss: 0.243, Training Accuracy: 92.39%\n",
      "Epoch [20/60], Step [1800/2032], Loss: 0.265, Training Accuracy: 92.93%\n",
      "Epoch [20/60], Step [1900/2032], Loss: 0.283, Training Accuracy: 89.13%\n",
      "Epoch [20/60], Step [2000/2032], Loss: 0.223, Training Accuracy: 92.93%\n",
      "Testing epoch 20\n",
      "Epoch 20: Test Accuracy 68.16122388937923 %\n",
      "Epoch [21/60], Step [100/2032], Loss: 0.249, Training Accuracy: 91.85%\n",
      "Epoch [21/60], Step [200/2032], Loss: 0.230, Training Accuracy: 90.76%\n",
      "Epoch [21/60], Step [300/2032], Loss: 0.257, Training Accuracy: 90.76%\n",
      "Epoch [21/60], Step [400/2032], Loss: 0.169, Training Accuracy: 94.02%\n",
      "Epoch [21/60], Step [500/2032], Loss: 0.266, Training Accuracy: 90.76%\n",
      "Epoch [21/60], Step [600/2032], Loss: 0.235, Training Accuracy: 92.39%\n",
      "Epoch [21/60], Step [700/2032], Loss: 0.270, Training Accuracy: 90.76%\n",
      "Epoch [21/60], Step [800/2032], Loss: 0.302, Training Accuracy: 89.67%\n",
      "Epoch [21/60], Step [900/2032], Loss: 0.248, Training Accuracy: 91.30%\n",
      "Epoch [21/60], Step [1000/2032], Loss: 0.240, Training Accuracy: 92.39%\n",
      "Epoch [21/60], Step [1100/2032], Loss: 0.188, Training Accuracy: 92.93%\n",
      "Epoch [21/60], Step [1200/2032], Loss: 0.256, Training Accuracy: 91.30%\n",
      "Epoch [21/60], Step [1300/2032], Loss: 0.268, Training Accuracy: 89.13%\n",
      "Epoch [21/60], Step [1400/2032], Loss: 0.204, Training Accuracy: 91.30%\n",
      "Epoch [21/60], Step [1500/2032], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [21/60], Step [1600/2032], Loss: 0.249, Training Accuracy: 89.13%\n",
      "Epoch [21/60], Step [1700/2032], Loss: 0.183, Training Accuracy: 92.39%\n",
      "Epoch [21/60], Step [1800/2032], Loss: 0.165, Training Accuracy: 95.11%\n",
      "Epoch [21/60], Step [1900/2032], Loss: 0.252, Training Accuracy: 90.76%\n",
      "Epoch [21/60], Step [2000/2032], Loss: 0.259, Training Accuracy: 90.22%\n",
      "Testing epoch 21\n",
      "Epoch 21: Test Accuracy 66.56128807938163 %\n",
      "Epoch [22/60], Step [100/2032], Loss: 0.270, Training Accuracy: 89.13%\n",
      "Epoch [22/60], Step [200/2032], Loss: 0.277, Training Accuracy: 89.67%\n",
      "Epoch [22/60], Step [300/2032], Loss: 0.234, Training Accuracy: 92.39%\n",
      "Epoch [22/60], Step [400/2032], Loss: 0.312, Training Accuracy: 87.50%\n",
      "Epoch [22/60], Step [500/2032], Loss: 0.251, Training Accuracy: 90.22%\n",
      "Epoch [22/60], Step [600/2032], Loss: 0.173, Training Accuracy: 93.48%\n",
      "Epoch [22/60], Step [700/2032], Loss: 0.279, Training Accuracy: 89.67%\n",
      "Epoch [22/60], Step [800/2032], Loss: 0.375, Training Accuracy: 85.33%\n",
      "Epoch [22/60], Step [900/2032], Loss: 0.251, Training Accuracy: 90.76%\n",
      "Epoch [22/60], Step [1000/2032], Loss: 0.331, Training Accuracy: 84.24%\n",
      "Epoch [22/60], Step [1100/2032], Loss: 0.279, Training Accuracy: 88.04%\n",
      "Epoch [22/60], Step [1200/2032], Loss: 0.233, Training Accuracy: 91.85%\n",
      "Epoch [22/60], Step [1300/2032], Loss: 0.265, Training Accuracy: 87.50%\n",
      "Epoch [22/60], Step [1400/2032], Loss: 0.268, Training Accuracy: 89.13%\n",
      "Epoch [22/60], Step [1500/2032], Loss: 0.226, Training Accuracy: 91.85%\n",
      "Epoch [22/60], Step [1600/2032], Loss: 0.306, Training Accuracy: 89.67%\n",
      "Epoch [22/60], Step [1700/2032], Loss: 0.260, Training Accuracy: 89.67%\n",
      "Epoch [22/60], Step [1800/2032], Loss: 0.233, Training Accuracy: 89.13%\n",
      "Epoch [22/60], Step [1900/2032], Loss: 0.279, Training Accuracy: 90.76%\n",
      "Epoch [22/60], Step [2000/2032], Loss: 0.260, Training Accuracy: 89.67%\n",
      "Testing epoch 22\n",
      "Epoch 22: Test Accuracy 68.1318034716093 %\n",
      "Epoch [23/60], Step [100/2032], Loss: 0.220, Training Accuracy: 92.93%\n",
      "Epoch [23/60], Step [200/2032], Loss: 0.244, Training Accuracy: 91.30%\n",
      "Epoch [23/60], Step [300/2032], Loss: 0.217, Training Accuracy: 90.76%\n",
      "Epoch [23/60], Step [400/2032], Loss: 0.192, Training Accuracy: 94.02%\n",
      "Epoch [23/60], Step [500/2032], Loss: 0.202, Training Accuracy: 92.39%\n",
      "Epoch [23/60], Step [600/2032], Loss: 0.251, Training Accuracy: 89.67%\n",
      "Epoch [23/60], Step [700/2032], Loss: 0.287, Training Accuracy: 86.41%\n",
      "Epoch [23/60], Step [800/2032], Loss: 0.255, Training Accuracy: 90.22%\n",
      "Epoch [23/60], Step [900/2032], Loss: 0.191, Training Accuracy: 94.57%\n",
      "Epoch [23/60], Step [1000/2032], Loss: 0.220, Training Accuracy: 91.85%\n",
      "Epoch [23/60], Step [1100/2032], Loss: 0.309, Training Accuracy: 89.67%\n",
      "Epoch [23/60], Step [1200/2032], Loss: 0.194, Training Accuracy: 93.48%\n",
      "Epoch [23/60], Step [1300/2032], Loss: 0.254, Training Accuracy: 90.76%\n",
      "Epoch [23/60], Step [1400/2032], Loss: 0.259, Training Accuracy: 92.93%\n",
      "Epoch [23/60], Step [1500/2032], Loss: 0.180, Training Accuracy: 94.02%\n",
      "Epoch [23/60], Step [1600/2032], Loss: 0.182, Training Accuracy: 94.57%\n",
      "Epoch [23/60], Step [1700/2032], Loss: 0.263, Training Accuracy: 86.96%\n",
      "Epoch [23/60], Step [1800/2032], Loss: 0.216, Training Accuracy: 92.93%\n",
      "Epoch [23/60], Step [1900/2032], Loss: 0.207, Training Accuracy: 92.39%\n",
      "Epoch [23/60], Step [2000/2032], Loss: 0.258, Training Accuracy: 91.30%\n",
      "Testing epoch 23\n",
      "Epoch 23: Test Accuracy 68.5693653213512 %\n",
      "Finished early after 23 epochs!\n",
      "Final Training Accuracy 91.38627871084398 %\n",
      "Final Test Accuracy 68.62820615689107 %\n",
      "Epoch [1/60], Step [100/1990], Loss: 1.083, Training Accuracy: 60.87%\n",
      "Epoch [1/60], Step [200/1990], Loss: 0.868, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [300/1990], Loss: 0.817, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [400/1990], Loss: 0.684, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [500/1990], Loss: 0.659, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [600/1990], Loss: 0.605, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [700/1990], Loss: 0.654, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [800/1990], Loss: 0.610, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [900/1990], Loss: 0.569, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [1000/1990], Loss: 0.646, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [1100/1990], Loss: 0.589, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1200/1990], Loss: 0.615, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1300/1990], Loss: 0.500, Training Accuracy: 81.52%\n",
      "Epoch [1/60], Step [1400/1990], Loss: 0.482, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [1500/1990], Loss: 0.625, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [1600/1990], Loss: 0.528, Training Accuracy: 77.72%\n",
      "Epoch [1/60], Step [1700/1990], Loss: 0.526, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1800/1990], Loss: 0.570, Training Accuracy: 78.80%\n",
      "Epoch [1/60], Step [1900/1990], Loss: 0.467, Training Accuracy: 80.98%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 51.93442521917323 %\n",
      "Epoch [2/60], Step [100/1990], Loss: 0.548, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [200/1990], Loss: 0.414, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [300/1990], Loss: 0.527, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [400/1990], Loss: 0.518, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [500/1990], Loss: 0.588, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [600/1990], Loss: 0.409, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [700/1990], Loss: 0.437, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [800/1990], Loss: 0.503, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [900/1990], Loss: 0.431, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1000/1990], Loss: 0.479, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [1100/1990], Loss: 0.356, Training Accuracy: 87.50%\n",
      "Epoch [2/60], Step [1200/1990], Loss: 0.444, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [1300/1990], Loss: 0.432, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1400/1990], Loss: 0.441, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1500/1990], Loss: 0.463, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1600/1990], Loss: 0.485, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1700/1990], Loss: 0.404, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [1800/1990], Loss: 0.460, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [1900/1990], Loss: 0.613, Training Accuracy: 76.63%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 53.13108279535101 %\n",
      "Epoch [3/60], Step [100/1990], Loss: 0.532, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [200/1990], Loss: 0.457, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [300/1990], Loss: 0.394, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [400/1990], Loss: 0.391, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [500/1990], Loss: 0.348, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [600/1990], Loss: 0.493, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [700/1990], Loss: 0.360, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [800/1990], Loss: 0.541, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [900/1990], Loss: 0.393, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1000/1990], Loss: 0.498, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [1100/1990], Loss: 0.499, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1200/1990], Loss: 0.417, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1300/1990], Loss: 0.405, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1400/1990], Loss: 0.415, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1500/1990], Loss: 0.414, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1600/1990], Loss: 0.446, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [1700/1990], Loss: 0.423, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1800/1990], Loss: 0.381, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1900/1990], Loss: 0.362, Training Accuracy: 87.50%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 52.516832812042594 %\n",
      "Epoch [4/60], Step [100/1990], Loss: 0.319, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [200/1990], Loss: 0.406, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [300/1990], Loss: 0.367, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [400/1990], Loss: 0.309, Training Accuracy: 89.67%\n",
      "Epoch [4/60], Step [500/1990], Loss: 0.433, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [600/1990], Loss: 0.439, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [700/1990], Loss: 0.368, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [800/1990], Loss: 0.426, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [900/1990], Loss: 0.437, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1000/1990], Loss: 0.521, Training Accuracy: 79.35%\n",
      "Epoch [4/60], Step [1100/1990], Loss: 0.445, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [1200/1990], Loss: 0.415, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [1300/1990], Loss: 0.485, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [1400/1990], Loss: 0.306, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [1500/1990], Loss: 0.357, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1600/1990], Loss: 0.368, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1700/1990], Loss: 0.487, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [1800/1990], Loss: 0.448, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [1900/1990], Loss: 0.388, Training Accuracy: 83.70%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 49.17030029428532 %\n",
      "Epoch [5/60], Step [100/1990], Loss: 0.417, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [200/1990], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [300/1990], Loss: 0.327, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [400/1990], Loss: 0.363, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [500/1990], Loss: 0.341, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [600/1990], Loss: 0.356, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [700/1990], Loss: 0.327, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [800/1990], Loss: 0.325, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [900/1990], Loss: 0.386, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [1000/1990], Loss: 0.341, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1100/1990], Loss: 0.397, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [1200/1990], Loss: 0.371, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1300/1990], Loss: 0.341, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1400/1990], Loss: 0.232, Training Accuracy: 92.39%\n",
      "Epoch [5/60], Step [1500/1990], Loss: 0.422, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [1600/1990], Loss: 0.344, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1700/1990], Loss: 0.403, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [1800/1990], Loss: 0.404, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [1900/1990], Loss: 0.310, Training Accuracy: 89.13%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 50.15433287624096 %\n",
      "Epoch [6/60], Step [100/1990], Loss: 0.373, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [200/1990], Loss: 0.394, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [300/1990], Loss: 0.409, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [400/1990], Loss: 0.361, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [500/1990], Loss: 0.400, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [600/1990], Loss: 0.267, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [700/1990], Loss: 0.493, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [800/1990], Loss: 0.366, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [900/1990], Loss: 0.393, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1000/1990], Loss: 0.342, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1100/1990], Loss: 0.420, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1200/1990], Loss: 0.387, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1300/1990], Loss: 0.303, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1400/1990], Loss: 0.387, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1500/1990], Loss: 0.303, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [1600/1990], Loss: 0.381, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1700/1990], Loss: 0.396, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [1800/1990], Loss: 0.282, Training Accuracy: 92.39%\n",
      "Epoch [6/60], Step [1900/1990], Loss: 0.334, Training Accuracy: 86.96%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 50.98634372319718 %\n",
      "Epoch [7/60], Step [100/1990], Loss: 0.331, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [200/1990], Loss: 0.310, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [300/1990], Loss: 0.350, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [400/1990], Loss: 0.354, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [500/1990], Loss: 0.260, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [600/1990], Loss: 0.233, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [700/1990], Loss: 0.265, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [800/1990], Loss: 0.291, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [900/1990], Loss: 0.316, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [1000/1990], Loss: 0.251, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1100/1990], Loss: 0.336, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [1200/1990], Loss: 0.292, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [1300/1990], Loss: 0.295, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1400/1990], Loss: 0.334, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1500/1990], Loss: 0.362, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [1600/1990], Loss: 0.308, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1700/1990], Loss: 0.327, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1800/1990], Loss: 0.257, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1900/1990], Loss: 0.327, Training Accuracy: 84.78%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 50.68743612262402 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 87.4668458561076 %\n",
      "Final Test Accuracy 50.74906655573189 %\n",
      "Epoch [1/60], Step [100/2037], Loss: 0.992, Training Accuracy: 63.59%\n",
      "Epoch [1/60], Step [200/2037], Loss: 0.892, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [300/2037], Loss: 0.763, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [400/2037], Loss: 0.749, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [500/2037], Loss: 0.734, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [600/2037], Loss: 0.662, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [700/2037], Loss: 0.543, Training Accuracy: 83.15%\n",
      "Epoch [1/60], Step [800/2037], Loss: 0.618, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [900/2037], Loss: 0.551, Training Accuracy: 81.52%\n",
      "Epoch [1/60], Step [1000/2037], Loss: 0.700, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1100/2037], Loss: 0.543, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1200/2037], Loss: 0.579, Training Accuracy: 78.26%\n",
      "Epoch [1/60], Step [1300/2037], Loss: 0.496, Training Accuracy: 83.15%\n",
      "Epoch [1/60], Step [1400/2037], Loss: 0.557, Training Accuracy: 78.80%\n",
      "Epoch [1/60], Step [1500/2037], Loss: 0.563, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1600/2037], Loss: 0.504, Training Accuracy: 82.61%\n",
      "Epoch [1/60], Step [1700/2037], Loss: 0.583, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1800/2037], Loss: 0.537, Training Accuracy: 80.98%\n",
      "Epoch [1/60], Step [1900/2037], Loss: 0.547, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [2000/2037], Loss: 0.541, Training Accuracy: 81.52%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 65.46718495459605 %\n",
      "Epoch [2/60], Step [100/2037], Loss: 0.512, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [200/2037], Loss: 0.553, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [300/2037], Loss: 0.501, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [400/2037], Loss: 0.563, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [500/2037], Loss: 0.596, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [600/2037], Loss: 0.520, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [700/2037], Loss: 0.493, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [800/2037], Loss: 0.401, Training Accuracy: 84.24%\n",
      "Epoch [2/60], Step [900/2037], Loss: 0.560, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1000/2037], Loss: 0.576, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [1100/2037], Loss: 0.443, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [1200/2037], Loss: 0.411, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1300/2037], Loss: 0.450, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [1400/2037], Loss: 0.481, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1500/2037], Loss: 0.429, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [1600/2037], Loss: 0.509, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1700/2037], Loss: 0.524, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1800/2037], Loss: 0.580, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1900/2037], Loss: 0.421, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [2000/2037], Loss: 0.466, Training Accuracy: 84.24%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 66.95484335768856 %\n",
      "Epoch [3/60], Step [100/2037], Loss: 0.635, Training Accuracy: 74.46%\n",
      "Epoch [3/60], Step [200/2037], Loss: 0.429, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [300/2037], Loss: 0.475, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [400/2037], Loss: 0.557, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [500/2037], Loss: 0.406, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [600/2037], Loss: 0.400, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [700/2037], Loss: 0.493, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [800/2037], Loss: 0.451, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [900/2037], Loss: 0.392, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1000/2037], Loss: 0.424, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1100/2037], Loss: 0.454, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1200/2037], Loss: 0.400, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [1300/2037], Loss: 0.384, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1400/2037], Loss: 0.419, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1500/2037], Loss: 0.493, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1600/2037], Loss: 0.378, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1700/2037], Loss: 0.456, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1800/2037], Loss: 0.371, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [1900/2037], Loss: 0.401, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [2000/2037], Loss: 0.427, Training Accuracy: 86.41%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 67.3338817292752 %\n",
      "Epoch [4/60], Step [100/2037], Loss: 0.367, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [200/2037], Loss: 0.421, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [300/2037], Loss: 0.510, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/2037], Loss: 0.432, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [500/2037], Loss: 0.355, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [600/2037], Loss: 0.402, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [700/2037], Loss: 0.395, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [800/2037], Loss: 0.428, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [900/2037], Loss: 0.425, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [1000/2037], Loss: 0.377, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1100/2037], Loss: 0.476, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1200/2037], Loss: 0.363, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1300/2037], Loss: 0.283, Training Accuracy: 91.30%\n",
      "Epoch [4/60], Step [1400/2037], Loss: 0.396, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [1500/2037], Loss: 0.332, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1600/2037], Loss: 0.436, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1700/2037], Loss: 0.351, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1800/2037], Loss: 0.318, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [1900/2037], Loss: 0.409, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [2000/2037], Loss: 0.355, Training Accuracy: 88.04%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 67.63388656806293 %\n",
      "Epoch [5/60], Step [100/2037], Loss: 0.404, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [200/2037], Loss: 0.358, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [300/2037], Loss: 0.422, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [400/2037], Loss: 0.426, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [500/2037], Loss: 0.411, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [600/2037], Loss: 0.342, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [700/2037], Loss: 0.373, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [800/2037], Loss: 0.369, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [900/2037], Loss: 0.307, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1000/2037], Loss: 0.418, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1100/2037], Loss: 0.447, Training Accuracy: 78.80%\n",
      "Epoch [5/60], Step [1200/2037], Loss: 0.374, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1300/2037], Loss: 0.343, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [1400/2037], Loss: 0.331, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [1500/2037], Loss: 0.304, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1600/2037], Loss: 0.266, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [1700/2037], Loss: 0.396, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1800/2037], Loss: 0.302, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [1900/2037], Loss: 0.347, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [2000/2037], Loss: 0.369, Training Accuracy: 86.41%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 66.71612983005102 %\n",
      "Epoch [6/60], Step [100/2037], Loss: 0.444, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [200/2037], Loss: 0.372, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [300/2037], Loss: 0.348, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [400/2037], Loss: 0.337, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [500/2037], Loss: 0.313, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [600/2037], Loss: 0.425, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [700/2037], Loss: 0.359, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [800/2037], Loss: 0.342, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [900/2037], Loss: 0.401, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1000/2037], Loss: 0.416, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1100/2037], Loss: 0.267, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [1200/2037], Loss: 0.249, Training Accuracy: 92.93%\n",
      "Epoch [6/60], Step [1300/2037], Loss: 0.413, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1400/2037], Loss: 0.293, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1500/2037], Loss: 0.282, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [1600/2037], Loss: 0.350, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1700/2037], Loss: 0.286, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1800/2037], Loss: 0.427, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1900/2037], Loss: 0.281, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [2000/2037], Loss: 0.303, Training Accuracy: 91.30%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 67.7349634671527 %\n",
      "Epoch [7/60], Step [100/2037], Loss: 0.347, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [200/2037], Loss: 0.252, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [300/2037], Loss: 0.271, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [400/2037], Loss: 0.332, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [500/2037], Loss: 0.323, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [600/2037], Loss: 0.379, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [700/2037], Loss: 0.314, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [800/2037], Loss: 0.276, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [900/2037], Loss: 0.385, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [1000/2037], Loss: 0.287, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [1100/2037], Loss: 0.275, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1200/2037], Loss: 0.376, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [1300/2037], Loss: 0.345, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [1400/2037], Loss: 0.336, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1500/2037], Loss: 0.303, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1600/2037], Loss: 0.289, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1700/2037], Loss: 0.395, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1800/2037], Loss: 0.271, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1900/2037], Loss: 0.262, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [2000/2037], Loss: 0.278, Training Accuracy: 91.85%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 68.09679726017086 %\n",
      "Epoch [8/60], Step [100/2037], Loss: 0.317, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [200/2037], Loss: 0.382, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [300/2037], Loss: 0.313, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [400/2037], Loss: 0.290, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [500/2037], Loss: 0.321, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [600/2037], Loss: 0.352, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [700/2037], Loss: 0.310, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [800/2037], Loss: 0.262, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [900/2037], Loss: 0.287, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1000/2037], Loss: 0.333, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1100/2037], Loss: 0.292, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1200/2037], Loss: 0.338, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1300/2037], Loss: 0.254, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [1400/2037], Loss: 0.291, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1500/2037], Loss: 0.273, Training Accuracy: 93.48%\n",
      "Epoch [8/60], Step [1600/2037], Loss: 0.336, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1700/2037], Loss: 0.316, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1800/2037], Loss: 0.297, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1900/2037], Loss: 0.416, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [2000/2037], Loss: 0.310, Training Accuracy: 90.22%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 66.27149900267209 %\n",
      "Epoch [9/60], Step [100/2037], Loss: 0.311, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [200/2037], Loss: 0.288, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [300/2037], Loss: 0.234, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [400/2037], Loss: 0.384, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [500/2037], Loss: 0.211, Training Accuracy: 94.02%\n",
      "Epoch [9/60], Step [600/2037], Loss: 0.428, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [700/2037], Loss: 0.286, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [800/2037], Loss: 0.334, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [900/2037], Loss: 0.284, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [1000/2037], Loss: 0.213, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [1100/2037], Loss: 0.297, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [1200/2037], Loss: 0.327, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1300/2037], Loss: 0.325, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1400/2037], Loss: 0.320, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1500/2037], Loss: 0.231, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [1600/2037], Loss: 0.252, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1700/2037], Loss: 0.378, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [1800/2037], Loss: 0.268, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [1900/2037], Loss: 0.332, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [2000/2037], Loss: 0.230, Training Accuracy: 90.22%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 67.1768899498379 %\n",
      "Epoch [10/60], Step [100/2037], Loss: 0.281, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [200/2037], Loss: 0.293, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [300/2037], Loss: 0.316, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [400/2037], Loss: 0.244, Training Accuracy: 91.85%\n",
      "Epoch [10/60], Step [500/2037], Loss: 0.314, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [600/2037], Loss: 0.273, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [700/2037], Loss: 0.249, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [800/2037], Loss: 0.266, Training Accuracy: 91.85%\n",
      "Epoch [10/60], Step [900/2037], Loss: 0.266, Training Accuracy: 94.02%\n",
      "Epoch [10/60], Step [1000/2037], Loss: 0.282, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1100/2037], Loss: 0.303, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1200/2037], Loss: 0.236, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [1300/2037], Loss: 0.382, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1400/2037], Loss: 0.306, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1500/2037], Loss: 0.374, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1600/2037], Loss: 0.337, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [1700/2037], Loss: 0.281, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [1800/2037], Loss: 0.274, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [1900/2037], Loss: 0.244, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [2000/2037], Loss: 0.337, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 66.89301440345812 %\n",
      "Epoch [11/60], Step [100/2037], Loss: 0.286, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [200/2037], Loss: 0.253, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [300/2037], Loss: 0.322, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [400/2037], Loss: 0.361, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [500/2037], Loss: 0.237, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [600/2037], Loss: 0.251, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [700/2037], Loss: 0.305, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [800/2037], Loss: 0.198, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [900/2037], Loss: 0.232, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1000/2037], Loss: 0.355, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1100/2037], Loss: 0.350, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1200/2037], Loss: 0.427, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [1300/2037], Loss: 0.410, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [1400/2037], Loss: 0.327, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1500/2037], Loss: 0.280, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1600/2037], Loss: 0.269, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [1700/2037], Loss: 0.242, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [1800/2037], Loss: 0.269, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [1900/2037], Loss: 0.249, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [2000/2037], Loss: 0.327, Training Accuracy: 86.96%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 67.88227767114523 %\n",
      "Epoch [12/60], Step [100/2037], Loss: 0.346, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [200/2037], Loss: 0.257, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [300/2037], Loss: 0.250, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [400/2037], Loss: 0.247, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [500/2037], Loss: 0.232, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [600/2037], Loss: 0.187, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [700/2037], Loss: 0.288, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [800/2037], Loss: 0.201, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [900/2037], Loss: 0.194, Training Accuracy: 92.93%\n",
      "Epoch [12/60], Step [1000/2037], Loss: 0.224, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [1100/2037], Loss: 0.304, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [1200/2037], Loss: 0.268, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [1300/2037], Loss: 0.277, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1400/2037], Loss: 0.273, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1500/2037], Loss: 0.278, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1600/2037], Loss: 0.240, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [1700/2037], Loss: 0.293, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [1800/2037], Loss: 0.375, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [1900/2037], Loss: 0.275, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [2000/2037], Loss: 0.289, Training Accuracy: 90.22%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 66.97849965322021 %\n",
      "Finished early after 12 epochs!\n",
      "Final Training Accuracy 90.31176211630047 %\n",
      "Final Test Accuracy 66.90914369586606 %\n",
      "Epoch [1/60], Step [100/2070], Loss: 1.085, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [200/2070], Loss: 0.907, Training Accuracy: 64.13%\n",
      "Epoch [1/60], Step [300/2070], Loss: 0.792, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [400/2070], Loss: 0.846, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [500/2070], Loss: 0.822, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [600/2070], Loss: 0.809, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [700/2070], Loss: 0.611, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [800/2070], Loss: 0.663, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [900/2070], Loss: 0.778, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [1000/2070], Loss: 0.701, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [1100/2070], Loss: 0.711, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [1200/2070], Loss: 0.760, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1300/2070], Loss: 0.562, Training Accuracy: 80.98%\n",
      "Epoch [1/60], Step [1400/2070], Loss: 0.615, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1500/2070], Loss: 0.571, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [1600/2070], Loss: 0.581, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1700/2070], Loss: 0.663, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [1800/2070], Loss: 0.584, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1900/2070], Loss: 0.574, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [2000/2070], Loss: 0.595, Training Accuracy: 75.00%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 65.77269900497512 %\n",
      "Epoch [2/60], Step [100/2070], Loss: 0.568, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [200/2070], Loss: 0.524, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [300/2070], Loss: 0.615, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [400/2070], Loss: 0.523, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [500/2070], Loss: 0.733, Training Accuracy: 69.57%\n",
      "Epoch [2/60], Step [600/2070], Loss: 0.477, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [700/2070], Loss: 0.527, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [800/2070], Loss: 0.557, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [900/2070], Loss: 0.594, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [1000/2070], Loss: 0.578, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [1100/2070], Loss: 0.500, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [1200/2070], Loss: 0.586, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1300/2070], Loss: 0.558, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1400/2070], Loss: 0.567, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1500/2070], Loss: 0.490, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [1600/2070], Loss: 0.570, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1700/2070], Loss: 0.566, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1800/2070], Loss: 0.467, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [1900/2070], Loss: 0.528, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [2000/2070], Loss: 0.470, Training Accuracy: 82.61%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 67.209710376688 %\n",
      "Epoch [3/60], Step [100/2070], Loss: 0.444, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [200/2070], Loss: 0.427, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [300/2070], Loss: 0.410, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [400/2070], Loss: 0.490, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [500/2070], Loss: 0.523, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [600/2070], Loss: 0.493, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [700/2070], Loss: 0.548, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [800/2070], Loss: 0.446, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [900/2070], Loss: 0.532, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1000/2070], Loss: 0.471, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1100/2070], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1200/2070], Loss: 0.367, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [1300/2070], Loss: 0.475, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1400/2070], Loss: 0.442, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [1500/2070], Loss: 0.415, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [1600/2070], Loss: 0.467, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1700/2070], Loss: 0.456, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1800/2070], Loss: 0.460, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1900/2070], Loss: 0.505, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [2000/2070], Loss: 0.382, Training Accuracy: 85.87%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 69.11480543710022 %\n",
      "Epoch [4/60], Step [100/2070], Loss: 0.374, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [200/2070], Loss: 0.519, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [300/2070], Loss: 0.495, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [400/2070], Loss: 0.395, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [500/2070], Loss: 0.531, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [600/2070], Loss: 0.399, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [700/2070], Loss: 0.518, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [800/2070], Loss: 0.445, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [900/2070], Loss: 0.406, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1000/2070], Loss: 0.422, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [1100/2070], Loss: 0.471, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [1200/2070], Loss: 0.456, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [1300/2070], Loss: 0.378, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1400/2070], Loss: 0.535, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [1500/2070], Loss: 0.443, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [1600/2070], Loss: 0.455, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1700/2070], Loss: 0.443, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1800/2070], Loss: 0.360, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1900/2070], Loss: 0.396, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [2000/2070], Loss: 0.314, Training Accuracy: 84.78%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 69.08926350390902 %\n",
      "Epoch [5/60], Step [100/2070], Loss: 0.488, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [200/2070], Loss: 0.395, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [300/2070], Loss: 0.354, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [400/2070], Loss: 0.378, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [500/2070], Loss: 0.480, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [600/2070], Loss: 0.431, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [700/2070], Loss: 0.447, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [800/2070], Loss: 0.397, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [900/2070], Loss: 0.410, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1000/2070], Loss: 0.380, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1100/2070], Loss: 0.443, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1200/2070], Loss: 0.437, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [1300/2070], Loss: 0.418, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1400/2070], Loss: 0.389, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1500/2070], Loss: 0.482, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [1600/2070], Loss: 0.407, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1700/2070], Loss: 0.453, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [1800/2070], Loss: 0.465, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1900/2070], Loss: 0.402, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [2000/2070], Loss: 0.323, Training Accuracy: 90.22%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 68.38908137882018 %\n",
      "Epoch [6/60], Step [100/2070], Loss: 0.460, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [200/2070], Loss: 0.368, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [300/2070], Loss: 0.397, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [400/2070], Loss: 0.367, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [500/2070], Loss: 0.405, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [600/2070], Loss: 0.334, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [700/2070], Loss: 0.370, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [800/2070], Loss: 0.403, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [900/2070], Loss: 0.422, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1000/2070], Loss: 0.382, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1100/2070], Loss: 0.305, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1200/2070], Loss: 0.371, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1300/2070], Loss: 0.443, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [1400/2070], Loss: 0.315, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [1500/2070], Loss: 0.417, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1600/2070], Loss: 0.293, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [1700/2070], Loss: 0.437, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [1800/2070], Loss: 0.401, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1900/2070], Loss: 0.412, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [2000/2070], Loss: 0.395, Training Accuracy: 84.78%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 67.598947228145 %\n",
      "Epoch [7/60], Step [100/2070], Loss: 0.436, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [200/2070], Loss: 0.402, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [300/2070], Loss: 0.411, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [400/2070], Loss: 0.450, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [500/2070], Loss: 0.326, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [600/2070], Loss: 0.328, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [700/2070], Loss: 0.233, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [800/2070], Loss: 0.360, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [900/2070], Loss: 0.301, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1000/2070], Loss: 0.371, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [1100/2070], Loss: 0.396, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [1200/2070], Loss: 0.378, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1300/2070], Loss: 0.271, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [1400/2070], Loss: 0.334, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [1500/2070], Loss: 0.341, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [1600/2070], Loss: 0.285, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [1700/2070], Loss: 0.295, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1800/2070], Loss: 0.357, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [1900/2070], Loss: 0.376, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [2000/2070], Loss: 0.306, Training Accuracy: 89.13%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 69.16477878464818 %\n",
      "Epoch [8/60], Step [100/2070], Loss: 0.437, Training Accuracy: 83.70%\n",
      "Epoch [8/60], Step [200/2070], Loss: 0.318, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [300/2070], Loss: 0.371, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [400/2070], Loss: 0.343, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [500/2070], Loss: 0.364, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [600/2070], Loss: 0.349, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [700/2070], Loss: 0.389, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [800/2070], Loss: 0.254, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [900/2070], Loss: 0.323, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [1000/2070], Loss: 0.366, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [1100/2070], Loss: 0.309, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1200/2070], Loss: 0.377, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1300/2070], Loss: 0.377, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [1400/2070], Loss: 0.379, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [1500/2070], Loss: 0.356, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1600/2070], Loss: 0.227, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [1700/2070], Loss: 0.234, Training Accuracy: 95.65%\n",
      "Epoch [8/60], Step [1800/2070], Loss: 0.369, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1900/2070], Loss: 0.317, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [2000/2070], Loss: 0.253, Training Accuracy: 91.85%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 68.29968461265103 %\n",
      "Epoch [9/60], Step [100/2070], Loss: 0.349, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [200/2070], Loss: 0.313, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [300/2070], Loss: 0.320, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [400/2070], Loss: 0.316, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [500/2070], Loss: 0.293, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [600/2070], Loss: 0.352, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [700/2070], Loss: 0.346, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [800/2070], Loss: 0.343, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [900/2070], Loss: 0.309, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1000/2070], Loss: 0.279, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1100/2070], Loss: 0.266, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1200/2070], Loss: 0.301, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1300/2070], Loss: 0.371, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [1400/2070], Loss: 0.238, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [1500/2070], Loss: 0.379, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [1600/2070], Loss: 0.263, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [1700/2070], Loss: 0.335, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [1800/2070], Loss: 0.255, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [1900/2070], Loss: 0.367, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [2000/2070], Loss: 0.391, Training Accuracy: 88.59%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 70.02598614072495 %\n",
      "Epoch [10/60], Step [100/2070], Loss: 0.416, Training Accuracy: 81.52%\n",
      "Epoch [10/60], Step [200/2070], Loss: 0.416, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [300/2070], Loss: 0.427, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [400/2070], Loss: 0.347, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [500/2070], Loss: 0.272, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [600/2070], Loss: 0.295, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [700/2070], Loss: 0.380, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [800/2070], Loss: 0.347, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [900/2070], Loss: 0.320, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1000/2070], Loss: 0.248, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1100/2070], Loss: 0.314, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [1200/2070], Loss: 0.360, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1300/2070], Loss: 0.361, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [1400/2070], Loss: 0.348, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1500/2070], Loss: 0.362, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [1600/2070], Loss: 0.535, Training Accuracy: 81.52%\n",
      "Epoch [10/60], Step [1700/2070], Loss: 0.231, Training Accuracy: 94.02%\n",
      "Epoch [10/60], Step [1800/2070], Loss: 0.316, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1900/2070], Loss: 0.243, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [2000/2070], Loss: 0.362, Training Accuracy: 86.96%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 70.14092484008529 %\n",
      "Epoch [11/60], Step [100/2070], Loss: 0.313, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [200/2070], Loss: 0.282, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [300/2070], Loss: 0.253, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [400/2070], Loss: 0.268, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [500/2070], Loss: 0.310, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [600/2070], Loss: 0.293, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [700/2070], Loss: 0.370, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [800/2070], Loss: 0.416, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [900/2070], Loss: 0.322, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1000/2070], Loss: 0.378, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [1100/2070], Loss: 0.305, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [1200/2070], Loss: 0.332, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1300/2070], Loss: 0.319, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1400/2070], Loss: 0.279, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1500/2070], Loss: 0.255, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [1600/2070], Loss: 0.235, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [1700/2070], Loss: 0.240, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [1800/2070], Loss: 0.309, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [1900/2070], Loss: 0.315, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [2000/2070], Loss: 0.345, Training Accuracy: 88.59%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 69.38910358919688 %\n",
      "Epoch [12/60], Step [100/2070], Loss: 0.271, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [200/2070], Loss: 0.247, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [300/2070], Loss: 0.325, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [400/2070], Loss: 0.289, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [500/2070], Loss: 0.269, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [600/2070], Loss: 0.273, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [700/2070], Loss: 0.289, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [800/2070], Loss: 0.276, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [900/2070], Loss: 0.233, Training Accuracy: 93.48%\n",
      "Epoch [12/60], Step [1000/2070], Loss: 0.284, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [1100/2070], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [1200/2070], Loss: 0.273, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [1300/2070], Loss: 0.265, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [1400/2070], Loss: 0.353, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [1500/2070], Loss: 0.283, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1600/2070], Loss: 0.279, Training Accuracy: 91.30%\n",
      "Epoch [12/60], Step [1700/2070], Loss: 0.335, Training Accuracy: 87.50%\n",
      "Epoch [12/60], Step [1800/2070], Loss: 0.250, Training Accuracy: 92.39%\n",
      "Epoch [12/60], Step [1900/2070], Loss: 0.320, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [2000/2070], Loss: 0.186, Training Accuracy: 94.02%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 68.99875621890547 %\n",
      "Epoch [13/60], Step [100/2070], Loss: 0.276, Training Accuracy: 91.30%\n",
      "Epoch [13/60], Step [200/2070], Loss: 0.312, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [300/2070], Loss: 0.299, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [400/2070], Loss: 0.255, Training Accuracy: 89.67%\n",
      "Epoch [13/60], Step [500/2070], Loss: 0.237, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [600/2070], Loss: 0.253, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [700/2070], Loss: 0.296, Training Accuracy: 87.50%\n",
      "Epoch [13/60], Step [800/2070], Loss: 0.245, Training Accuracy: 91.85%\n",
      "Epoch [13/60], Step [900/2070], Loss: 0.325, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [1000/2070], Loss: 0.343, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [1100/2070], Loss: 0.395, Training Accuracy: 84.78%\n",
      "Epoch [13/60], Step [1200/2070], Loss: 0.341, Training Accuracy: 85.87%\n",
      "Epoch [13/60], Step [1300/2070], Loss: 0.336, Training Accuracy: 88.04%\n",
      "Epoch [13/60], Step [1400/2070], Loss: 0.277, Training Accuracy: 89.13%\n",
      "Epoch [13/60], Step [1500/2070], Loss: 0.248, Training Accuracy: 91.30%\n",
      "Epoch [13/60], Step [1600/2070], Loss: 0.330, Training Accuracy: 86.96%\n",
      "Epoch [13/60], Step [1700/2070], Loss: 0.259, Training Accuracy: 90.22%\n",
      "Epoch [13/60], Step [1800/2070], Loss: 0.263, Training Accuracy: 90.76%\n",
      "Epoch [13/60], Step [1900/2070], Loss: 0.188, Training Accuracy: 92.39%\n",
      "Epoch [13/60], Step [2000/2070], Loss: 0.238, Training Accuracy: 91.30%\n",
      "Testing epoch 13\n",
      "Epoch 13: Test Accuracy 68.64061389481165 %\n",
      "Epoch [14/60], Step [100/2070], Loss: 0.262, Training Accuracy: 89.13%\n",
      "Epoch [14/60], Step [200/2070], Loss: 0.227, Training Accuracy: 92.39%\n",
      "Epoch [14/60], Step [300/2070], Loss: 0.253, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [400/2070], Loss: 0.441, Training Accuracy: 84.24%\n",
      "Epoch [14/60], Step [500/2070], Loss: 0.313, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [600/2070], Loss: 0.222, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [700/2070], Loss: 0.260, Training Accuracy: 90.22%\n",
      "Epoch [14/60], Step [800/2070], Loss: 0.320, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [900/2070], Loss: 0.332, Training Accuracy: 84.78%\n",
      "Epoch [14/60], Step [1000/2070], Loss: 0.224, Training Accuracy: 88.59%\n",
      "Epoch [14/60], Step [1100/2070], Loss: 0.231, Training Accuracy: 90.76%\n",
      "Epoch [14/60], Step [1200/2070], Loss: 0.329, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [1300/2070], Loss: 0.287, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [1400/2070], Loss: 0.332, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [1500/2070], Loss: 0.313, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [1600/2070], Loss: 0.267, Training Accuracy: 91.85%\n",
      "Epoch [14/60], Step [1700/2070], Loss: 0.295, Training Accuracy: 89.67%\n",
      "Epoch [14/60], Step [1800/2070], Loss: 0.386, Training Accuracy: 86.41%\n",
      "Epoch [14/60], Step [1900/2070], Loss: 0.300, Training Accuracy: 87.50%\n",
      "Epoch [14/60], Step [2000/2070], Loss: 0.259, Training Accuracy: 89.67%\n",
      "Testing epoch 14\n",
      "Epoch 14: Test Accuracy 68.34965796019901 %\n",
      "Epoch [15/60], Step [100/2070], Loss: 0.346, Training Accuracy: 86.96%\n",
      "Epoch [15/60], Step [200/2070], Loss: 0.268, Training Accuracy: 89.13%\n",
      "Epoch [15/60], Step [300/2070], Loss: 0.194, Training Accuracy: 93.48%\n",
      "Epoch [15/60], Step [400/2070], Loss: 0.278, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [500/2070], Loss: 0.222, Training Accuracy: 89.67%\n",
      "Epoch [15/60], Step [600/2070], Loss: 0.246, Training Accuracy: 92.93%\n",
      "Epoch [15/60], Step [700/2070], Loss: 0.244, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [800/2070], Loss: 0.203, Training Accuracy: 91.85%\n",
      "Epoch [15/60], Step [900/2070], Loss: 0.302, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [1000/2070], Loss: 0.272, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [1100/2070], Loss: 0.222, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [1200/2070], Loss: 0.304, Training Accuracy: 88.04%\n",
      "Epoch [15/60], Step [1300/2070], Loss: 0.197, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [1400/2070], Loss: 0.301, Training Accuracy: 88.04%\n",
      "Epoch [15/60], Step [1500/2070], Loss: 0.227, Training Accuracy: 91.85%\n",
      "Epoch [15/60], Step [1600/2070], Loss: 0.248, Training Accuracy: 90.76%\n",
      "Epoch [15/60], Step [1700/2070], Loss: 0.230, Training Accuracy: 92.39%\n",
      "Epoch [15/60], Step [1800/2070], Loss: 0.291, Training Accuracy: 88.59%\n",
      "Epoch [15/60], Step [1900/2070], Loss: 0.278, Training Accuracy: 90.22%\n",
      "Epoch [15/60], Step [2000/2070], Loss: 0.244, Training Accuracy: 92.93%\n",
      "Testing epoch 15\n",
      "Epoch 15: Test Accuracy 70.05763592750533 %\n",
      "Finished early after 15 epochs!\n",
      "Final Training Accuracy 90.28909447184968 %\n",
      "Final Test Accuracy 70.09539356787491 %\n",
      "Epoch [1/60], Step [100/1934], Loss: 0.846, Training Accuracy: 68.48%\n",
      "Epoch [1/60], Step [200/1934], Loss: 0.776, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [300/1934], Loss: 0.678, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [400/1934], Loss: 0.524, Training Accuracy: 82.07%\n",
      "Epoch [1/60], Step [500/1934], Loss: 0.642, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [600/1934], Loss: 0.615, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [700/1934], Loss: 0.530, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [800/1934], Loss: 0.479, Training Accuracy: 85.33%\n",
      "Epoch [1/60], Step [900/1934], Loss: 0.434, Training Accuracy: 81.52%\n",
      "Epoch [1/60], Step [1000/1934], Loss: 0.495, Training Accuracy: 85.33%\n",
      "Epoch [1/60], Step [1100/1934], Loss: 0.579, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [1200/1934], Loss: 0.530, Training Accuracy: 80.98%\n",
      "Epoch [1/60], Step [1300/1934], Loss: 0.451, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1400/1934], Loss: 0.423, Training Accuracy: 83.15%\n",
      "Epoch [1/60], Step [1500/1934], Loss: 0.501, Training Accuracy: 82.07%\n",
      "Epoch [1/60], Step [1600/1934], Loss: 0.485, Training Accuracy: 85.33%\n",
      "Epoch [1/60], Step [1700/1934], Loss: 0.366, Training Accuracy: 85.87%\n",
      "Epoch [1/60], Step [1800/1934], Loss: 0.380, Training Accuracy: 84.24%\n",
      "Epoch [1/60], Step [1900/1934], Loss: 0.461, Training Accuracy: 82.07%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 52.003764678344325 %\n",
      "Epoch [2/60], Step [100/1934], Loss: 0.383, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [200/1934], Loss: 0.477, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [300/1934], Loss: 0.409, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [400/1934], Loss: 0.433, Training Accuracy: 88.59%\n",
      "Epoch [2/60], Step [500/1934], Loss: 0.359, Training Accuracy: 88.59%\n",
      "Epoch [2/60], Step [600/1934], Loss: 0.519, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [700/1934], Loss: 0.360, Training Accuracy: 86.96%\n",
      "Epoch [2/60], Step [800/1934], Loss: 0.488, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [900/1934], Loss: 0.415, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [1000/1934], Loss: 0.273, Training Accuracy: 90.22%\n",
      "Epoch [2/60], Step [1100/1934], Loss: 0.394, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [1200/1934], Loss: 0.345, Training Accuracy: 88.59%\n",
      "Epoch [2/60], Step [1300/1934], Loss: 0.289, Training Accuracy: 90.76%\n",
      "Epoch [2/60], Step [1400/1934], Loss: 0.357, Training Accuracy: 88.04%\n",
      "Epoch [2/60], Step [1500/1934], Loss: 0.376, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [1600/1934], Loss: 0.404, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [1700/1934], Loss: 0.337, Training Accuracy: 88.59%\n",
      "Epoch [2/60], Step [1800/1934], Loss: 0.350, Training Accuracy: 89.67%\n",
      "Epoch [2/60], Step [1900/1934], Loss: 0.308, Training Accuracy: 90.22%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 51.20401435649359 %\n",
      "Epoch [3/60], Step [100/1934], Loss: 0.310, Training Accuracy: 89.67%\n",
      "Epoch [3/60], Step [200/1934], Loss: 0.264, Training Accuracy: 89.67%\n",
      "Epoch [3/60], Step [300/1934], Loss: 0.351, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [400/1934], Loss: 0.377, Training Accuracy: 89.13%\n",
      "Epoch [3/60], Step [500/1934], Loss: 0.302, Training Accuracy: 90.22%\n",
      "Epoch [3/60], Step [600/1934], Loss: 0.275, Training Accuracy: 89.67%\n",
      "Epoch [3/60], Step [700/1934], Loss: 0.350, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [800/1934], Loss: 0.488, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [900/1934], Loss: 0.382, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1000/1934], Loss: 0.354, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [1100/1934], Loss: 0.311, Training Accuracy: 89.13%\n",
      "Epoch [3/60], Step [1200/1934], Loss: 0.371, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1300/1934], Loss: 0.294, Training Accuracy: 90.76%\n",
      "Epoch [3/60], Step [1400/1934], Loss: 0.321, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1500/1934], Loss: 0.327, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1600/1934], Loss: 0.414, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [1700/1934], Loss: 0.299, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [1800/1934], Loss: 0.280, Training Accuracy: 89.13%\n",
      "Epoch [3/60], Step [1900/1934], Loss: 0.372, Training Accuracy: 89.13%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 53.483302773768195 %\n",
      "Epoch [4/60], Step [100/1934], Loss: 0.393, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [200/1934], Loss: 0.238, Training Accuracy: 92.39%\n",
      "Epoch [4/60], Step [300/1934], Loss: 0.236, Training Accuracy: 92.93%\n",
      "Epoch [4/60], Step [400/1934], Loss: 0.301, Training Accuracy: 90.22%\n",
      "Epoch [4/60], Step [500/1934], Loss: 0.246, Training Accuracy: 94.02%\n",
      "Epoch [4/60], Step [600/1934], Loss: 0.324, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [700/1934], Loss: 0.279, Training Accuracy: 90.76%\n",
      "Epoch [4/60], Step [800/1934], Loss: 0.347, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [900/1934], Loss: 0.268, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1000/1934], Loss: 0.240, Training Accuracy: 90.76%\n",
      "Epoch [4/60], Step [1100/1934], Loss: 0.288, Training Accuracy: 90.76%\n",
      "Epoch [4/60], Step [1200/1934], Loss: 0.322, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1300/1934], Loss: 0.318, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1400/1934], Loss: 0.270, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1500/1934], Loss: 0.241, Training Accuracy: 93.48%\n",
      "Epoch [4/60], Step [1600/1934], Loss: 0.356, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [1700/1934], Loss: 0.329, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1800/1934], Loss: 0.274, Training Accuracy: 91.85%\n",
      "Epoch [4/60], Step [1900/1934], Loss: 0.349, Training Accuracy: 85.87%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 52.80741622127726 %\n",
      "Epoch [5/60], Step [100/1934], Loss: 0.243, Training Accuracy: 91.30%\n",
      "Epoch [5/60], Step [200/1934], Loss: 0.268, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [300/1934], Loss: 0.305, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [400/1934], Loss: 0.227, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [500/1934], Loss: 0.210, Training Accuracy: 93.48%\n",
      "Epoch [5/60], Step [600/1934], Loss: 0.313, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [700/1934], Loss: 0.271, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [800/1934], Loss: 0.239, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [900/1934], Loss: 0.213, Training Accuracy: 92.93%\n",
      "Epoch [5/60], Step [1000/1934], Loss: 0.284, Training Accuracy: 92.39%\n",
      "Epoch [5/60], Step [1100/1934], Loss: 0.299, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1200/1934], Loss: 0.313, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1300/1934], Loss: 0.311, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1400/1934], Loss: 0.347, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1500/1934], Loss: 0.239, Training Accuracy: 91.30%\n",
      "Epoch [5/60], Step [1600/1934], Loss: 0.264, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [1700/1934], Loss: 0.303, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1800/1934], Loss: 0.311, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1900/1934], Loss: 0.245, Training Accuracy: 89.67%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 52.626497093590295 %\n",
      "Epoch [6/60], Step [100/1934], Loss: 0.210, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [200/1934], Loss: 0.285, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [300/1934], Loss: 0.258, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [400/1934], Loss: 0.215, Training Accuracy: 93.48%\n",
      "Epoch [6/60], Step [500/1934], Loss: 0.228, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [600/1934], Loss: 0.334, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [700/1934], Loss: 0.419, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [800/1934], Loss: 0.244, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [900/1934], Loss: 0.289, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [1000/1934], Loss: 0.247, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [1100/1934], Loss: 0.249, Training Accuracy: 91.30%\n",
      "Epoch [6/60], Step [1200/1934], Loss: 0.297, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [1300/1934], Loss: 0.255, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [1400/1934], Loss: 0.239, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [1500/1934], Loss: 0.256, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1600/1934], Loss: 0.291, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1700/1934], Loss: 0.296, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1800/1934], Loss: 0.197, Training Accuracy: 94.02%\n",
      "Epoch [6/60], Step [1900/1934], Loss: 0.239, Training Accuracy: 91.30%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 52.23101080638239 %\n",
      "Epoch [7/60], Step [100/1934], Loss: 0.325, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [200/1934], Loss: 0.148, Training Accuracy: 95.11%\n",
      "Epoch [7/60], Step [300/1934], Loss: 0.218, Training Accuracy: 93.48%\n",
      "Epoch [7/60], Step [400/1934], Loss: 0.260, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [500/1934], Loss: 0.207, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [600/1934], Loss: 0.218, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [700/1934], Loss: 0.269, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [800/1934], Loss: 0.271, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [900/1934], Loss: 0.351, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [1000/1934], Loss: 0.105, Training Accuracy: 97.83%\n",
      "Epoch [7/60], Step [1100/1934], Loss: 0.322, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1200/1934], Loss: 0.205, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1300/1934], Loss: 0.252, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1400/1934], Loss: 0.259, Training Accuracy: 91.30%\n",
      "Epoch [7/60], Step [1500/1934], Loss: 0.244, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1600/1934], Loss: 0.203, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [1700/1934], Loss: 0.327, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1800/1934], Loss: 0.217, Training Accuracy: 95.11%\n",
      "Epoch [7/60], Step [1900/1934], Loss: 0.162, Training Accuracy: 95.11%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 51.841864003433066 %\n",
      "Epoch [8/60], Step [100/1934], Loss: 0.301, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [200/1934], Loss: 0.243, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [300/1934], Loss: 0.221, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [400/1934], Loss: 0.227, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [500/1934], Loss: 0.194, Training Accuracy: 93.48%\n",
      "Epoch [8/60], Step [600/1934], Loss: 0.198, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [700/1934], Loss: 0.249, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [800/1934], Loss: 0.229, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [900/1934], Loss: 0.176, Training Accuracy: 94.02%\n",
      "Epoch [8/60], Step [1000/1934], Loss: 0.236, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1100/1934], Loss: 0.273, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1200/1934], Loss: 0.130, Training Accuracy: 95.65%\n",
      "Epoch [8/60], Step [1300/1934], Loss: 0.225, Training Accuracy: 93.48%\n",
      "Epoch [8/60], Step [1400/1934], Loss: 0.198, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [1500/1934], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1600/1934], Loss: 0.231, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [1700/1934], Loss: 0.233, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [1800/1934], Loss: 0.278, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1900/1934], Loss: 0.295, Training Accuracy: 88.59%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 52.10568407911676 %\n",
      "Finished early after 8 epochs!\n",
      "Final Training Accuracy 91.79030634905465 %\n",
      "Final Test Accuracy 52.126165489798304 %\n",
      "Epoch [1/60], Step [100/2087], Loss: 1.042, Training Accuracy: 58.70%\n",
      "Epoch [1/60], Step [200/2087], Loss: 0.876, Training Accuracy: 63.04%\n",
      "Epoch [1/60], Step [300/2087], Loss: 0.835, Training Accuracy: 63.04%\n",
      "Epoch [1/60], Step [400/2087], Loss: 0.767, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [500/2087], Loss: 0.790, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [600/2087], Loss: 0.733, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [700/2087], Loss: 0.769, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [800/2087], Loss: 0.693, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [900/2087], Loss: 0.751, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [1000/2087], Loss: 0.690, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [1100/2087], Loss: 0.688, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [1200/2087], Loss: 0.661, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [1300/2087], Loss: 0.725, Training Accuracy: 69.57%\n",
      "Epoch [1/60], Step [1400/2087], Loss: 0.771, Training Accuracy: 64.67%\n",
      "Epoch [1/60], Step [1500/2087], Loss: 0.622, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [1600/2087], Loss: 0.620, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [1700/2087], Loss: 0.542, Training Accuracy: 78.80%\n",
      "Epoch [1/60], Step [1800/2087], Loss: 0.586, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1900/2087], Loss: 0.639, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [2000/2087], Loss: 0.661, Training Accuracy: 71.20%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 60.242414648900144 %\n",
      "Epoch [2/60], Step [100/2087], Loss: 0.557, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [200/2087], Loss: 0.597, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/2087], Loss: 0.551, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [400/2087], Loss: 0.655, Training Accuracy: 74.46%\n",
      "Epoch [2/60], Step [500/2087], Loss: 0.607, Training Accuracy: 73.37%\n",
      "Epoch [2/60], Step [600/2087], Loss: 0.562, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [700/2087], Loss: 0.546, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [800/2087], Loss: 0.548, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [900/2087], Loss: 0.553, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [1000/2087], Loss: 0.510, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1100/2087], Loss: 0.593, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [1200/2087], Loss: 0.525, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1300/2087], Loss: 0.528, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [1400/2087], Loss: 0.506, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1500/2087], Loss: 0.517, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1600/2087], Loss: 0.525, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [1700/2087], Loss: 0.532, Training Accuracy: 80.43%\n",
      "Epoch [2/60], Step [1800/2087], Loss: 0.517, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [1900/2087], Loss: 0.433, Training Accuracy: 83.15%\n",
      "Epoch [2/60], Step [2000/2087], Loss: 0.481, Training Accuracy: 82.07%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 58.536805983480974 %\n",
      "Epoch [3/60], Step [100/2087], Loss: 0.513, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [200/2087], Loss: 0.452, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [300/2087], Loss: 0.500, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [400/2087], Loss: 0.519, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [500/2087], Loss: 0.466, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [600/2087], Loss: 0.491, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [700/2087], Loss: 0.467, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [800/2087], Loss: 0.547, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [900/2087], Loss: 0.505, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [1000/2087], Loss: 0.501, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1100/2087], Loss: 0.457, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1200/2087], Loss: 0.405, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [1300/2087], Loss: 0.522, Training Accuracy: 80.98%\n",
      "Epoch [3/60], Step [1400/2087], Loss: 0.472, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [1500/2087], Loss: 0.378, Training Accuracy: 85.87%\n",
      "Epoch [3/60], Step [1600/2087], Loss: 0.480, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1700/2087], Loss: 0.529, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1800/2087], Loss: 0.374, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [1900/2087], Loss: 0.532, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [2000/2087], Loss: 0.493, Training Accuracy: 81.52%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 59.91452170029454 %\n",
      "Epoch [4/60], Step [100/2087], Loss: 0.435, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [200/2087], Loss: 0.452, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [300/2087], Loss: 0.461, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [400/2087], Loss: 0.327, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [500/2087], Loss: 0.415, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [600/2087], Loss: 0.414, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [700/2087], Loss: 0.418, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [800/2087], Loss: 0.410, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [900/2087], Loss: 0.399, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1000/2087], Loss: 0.541, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [1100/2087], Loss: 0.478, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1200/2087], Loss: 0.436, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [1300/2087], Loss: 0.405, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [1400/2087], Loss: 0.458, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [1500/2087], Loss: 0.462, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1600/2087], Loss: 0.436, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1700/2087], Loss: 0.484, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [1800/2087], Loss: 0.412, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [1900/2087], Loss: 0.412, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [2000/2087], Loss: 0.443, Training Accuracy: 83.15%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 61.147512253585624 %\n",
      "Epoch [5/60], Step [100/2087], Loss: 0.367, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [200/2087], Loss: 0.427, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [300/2087], Loss: 0.421, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [400/2087], Loss: 0.442, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [500/2087], Loss: 0.338, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [600/2087], Loss: 0.438, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [700/2087], Loss: 0.505, Training Accuracy: 80.98%\n",
      "Epoch [5/60], Step [800/2087], Loss: 0.474, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [900/2087], Loss: 0.449, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1000/2087], Loss: 0.429, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [1100/2087], Loss: 0.350, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1200/2087], Loss: 0.429, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [1300/2087], Loss: 0.431, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [1400/2087], Loss: 0.419, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1500/2087], Loss: 0.373, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [1600/2087], Loss: 0.338, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1700/2087], Loss: 0.351, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1800/2087], Loss: 0.368, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1900/2087], Loss: 0.450, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [2000/2087], Loss: 0.364, Training Accuracy: 84.78%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 59.74661789730167 %\n",
      "Epoch [6/60], Step [100/2087], Loss: 0.421, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [200/2087], Loss: 0.357, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [300/2087], Loss: 0.464, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [400/2087], Loss: 0.314, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [500/2087], Loss: 0.422, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [600/2087], Loss: 0.445, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [700/2087], Loss: 0.465, Training Accuracy: 79.89%\n",
      "Epoch [6/60], Step [800/2087], Loss: 0.422, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [900/2087], Loss: 0.423, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1000/2087], Loss: 0.375, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1100/2087], Loss: 0.455, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [1200/2087], Loss: 0.477, Training Accuracy: 79.35%\n",
      "Epoch [6/60], Step [1300/2087], Loss: 0.315, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [1400/2087], Loss: 0.438, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1500/2087], Loss: 0.351, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [1600/2087], Loss: 0.351, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1700/2087], Loss: 0.340, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1800/2087], Loss: 0.420, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [1900/2087], Loss: 0.367, Training Accuracy: 90.22%\n",
      "Epoch [6/60], Step [2000/2087], Loss: 0.423, Training Accuracy: 83.15%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 58.10206516024354 %\n",
      "Epoch [7/60], Step [100/2087], Loss: 0.386, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [200/2087], Loss: 0.336, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [300/2087], Loss: 0.445, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [400/2087], Loss: 0.377, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [500/2087], Loss: 0.331, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [600/2087], Loss: 0.355, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [700/2087], Loss: 0.429, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [800/2087], Loss: 0.468, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [900/2087], Loss: 0.306, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1000/2087], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1100/2087], Loss: 0.393, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [1200/2087], Loss: 0.299, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1300/2087], Loss: 0.348, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [1400/2087], Loss: 0.409, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [1500/2087], Loss: 0.396, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [1600/2087], Loss: 0.403, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1700/2087], Loss: 0.372, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [1800/2087], Loss: 0.466, Training Accuracy: 80.43%\n",
      "Epoch [7/60], Step [1900/2087], Loss: 0.327, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [2000/2087], Loss: 0.434, Training Accuracy: 85.33%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 61.20856818194667 %\n",
      "Epoch [8/60], Step [100/2087], Loss: 0.361, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [200/2087], Loss: 0.361, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [300/2087], Loss: 0.322, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [400/2087], Loss: 0.268, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [500/2087], Loss: 0.354, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [600/2087], Loss: 0.350, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [700/2087], Loss: 0.376, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [800/2087], Loss: 0.385, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [900/2087], Loss: 0.395, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [1000/2087], Loss: 0.346, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [1100/2087], Loss: 0.345, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1200/2087], Loss: 0.379, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [1300/2087], Loss: 0.241, Training Accuracy: 90.22%\n",
      "Epoch [8/60], Step [1400/2087], Loss: 0.320, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [1500/2087], Loss: 0.314, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1600/2087], Loss: 0.369, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [1700/2087], Loss: 0.411, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [1800/2087], Loss: 0.288, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1900/2087], Loss: 0.384, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [2000/2087], Loss: 0.336, Training Accuracy: 86.96%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 58.23831033371587 %\n",
      "Epoch [9/60], Step [100/2087], Loss: 0.335, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [200/2087], Loss: 0.275, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [300/2087], Loss: 0.420, Training Accuracy: 84.78%\n",
      "Epoch [9/60], Step [400/2087], Loss: 0.330, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [500/2087], Loss: 0.305, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [600/2087], Loss: 0.283, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [700/2087], Loss: 0.386, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [800/2087], Loss: 0.325, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [900/2087], Loss: 0.385, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [1000/2087], Loss: 0.421, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [1100/2087], Loss: 0.289, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1200/2087], Loss: 0.419, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [1300/2087], Loss: 0.433, Training Accuracy: 80.43%\n",
      "Epoch [9/60], Step [1400/2087], Loss: 0.323, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [1500/2087], Loss: 0.304, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [1600/2087], Loss: 0.300, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1700/2087], Loss: 0.346, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [1800/2087], Loss: 0.247, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [1900/2087], Loss: 0.310, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [2000/2087], Loss: 0.312, Training Accuracy: 88.59%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 58.03818256853245 %\n",
      "Epoch [10/60], Step [100/2087], Loss: 0.347, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [200/2087], Loss: 0.342, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [300/2087], Loss: 0.371, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [400/2087], Loss: 0.345, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [500/2087], Loss: 0.288, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [600/2087], Loss: 0.265, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [700/2087], Loss: 0.387, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [800/2087], Loss: 0.408, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [900/2087], Loss: 0.274, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [1000/2087], Loss: 0.337, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [1100/2087], Loss: 0.446, Training Accuracy: 82.07%\n",
      "Epoch [10/60], Step [1200/2087], Loss: 0.308, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1300/2087], Loss: 0.306, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1400/2087], Loss: 0.429, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [1500/2087], Loss: 0.313, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [1600/2087], Loss: 0.358, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1700/2087], Loss: 0.277, Training Accuracy: 89.13%\n",
      "Epoch [10/60], Step [1800/2087], Loss: 0.299, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [1900/2087], Loss: 0.390, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [2000/2087], Loss: 0.433, Training Accuracy: 84.24%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 58.50514735396044 %\n",
      "Epoch [11/60], Step [100/2087], Loss: 0.302, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [200/2087], Loss: 0.318, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [300/2087], Loss: 0.300, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [400/2087], Loss: 0.342, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [500/2087], Loss: 0.304, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [600/2087], Loss: 0.307, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [700/2087], Loss: 0.350, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [800/2087], Loss: 0.281, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [900/2087], Loss: 0.273, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1000/2087], Loss: 0.337, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [1100/2087], Loss: 0.332, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1200/2087], Loss: 0.276, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1300/2087], Loss: 0.259, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [1400/2087], Loss: 0.495, Training Accuracy: 80.98%\n",
      "Epoch [11/60], Step [1500/2087], Loss: 0.294, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1600/2087], Loss: 0.301, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1700/2087], Loss: 0.276, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1800/2087], Loss: 0.398, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [1900/2087], Loss: 0.264, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [2000/2087], Loss: 0.241, Training Accuracy: 90.76%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 57.672977663706206 %\n",
      "Epoch [12/60], Step [100/2087], Loss: 0.383, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [200/2087], Loss: 0.307, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [300/2087], Loss: 0.270, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [400/2087], Loss: 0.299, Training Accuracy: 90.76%\n",
      "Epoch [12/60], Step [500/2087], Loss: 0.343, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [600/2087], Loss: 0.378, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [700/2087], Loss: 0.349, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [800/2087], Loss: 0.324, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [900/2087], Loss: 0.342, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [1000/2087], Loss: 0.303, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [1100/2087], Loss: 0.333, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1200/2087], Loss: 0.374, Training Accuracy: 84.78%\n",
      "Epoch [12/60], Step [1300/2087], Loss: 0.293, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [1400/2087], Loss: 0.416, Training Accuracy: 86.96%\n",
      "Epoch [12/60], Step [1500/2087], Loss: 0.356, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [1600/2087], Loss: 0.303, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [1700/2087], Loss: 0.364, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [1800/2087], Loss: 0.320, Training Accuracy: 85.33%\n",
      "Epoch [12/60], Step [1900/2087], Loss: 0.310, Training Accuracy: 89.13%\n",
      "Epoch [12/60], Step [2000/2087], Loss: 0.300, Training Accuracy: 89.67%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 56.23533668387163 %\n",
      "Finished early after 12 epochs!\n",
      "Final Training Accuracy 88.28256254639699 %\n",
      "Final Test Accuracy 56.21272337707124 %\n",
      "Epoch [1/60], Step [100/2076], Loss: 1.149, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [200/2076], Loss: 1.056, Training Accuracy: 56.52%\n",
      "Epoch [1/60], Step [300/2076], Loss: 0.766, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [400/2076], Loss: 1.022, Training Accuracy: 59.24%\n",
      "Epoch [1/60], Step [500/2076], Loss: 0.744, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [600/2076], Loss: 0.730, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [700/2076], Loss: 0.757, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [800/2076], Loss: 0.803, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [900/2076], Loss: 0.715, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [1000/2076], Loss: 0.619, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1100/2076], Loss: 0.680, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [1200/2076], Loss: 0.656, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [1300/2076], Loss: 0.673, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1400/2076], Loss: 0.587, Training Accuracy: 80.43%\n",
      "Epoch [1/60], Step [1500/2076], Loss: 0.645, Training Accuracy: 73.91%\n",
      "Epoch [1/60], Step [1600/2076], Loss: 0.619, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [1700/2076], Loss: 0.737, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [1800/2076], Loss: 0.680, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [1900/2076], Loss: 0.561, Training Accuracy: 79.89%\n",
      "Epoch [1/60], Step [2000/2076], Loss: 0.731, Training Accuracy: 72.28%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 67.45670977517598 %\n",
      "Epoch [2/60], Step [100/2076], Loss: 0.532, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [200/2076], Loss: 0.540, Training Accuracy: 80.98%\n",
      "Epoch [2/60], Step [300/2076], Loss: 0.571, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [400/2076], Loss: 0.590, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [500/2076], Loss: 0.577, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [600/2076], Loss: 0.655, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [700/2076], Loss: 0.671, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [800/2076], Loss: 0.450, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [900/2076], Loss: 0.624, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1000/2076], Loss: 0.509, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [1100/2076], Loss: 0.523, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1200/2076], Loss: 0.542, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1300/2076], Loss: 0.542, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [1400/2076], Loss: 0.554, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1500/2076], Loss: 0.507, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1600/2076], Loss: 0.508, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1700/2076], Loss: 0.572, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [1800/2076], Loss: 0.552, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [1900/2076], Loss: 0.511, Training Accuracy: 81.52%\n",
      "Epoch [2/60], Step [2000/2076], Loss: 0.518, Training Accuracy: 80.98%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 69.16371728421983 %\n",
      "Epoch [3/60], Step [100/2076], Loss: 0.567, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [200/2076], Loss: 0.562, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [300/2076], Loss: 0.552, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [400/2076], Loss: 0.496, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [500/2076], Loss: 0.483, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [600/2076], Loss: 0.523, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [700/2076], Loss: 0.463, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [800/2076], Loss: 0.522, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [900/2076], Loss: 0.492, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [1000/2076], Loss: 0.469, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1100/2076], Loss: 0.493, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [1200/2076], Loss: 0.469, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1300/2076], Loss: 0.454, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1400/2076], Loss: 0.475, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [1500/2076], Loss: 0.490, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [1600/2076], Loss: 0.606, Training Accuracy: 77.17%\n",
      "Epoch [3/60], Step [1700/2076], Loss: 0.456, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [1800/2076], Loss: 0.406, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1900/2076], Loss: 0.626, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [2000/2076], Loss: 0.472, Training Accuracy: 83.70%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 69.64680096840387 %\n",
      "Epoch [4/60], Step [100/2076], Loss: 0.463, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [200/2076], Loss: 0.444, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [300/2076], Loss: 0.394, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [400/2076], Loss: 0.427, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [500/2076], Loss: 0.419, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [600/2076], Loss: 0.550, Training Accuracy: 81.52%\n",
      "Epoch [4/60], Step [700/2076], Loss: 0.466, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [800/2076], Loss: 0.405, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [900/2076], Loss: 0.504, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1000/2076], Loss: 0.405, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1100/2076], Loss: 0.498, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [1200/2076], Loss: 0.475, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1300/2076], Loss: 0.405, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1400/2076], Loss: 0.348, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1500/2076], Loss: 0.526, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [1600/2076], Loss: 0.312, Training Accuracy: 91.30%\n",
      "Epoch [4/60], Step [1700/2076], Loss: 0.524, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1800/2076], Loss: 0.429, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [1900/2076], Loss: 0.501, Training Accuracy: 78.26%\n",
      "Epoch [4/60], Step [2000/2076], Loss: 0.531, Training Accuracy: 78.26%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 69.56293227323303 %\n",
      "Epoch [5/60], Step [100/2076], Loss: 0.434, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [200/2076], Loss: 0.411, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [300/2076], Loss: 0.319, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [400/2076], Loss: 0.534, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [500/2076], Loss: 0.444, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [600/2076], Loss: 0.410, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [700/2076], Loss: 0.488, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [800/2076], Loss: 0.359, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [900/2076], Loss: 0.438, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [1000/2076], Loss: 0.454, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1100/2076], Loss: 0.493, Training Accuracy: 78.80%\n",
      "Epoch [5/60], Step [1200/2076], Loss: 0.415, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [1300/2076], Loss: 0.465, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [1400/2076], Loss: 0.310, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [1500/2076], Loss: 0.542, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [1600/2076], Loss: 0.429, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1700/2076], Loss: 0.313, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1800/2076], Loss: 0.435, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1900/2076], Loss: 0.361, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [2000/2076], Loss: 0.417, Training Accuracy: 85.33%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 70.08124080938882 %\n",
      "Epoch [6/60], Step [100/2076], Loss: 0.400, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [200/2076], Loss: 0.482, Training Accuracy: 80.98%\n",
      "Epoch [6/60], Step [300/2076], Loss: 0.456, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [400/2076], Loss: 0.369, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [500/2076], Loss: 0.423, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/2076], Loss: 0.361, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [700/2076], Loss: 0.525, Training Accuracy: 79.35%\n",
      "Epoch [6/60], Step [800/2076], Loss: 0.400, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [900/2076], Loss: 0.453, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [1000/2076], Loss: 0.376, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1100/2076], Loss: 0.367, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1200/2076], Loss: 0.499, Training Accuracy: 82.61%\n",
      "Epoch [6/60], Step [1300/2076], Loss: 0.306, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [1400/2076], Loss: 0.298, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [1500/2076], Loss: 0.436, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1600/2076], Loss: 0.355, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1700/2076], Loss: 0.444, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1800/2076], Loss: 0.442, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [1900/2076], Loss: 0.454, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [2000/2076], Loss: 0.357, Training Accuracy: 85.87%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 69.6904126898927 %\n",
      "Epoch [7/60], Step [100/2076], Loss: 0.469, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [200/2076], Loss: 0.331, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [300/2076], Loss: 0.426, Training Accuracy: 82.07%\n",
      "Epoch [7/60], Step [400/2076], Loss: 0.333, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [500/2076], Loss: 0.442, Training Accuracy: 80.98%\n",
      "Epoch [7/60], Step [600/2076], Loss: 0.358, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [700/2076], Loss: 0.351, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [800/2076], Loss: 0.391, Training Accuracy: 85.33%\n",
      "Epoch [7/60], Step [900/2076], Loss: 0.326, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1000/2076], Loss: 0.392, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1100/2076], Loss: 0.329, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1200/2076], Loss: 0.283, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1300/2076], Loss: 0.291, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1400/2076], Loss: 0.308, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1500/2076], Loss: 0.390, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [1600/2076], Loss: 0.276, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1700/2076], Loss: 0.338, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [1800/2076], Loss: 0.423, Training Accuracy: 83.15%\n",
      "Epoch [7/60], Step [1900/2076], Loss: 0.356, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [2000/2076], Loss: 0.372, Training Accuracy: 87.50%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 70.46200468546444 %\n",
      "Epoch [8/60], Step [100/2076], Loss: 0.353, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [200/2076], Loss: 0.413, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [300/2076], Loss: 0.379, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [400/2076], Loss: 0.435, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [500/2076], Loss: 0.230, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [600/2076], Loss: 0.359, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [700/2076], Loss: 0.398, Training Accuracy: 82.07%\n",
      "Epoch [8/60], Step [800/2076], Loss: 0.419, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [900/2076], Loss: 0.252, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [1000/2076], Loss: 0.337, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [1100/2076], Loss: 0.369, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [1200/2076], Loss: 0.442, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [1300/2076], Loss: 0.325, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1400/2076], Loss: 0.314, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1500/2076], Loss: 0.361, Training Accuracy: 88.04%\n",
      "Epoch [8/60], Step [1600/2076], Loss: 0.334, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1700/2076], Loss: 0.332, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [1800/2076], Loss: 0.343, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [1900/2076], Loss: 0.296, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [2000/2076], Loss: 0.295, Training Accuracy: 88.04%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 69.86709607438594 %\n",
      "Epoch [9/60], Step [100/2076], Loss: 0.447, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [200/2076], Loss: 0.407, Training Accuracy: 83.15%\n",
      "Epoch [9/60], Step [300/2076], Loss: 0.339, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [400/2076], Loss: 0.314, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [500/2076], Loss: 0.311, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [600/2076], Loss: 0.391, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [700/2076], Loss: 0.288, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [800/2076], Loss: 0.342, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [900/2076], Loss: 0.362, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [1000/2076], Loss: 0.324, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1100/2076], Loss: 0.370, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [1200/2076], Loss: 0.431, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1300/2076], Loss: 0.331, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1400/2076], Loss: 0.273, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1500/2076], Loss: 0.295, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [1600/2076], Loss: 0.313, Training Accuracy: 86.96%\n",
      "Epoch [9/60], Step [1700/2076], Loss: 0.363, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [1800/2076], Loss: 0.289, Training Accuracy: 90.22%\n",
      "Epoch [9/60], Step [1900/2076], Loss: 0.297, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [2000/2076], Loss: 0.293, Training Accuracy: 88.59%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 70.02029622423134 %\n",
      "Epoch [10/60], Step [100/2076], Loss: 0.345, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [200/2076], Loss: 0.252, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [300/2076], Loss: 0.289, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [400/2076], Loss: 0.323, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [500/2076], Loss: 0.339, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [600/2076], Loss: 0.232, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [700/2076], Loss: 0.366, Training Accuracy: 85.33%\n",
      "Epoch [10/60], Step [800/2076], Loss: 0.261, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [900/2076], Loss: 0.428, Training Accuracy: 82.07%\n",
      "Epoch [10/60], Step [1000/2076], Loss: 0.322, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [1100/2076], Loss: 0.314, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [1200/2076], Loss: 0.353, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1300/2076], Loss: 0.294, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [1400/2076], Loss: 0.292, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [1500/2076], Loss: 0.315, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1600/2076], Loss: 0.286, Training Accuracy: 89.67%\n",
      "Epoch [10/60], Step [1700/2076], Loss: 0.439, Training Accuracy: 80.43%\n",
      "Epoch [10/60], Step [1800/2076], Loss: 0.394, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [1900/2076], Loss: 0.305, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [2000/2076], Loss: 0.284, Training Accuracy: 88.59%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 69.3717116482435 %\n",
      "Epoch [11/60], Step [100/2076], Loss: 0.369, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [200/2076], Loss: 0.261, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [300/2076], Loss: 0.352, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [400/2076], Loss: 0.338, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [500/2076], Loss: 0.286, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [600/2076], Loss: 0.352, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [700/2076], Loss: 0.311, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [800/2076], Loss: 0.259, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [900/2076], Loss: 0.329, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1000/2076], Loss: 0.261, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [1100/2076], Loss: 0.336, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1200/2076], Loss: 0.277, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [1300/2076], Loss: 0.306, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1400/2076], Loss: 0.325, Training Accuracy: 88.59%\n",
      "Epoch [11/60], Step [1500/2076], Loss: 0.273, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1600/2076], Loss: 0.326, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1700/2076], Loss: 0.375, Training Accuracy: 85.87%\n",
      "Epoch [11/60], Step [1800/2076], Loss: 0.316, Training Accuracy: 84.78%\n",
      "Epoch [11/60], Step [1900/2076], Loss: 0.303, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [2000/2076], Loss: 0.267, Training Accuracy: 92.39%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 69.42594673778733 %\n",
      "Epoch [12/60], Step [100/2076], Loss: 0.263, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [200/2076], Loss: 0.308, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [300/2076], Loss: 0.327, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [400/2076], Loss: 0.326, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [500/2076], Loss: 0.254, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [600/2076], Loss: 0.287, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [700/2076], Loss: 0.294, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [800/2076], Loss: 0.333, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [900/2076], Loss: 0.262, Training Accuracy: 89.67%\n",
      "Epoch [12/60], Step [1000/2076], Loss: 0.304, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [1100/2076], Loss: 0.318, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [1200/2076], Loss: 0.315, Training Accuracy: 88.04%\n",
      "Epoch [12/60], Step [1300/2076], Loss: 0.292, Training Accuracy: 88.59%\n",
      "Epoch [12/60], Step [1400/2076], Loss: 0.246, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [1500/2076], Loss: 0.392, Training Accuracy: 86.41%\n",
      "Epoch [12/60], Step [1600/2076], Loss: 0.270, Training Accuracy: 90.22%\n",
      "Epoch [12/60], Step [1700/2076], Loss: 0.329, Training Accuracy: 85.87%\n",
      "Epoch [12/60], Step [1800/2076], Loss: 0.387, Training Accuracy: 84.24%\n",
      "Epoch [12/60], Step [1900/2076], Loss: 0.258, Training Accuracy: 91.85%\n",
      "Epoch [12/60], Step [2000/2076], Loss: 0.263, Training Accuracy: 91.30%\n",
      "Testing epoch 12\n",
      "Epoch 12: Test Accuracy 69.83746246875891 %\n",
      "Finished early after 12 epochs!\n",
      "Final Training Accuracy 88.9404661854531 %\n",
      "Final Test Accuracy 69.78658212702193 %\n",
      "Epoch [1/60], Step [100/2039], Loss: 1.160, Training Accuracy: 55.43%\n",
      "Epoch [1/60], Step [200/2039], Loss: 1.107, Training Accuracy: 58.15%\n",
      "Epoch [1/60], Step [300/2039], Loss: 0.924, Training Accuracy: 66.30%\n",
      "Epoch [1/60], Step [400/2039], Loss: 0.867, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [500/2039], Loss: 0.838, Training Accuracy: 67.93%\n",
      "Epoch [1/60], Step [600/2039], Loss: 0.844, Training Accuracy: 70.65%\n",
      "Epoch [1/60], Step [700/2039], Loss: 0.962, Training Accuracy: 59.78%\n",
      "Epoch [1/60], Step [800/2039], Loss: 0.770, Training Accuracy: 69.02%\n",
      "Epoch [1/60], Step [900/2039], Loss: 0.839, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [1000/2039], Loss: 0.726, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [1100/2039], Loss: 0.755, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [1200/2039], Loss: 0.804, Training Accuracy: 67.39%\n",
      "Epoch [1/60], Step [1300/2039], Loss: 0.720, Training Accuracy: 76.63%\n",
      "Epoch [1/60], Step [1400/2039], Loss: 0.687, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [1500/2039], Loss: 0.704, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [1600/2039], Loss: 0.796, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [1700/2039], Loss: 0.640, Training Accuracy: 75.00%\n",
      "Epoch [1/60], Step [1800/2039], Loss: 0.775, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [1900/2039], Loss: 0.721, Training Accuracy: 71.74%\n",
      "Epoch [1/60], Step [2000/2039], Loss: 0.590, Training Accuracy: 74.46%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 73.44152873099179 %\n",
      "Epoch [2/60], Step [100/2039], Loss: 0.601, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [200/2039], Loss: 0.695, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/2039], Loss: 0.645, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [400/2039], Loss: 0.701, Training Accuracy: 71.74%\n",
      "Epoch [2/60], Step [500/2039], Loss: 0.690, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [600/2039], Loss: 0.570, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [700/2039], Loss: 0.597, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [800/2039], Loss: 0.604, Training Accuracy: 75.54%\n",
      "Epoch [2/60], Step [900/2039], Loss: 0.573, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [1000/2039], Loss: 0.609, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [1100/2039], Loss: 0.601, Training Accuracy: 78.26%\n",
      "Epoch [2/60], Step [1200/2039], Loss: 0.565, Training Accuracy: 77.17%\n",
      "Epoch [2/60], Step [1300/2039], Loss: 0.574, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1400/2039], Loss: 0.559, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1500/2039], Loss: 0.681, Training Accuracy: 76.63%\n",
      "Epoch [2/60], Step [1600/2039], Loss: 0.580, Training Accuracy: 72.28%\n",
      "Epoch [2/60], Step [1700/2039], Loss: 0.371, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [1800/2039], Loss: 0.628, Training Accuracy: 77.72%\n",
      "Epoch [2/60], Step [1900/2039], Loss: 0.578, Training Accuracy: 79.35%\n",
      "Epoch [2/60], Step [2000/2039], Loss: 0.545, Training Accuracy: 77.17%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 75.74754407213027 %\n",
      "Epoch [3/60], Step [100/2039], Loss: 0.680, Training Accuracy: 70.65%\n",
      "Epoch [3/60], Step [200/2039], Loss: 0.621, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [300/2039], Loss: 0.635, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [400/2039], Loss: 0.646, Training Accuracy: 71.20%\n",
      "Epoch [3/60], Step [500/2039], Loss: 0.530, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [600/2039], Loss: 0.535, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [700/2039], Loss: 0.567, Training Accuracy: 77.72%\n",
      "Epoch [3/60], Step [800/2039], Loss: 0.690, Training Accuracy: 72.28%\n",
      "Epoch [3/60], Step [900/2039], Loss: 0.548, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [1000/2039], Loss: 0.612, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [1100/2039], Loss: 0.648, Training Accuracy: 76.63%\n",
      "Epoch [3/60], Step [1200/2039], Loss: 0.620, Training Accuracy: 76.09%\n",
      "Epoch [3/60], Step [1300/2039], Loss: 0.519, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [1400/2039], Loss: 0.608, Training Accuracy: 75.00%\n",
      "Epoch [3/60], Step [1500/2039], Loss: 0.662, Training Accuracy: 73.91%\n",
      "Epoch [3/60], Step [1600/2039], Loss: 0.496, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1700/2039], Loss: 0.601, Training Accuracy: 78.26%\n",
      "Epoch [3/60], Step [1800/2039], Loss: 0.525, Training Accuracy: 80.43%\n",
      "Epoch [3/60], Step [1900/2039], Loss: 0.559, Training Accuracy: 72.83%\n",
      "Epoch [3/60], Step [2000/2039], Loss: 0.584, Training Accuracy: 77.72%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 75.95801372628179 %\n",
      "Epoch [4/60], Step [100/2039], Loss: 0.381, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [200/2039], Loss: 0.476, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [300/2039], Loss: 0.566, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [400/2039], Loss: 0.508, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [500/2039], Loss: 0.519, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [600/2039], Loss: 0.389, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [700/2039], Loss: 0.463, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [800/2039], Loss: 0.454, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [900/2039], Loss: 0.508, Training Accuracy: 79.89%\n",
      "Epoch [4/60], Step [1000/2039], Loss: 0.385, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [1100/2039], Loss: 0.421, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1200/2039], Loss: 0.499, Training Accuracy: 80.98%\n",
      "Epoch [4/60], Step [1300/2039], Loss: 0.420, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1400/2039], Loss: 0.562, Training Accuracy: 77.72%\n",
      "Epoch [4/60], Step [1500/2039], Loss: 0.415, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1600/2039], Loss: 0.506, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [1700/2039], Loss: 0.493, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [1800/2039], Loss: 0.435, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1900/2039], Loss: 0.486, Training Accuracy: 83.15%\n",
      "Epoch [4/60], Step [2000/2039], Loss: 0.501, Training Accuracy: 80.98%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 77.34194590230116 %\n",
      "Epoch [5/60], Step [100/2039], Loss: 0.464, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [200/2039], Loss: 0.465, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [300/2039], Loss: 0.584, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [400/2039], Loss: 0.527, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [500/2039], Loss: 0.518, Training Accuracy: 77.72%\n",
      "Epoch [5/60], Step [600/2039], Loss: 0.390, Training Accuracy: 85.87%\n",
      "Epoch [5/60], Step [700/2039], Loss: 0.467, Training Accuracy: 79.35%\n",
      "Epoch [5/60], Step [800/2039], Loss: 0.533, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [900/2039], Loss: 0.468, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [1000/2039], Loss: 0.488, Training Accuracy: 82.61%\n",
      "Epoch [5/60], Step [1100/2039], Loss: 0.429, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1200/2039], Loss: 0.481, Training Accuracy: 79.89%\n",
      "Epoch [5/60], Step [1300/2039], Loss: 0.505, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1400/2039], Loss: 0.422, Training Accuracy: 82.07%\n",
      "Epoch [5/60], Step [1500/2039], Loss: 0.479, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [1600/2039], Loss: 0.418, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1700/2039], Loss: 0.511, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [1800/2039], Loss: 0.482, Training Accuracy: 84.78%\n",
      "Epoch [5/60], Step [1900/2039], Loss: 0.519, Training Accuracy: 80.43%\n",
      "Epoch [5/60], Step [2000/2039], Loss: 0.448, Training Accuracy: 82.07%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 77.20522136993675 %\n",
      "Epoch [6/60], Step [100/2039], Loss: 0.408, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [200/2039], Loss: 0.511, Training Accuracy: 78.26%\n",
      "Epoch [6/60], Step [300/2039], Loss: 0.380, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [400/2039], Loss: 0.517, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [500/2039], Loss: 0.423, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [600/2039], Loss: 0.373, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [700/2039], Loss: 0.536, Training Accuracy: 79.35%\n",
      "Epoch [6/60], Step [800/2039], Loss: 0.491, Training Accuracy: 79.89%\n",
      "Epoch [6/60], Step [900/2039], Loss: 0.382, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [1000/2039], Loss: 0.395, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1100/2039], Loss: 0.423, Training Accuracy: 80.43%\n",
      "Epoch [6/60], Step [1200/2039], Loss: 0.423, Training Accuracy: 82.07%\n",
      "Epoch [6/60], Step [1300/2039], Loss: 0.365, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [1400/2039], Loss: 0.537, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [1500/2039], Loss: 0.469, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [1600/2039], Loss: 0.488, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [1700/2039], Loss: 0.486, Training Accuracy: 83.15%\n",
      "Epoch [6/60], Step [1800/2039], Loss: 0.325, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [1900/2039], Loss: 0.438, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [2000/2039], Loss: 0.379, Training Accuracy: 87.50%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 78.70703808370341 %\n",
      "Epoch [7/60], Step [100/2039], Loss: 0.393, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [200/2039], Loss: 0.343, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [300/2039], Loss: 0.402, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [400/2039], Loss: 0.424, Training Accuracy: 84.78%\n",
      "Epoch [7/60], Step [500/2039], Loss: 0.318, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [600/2039], Loss: 0.342, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [700/2039], Loss: 0.404, Training Accuracy: 83.70%\n",
      "Epoch [7/60], Step [800/2039], Loss: 0.461, Training Accuracy: 81.52%\n",
      "Epoch [7/60], Step [900/2039], Loss: 0.339, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1000/2039], Loss: 0.337, Training Accuracy: 89.13%\n",
      "Epoch [7/60], Step [1100/2039], Loss: 0.331, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [1200/2039], Loss: 0.486, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [1300/2039], Loss: 0.410, Training Accuracy: 87.50%\n",
      "Epoch [7/60], Step [1400/2039], Loss: 0.433, Training Accuracy: 84.24%\n",
      "Epoch [7/60], Step [1500/2039], Loss: 0.466, Training Accuracy: 82.61%\n",
      "Epoch [7/60], Step [1600/2039], Loss: 0.388, Training Accuracy: 88.59%\n",
      "Epoch [7/60], Step [1700/2039], Loss: 0.367, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [1800/2039], Loss: 0.375, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1900/2039], Loss: 0.372, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [2000/2039], Loss: 0.440, Training Accuracy: 82.07%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 78.05463598438972 %\n",
      "Epoch [8/60], Step [100/2039], Loss: 0.347, Training Accuracy: 84.78%\n",
      "Epoch [8/60], Step [200/2039], Loss: 0.422, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [300/2039], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [400/2039], Loss: 0.426, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [500/2039], Loss: 0.410, Training Accuracy: 80.98%\n",
      "Epoch [8/60], Step [600/2039], Loss: 0.461, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [700/2039], Loss: 0.354, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [800/2039], Loss: 0.476, Training Accuracy: 81.52%\n",
      "Epoch [8/60], Step [900/2039], Loss: 0.379, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [1000/2039], Loss: 0.455, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1100/2039], Loss: 0.344, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1200/2039], Loss: 0.398, Training Accuracy: 86.41%\n",
      "Epoch [8/60], Step [1300/2039], Loss: 0.444, Training Accuracy: 82.61%\n",
      "Epoch [8/60], Step [1400/2039], Loss: 0.438, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [1500/2039], Loss: 0.413, Training Accuracy: 84.24%\n",
      "Epoch [8/60], Step [1600/2039], Loss: 0.320, Training Accuracy: 86.96%\n",
      "Epoch [8/60], Step [1700/2039], Loss: 0.376, Training Accuracy: 85.33%\n",
      "Epoch [8/60], Step [1800/2039], Loss: 0.411, Training Accuracy: 85.87%\n",
      "Epoch [8/60], Step [1900/2039], Loss: 0.425, Training Accuracy: 83.15%\n",
      "Epoch [8/60], Step [2000/2039], Loss: 0.405, Training Accuracy: 83.70%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 77.23321221908222 %\n",
      "Epoch [9/60], Step [100/2039], Loss: 0.343, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [200/2039], Loss: 0.375, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [300/2039], Loss: 0.416, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [400/2039], Loss: 0.332, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [500/2039], Loss: 0.367, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [600/2039], Loss: 0.396, Training Accuracy: 85.87%\n",
      "Epoch [9/60], Step [700/2039], Loss: 0.388, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [800/2039], Loss: 0.357, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [900/2039], Loss: 0.321, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1000/2039], Loss: 0.352, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [1100/2039], Loss: 0.452, Training Accuracy: 83.70%\n",
      "Epoch [9/60], Step [1200/2039], Loss: 0.426, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [1300/2039], Loss: 0.331, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [1400/2039], Loss: 0.306, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [1500/2039], Loss: 0.457, Training Accuracy: 79.35%\n",
      "Epoch [9/60], Step [1600/2039], Loss: 0.429, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1700/2039], Loss: 0.422, Training Accuracy: 84.24%\n",
      "Epoch [9/60], Step [1800/2039], Loss: 0.355, Training Accuracy: 89.13%\n",
      "Epoch [9/60], Step [1900/2039], Loss: 0.330, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [2000/2039], Loss: 0.381, Training Accuracy: 85.33%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 77.29942134302247 %\n",
      "Epoch [10/60], Step [100/2039], Loss: 0.369, Training Accuracy: 86.96%\n",
      "Epoch [10/60], Step [200/2039], Loss: 0.340, Training Accuracy: 83.70%\n",
      "Epoch [10/60], Step [300/2039], Loss: 0.488, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [400/2039], Loss: 0.319, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [500/2039], Loss: 0.307, Training Accuracy: 90.76%\n",
      "Epoch [10/60], Step [600/2039], Loss: 0.329, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [700/2039], Loss: 0.359, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [800/2039], Loss: 0.370, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [900/2039], Loss: 0.405, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [1000/2039], Loss: 0.389, Training Accuracy: 84.78%\n",
      "Epoch [10/60], Step [1100/2039], Loss: 0.355, Training Accuracy: 84.24%\n",
      "Epoch [10/60], Step [1200/2039], Loss: 0.372, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1300/2039], Loss: 0.340, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [1400/2039], Loss: 0.258, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [1500/2039], Loss: 0.445, Training Accuracy: 83.15%\n",
      "Epoch [10/60], Step [1600/2039], Loss: 0.398, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [1700/2039], Loss: 0.361, Training Accuracy: 85.87%\n",
      "Epoch [10/60], Step [1800/2039], Loss: 0.350, Training Accuracy: 86.41%\n",
      "Epoch [10/60], Step [1900/2039], Loss: 0.328, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [2000/2039], Loss: 0.317, Training Accuracy: 89.13%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 78.18059480554435 %\n",
      "Epoch [11/60], Step [100/2039], Loss: 0.292, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [200/2039], Loss: 0.334, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [300/2039], Loss: 0.309, Training Accuracy: 88.04%\n",
      "Epoch [11/60], Step [400/2039], Loss: 0.333, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [500/2039], Loss: 0.372, Training Accuracy: 82.61%\n",
      "Epoch [11/60], Step [600/2039], Loss: 0.373, Training Accuracy: 84.24%\n",
      "Epoch [11/60], Step [700/2039], Loss: 0.384, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [800/2039], Loss: 0.358, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [900/2039], Loss: 0.339, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [1000/2039], Loss: 0.369, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [1100/2039], Loss: 0.321, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1200/2039], Loss: 0.296, Training Accuracy: 89.13%\n",
      "Epoch [11/60], Step [1300/2039], Loss: 0.350, Training Accuracy: 86.96%\n",
      "Epoch [11/60], Step [1400/2039], Loss: 0.310, Training Accuracy: 89.67%\n",
      "Epoch [11/60], Step [1500/2039], Loss: 0.390, Training Accuracy: 86.41%\n",
      "Epoch [11/60], Step [1600/2039], Loss: 0.396, Training Accuracy: 85.33%\n",
      "Epoch [11/60], Step [1700/2039], Loss: 0.394, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [1800/2039], Loss: 0.426, Training Accuracy: 79.89%\n",
      "Epoch [11/60], Step [1900/2039], Loss: 0.275, Training Accuracy: 87.50%\n",
      "Epoch [11/60], Step [2000/2039], Loss: 0.309, Training Accuracy: 88.59%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 76.64594267258781 %\n",
      "Finished early after 11 epochs!\n",
      "Final Training Accuracy 87.45183680108366 %\n",
      "Final Test Accuracy 76.60126497106715 %\n",
      "Epoch [1/60], Step [100/1912], Loss: 1.075, Training Accuracy: 63.59%\n",
      "Epoch [1/60], Step [200/1912], Loss: 0.885, Training Accuracy: 61.96%\n",
      "Epoch [1/60], Step [300/1912], Loss: 0.855, Training Accuracy: 65.76%\n",
      "Epoch [1/60], Step [400/1912], Loss: 0.763, Training Accuracy: 72.28%\n",
      "Epoch [1/60], Step [500/1912], Loss: 0.765, Training Accuracy: 66.85%\n",
      "Epoch [1/60], Step [600/1912], Loss: 0.704, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [700/1912], Loss: 0.614, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [800/1912], Loss: 0.677, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [900/1912], Loss: 0.759, Training Accuracy: 71.20%\n",
      "Epoch [1/60], Step [1000/1912], Loss: 0.670, Training Accuracy: 72.83%\n",
      "Epoch [1/60], Step [1100/1912], Loss: 0.660, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [1200/1912], Loss: 0.701, Training Accuracy: 70.11%\n",
      "Epoch [1/60], Step [1300/1912], Loss: 0.581, Training Accuracy: 74.46%\n",
      "Epoch [1/60], Step [1400/1912], Loss: 0.566, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [1500/1912], Loss: 0.678, Training Accuracy: 75.54%\n",
      "Epoch [1/60], Step [1600/1912], Loss: 0.486, Training Accuracy: 82.61%\n",
      "Epoch [1/60], Step [1700/1912], Loss: 0.510, Training Accuracy: 82.07%\n",
      "Epoch [1/60], Step [1800/1912], Loss: 0.528, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [1900/1912], Loss: 0.541, Training Accuracy: 78.80%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 60.068422689538025 %\n",
      "Epoch [2/60], Step [100/1912], Loss: 0.528, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [200/1912], Loss: 0.621, Training Accuracy: 76.09%\n",
      "Epoch [2/60], Step [300/1912], Loss: 0.609, Training Accuracy: 73.91%\n",
      "Epoch [2/60], Step [400/1912], Loss: 0.520, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [500/1912], Loss: 0.486, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [600/1912], Loss: 0.617, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [700/1912], Loss: 0.363, Training Accuracy: 87.50%\n",
      "Epoch [2/60], Step [800/1912], Loss: 0.502, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [900/1912], Loss: 0.538, Training Accuracy: 78.80%\n",
      "Epoch [2/60], Step [1000/1912], Loss: 0.455, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [1100/1912], Loss: 0.441, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [1200/1912], Loss: 0.623, Training Accuracy: 75.00%\n",
      "Epoch [2/60], Step [1300/1912], Loss: 0.543, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1400/1912], Loss: 0.501, Training Accuracy: 79.89%\n",
      "Epoch [2/60], Step [1500/1912], Loss: 0.486, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [1600/1912], Loss: 0.429, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [1700/1912], Loss: 0.502, Training Accuracy: 82.61%\n",
      "Epoch [2/60], Step [1800/1912], Loss: 0.412, Training Accuracy: 89.13%\n",
      "Epoch [2/60], Step [1900/1912], Loss: 0.547, Training Accuracy: 78.80%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 57.848273882150295 %\n",
      "Epoch [3/60], Step [100/1912], Loss: 0.456, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [200/1912], Loss: 0.524, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [300/1912], Loss: 0.453, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [400/1912], Loss: 0.494, Training Accuracy: 79.35%\n",
      "Epoch [3/60], Step [500/1912], Loss: 0.443, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [600/1912], Loss: 0.508, Training Accuracy: 78.80%\n",
      "Epoch [3/60], Step [700/1912], Loss: 0.476, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [800/1912], Loss: 0.444, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [900/1912], Loss: 0.393, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1000/1912], Loss: 0.443, Training Accuracy: 82.07%\n",
      "Epoch [3/60], Step [1100/1912], Loss: 0.379, Training Accuracy: 83.70%\n",
      "Epoch [3/60], Step [1200/1912], Loss: 0.449, Training Accuracy: 84.78%\n",
      "Epoch [3/60], Step [1300/1912], Loss: 0.518, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [1400/1912], Loss: 0.463, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1500/1912], Loss: 0.478, Training Accuracy: 82.61%\n",
      "Epoch [3/60], Step [1600/1912], Loss: 0.478, Training Accuracy: 79.89%\n",
      "Epoch [3/60], Step [1700/1912], Loss: 0.465, Training Accuracy: 81.52%\n",
      "Epoch [3/60], Step [1800/1912], Loss: 0.367, Training Accuracy: 83.15%\n",
      "Epoch [3/60], Step [1900/1912], Loss: 0.420, Training Accuracy: 84.24%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 57.61094763032608 %\n",
      "Epoch [4/60], Step [100/1912], Loss: 0.394, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [200/1912], Loss: 0.339, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [300/1912], Loss: 0.451, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [400/1912], Loss: 0.405, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [500/1912], Loss: 0.430, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [600/1912], Loss: 0.372, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [700/1912], Loss: 0.432, Training Accuracy: 82.07%\n",
      "Epoch [4/60], Step [800/1912], Loss: 0.392, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [900/1912], Loss: 0.484, Training Accuracy: 78.80%\n",
      "Epoch [4/60], Step [1000/1912], Loss: 0.411, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1100/1912], Loss: 0.375, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1200/1912], Loss: 0.425, Training Accuracy: 83.70%\n",
      "Epoch [4/60], Step [1300/1912], Loss: 0.390, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [1400/1912], Loss: 0.437, Training Accuracy: 82.61%\n",
      "Epoch [4/60], Step [1500/1912], Loss: 0.343, Training Accuracy: 86.96%\n",
      "Epoch [4/60], Step [1600/1912], Loss: 0.359, Training Accuracy: 86.41%\n",
      "Epoch [4/60], Step [1700/1912], Loss: 0.458, Training Accuracy: 80.43%\n",
      "Epoch [4/60], Step [1800/1912], Loss: 0.409, Training Accuracy: 84.24%\n",
      "Epoch [4/60], Step [1900/1912], Loss: 0.378, Training Accuracy: 85.87%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 57.57362616330535 %\n",
      "Epoch [5/60], Step [100/1912], Loss: 0.405, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [200/1912], Loss: 0.411, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [300/1912], Loss: 0.352, Training Accuracy: 86.41%\n",
      "Epoch [5/60], Step [400/1912], Loss: 0.334, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [500/1912], Loss: 0.425, Training Accuracy: 83.70%\n",
      "Epoch [5/60], Step [600/1912], Loss: 0.373, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [700/1912], Loss: 0.371, Training Accuracy: 86.96%\n",
      "Epoch [5/60], Step [800/1912], Loss: 0.385, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [900/1912], Loss: 0.432, Training Accuracy: 81.52%\n",
      "Epoch [5/60], Step [1000/1912], Loss: 0.306, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1100/1912], Loss: 0.439, Training Accuracy: 83.15%\n",
      "Epoch [5/60], Step [1200/1912], Loss: 0.307, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1300/1912], Loss: 0.319, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [1400/1912], Loss: 0.462, Training Accuracy: 85.33%\n",
      "Epoch [5/60], Step [1500/1912], Loss: 0.393, Training Accuracy: 84.24%\n",
      "Epoch [5/60], Step [1600/1912], Loss: 0.350, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [1700/1912], Loss: 0.298, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [1800/1912], Loss: 0.362, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1900/1912], Loss: 0.432, Training Accuracy: 84.78%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 56.54824278092777 %\n",
      "Epoch [6/60], Step [100/1912], Loss: 0.354, Training Accuracy: 84.24%\n",
      "Epoch [6/60], Step [200/1912], Loss: 0.320, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [300/1912], Loss: 0.309, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [400/1912], Loss: 0.326, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [500/1912], Loss: 0.308, Training Accuracy: 88.59%\n",
      "Epoch [6/60], Step [600/1912], Loss: 0.302, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [700/1912], Loss: 0.384, Training Accuracy: 84.78%\n",
      "Epoch [6/60], Step [800/1912], Loss: 0.353, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [900/1912], Loss: 0.501, Training Accuracy: 78.80%\n",
      "Epoch [6/60], Step [1000/1912], Loss: 0.288, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1100/1912], Loss: 0.379, Training Accuracy: 83.70%\n",
      "Epoch [6/60], Step [1200/1912], Loss: 0.319, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [1300/1912], Loss: 0.385, Training Accuracy: 86.41%\n",
      "Epoch [6/60], Step [1400/1912], Loss: 0.420, Training Accuracy: 85.87%\n",
      "Epoch [6/60], Step [1500/1912], Loss: 0.318, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1600/1912], Loss: 0.392, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1700/1912], Loss: 0.463, Training Accuracy: 81.52%\n",
      "Epoch [6/60], Step [1800/1912], Loss: 0.357, Training Accuracy: 87.50%\n",
      "Epoch [6/60], Step [1900/1912], Loss: 0.345, Training Accuracy: 86.96%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 58.83729275819996 %\n",
      "Finished early after 6 epochs!\n",
      "Final Training Accuracy 87.73446804640044 %\n",
      "Final Test Accuracy 58.82245986746094 %\n",
      "Epoch [1/60], Step [100/2146], Loss: 0.785, Training Accuracy: 73.37%\n",
      "Epoch [1/60], Step [200/2146], Loss: 0.548, Training Accuracy: 80.98%\n",
      "Epoch [1/60], Step [300/2146], Loss: 0.600, Training Accuracy: 77.17%\n",
      "Epoch [1/60], Step [400/2146], Loss: 0.576, Training Accuracy: 79.35%\n",
      "Epoch [1/60], Step [500/2146], Loss: 0.462, Training Accuracy: 85.87%\n",
      "Epoch [1/60], Step [600/2146], Loss: 0.594, Training Accuracy: 76.09%\n",
      "Epoch [1/60], Step [700/2146], Loss: 0.388, Training Accuracy: 86.41%\n",
      "Epoch [1/60], Step [800/2146], Loss: 0.393, Training Accuracy: 86.96%\n",
      "Epoch [1/60], Step [900/2146], Loss: 0.399, Training Accuracy: 86.41%\n",
      "Epoch [1/60], Step [1000/2146], Loss: 0.447, Training Accuracy: 82.07%\n",
      "Epoch [1/60], Step [1100/2146], Loss: 0.399, Training Accuracy: 86.96%\n",
      "Epoch [1/60], Step [1200/2146], Loss: 0.431, Training Accuracy: 84.24%\n",
      "Epoch [1/60], Step [1300/2146], Loss: 0.388, Training Accuracy: 88.04%\n",
      "Epoch [1/60], Step [1400/2146], Loss: 0.354, Training Accuracy: 87.50%\n",
      "Epoch [1/60], Step [1500/2146], Loss: 0.520, Training Accuracy: 82.07%\n",
      "Epoch [1/60], Step [1600/2146], Loss: 0.438, Training Accuracy: 85.33%\n",
      "Epoch [1/60], Step [1700/2146], Loss: 0.504, Training Accuracy: 80.98%\n",
      "Epoch [1/60], Step [1800/2146], Loss: 0.427, Training Accuracy: 83.70%\n",
      "Epoch [1/60], Step [1900/2146], Loss: 0.412, Training Accuracy: 84.24%\n",
      "Epoch [1/60], Step [2000/2146], Loss: 0.424, Training Accuracy: 84.24%\n",
      "Epoch [1/60], Step [2100/2146], Loss: 0.449, Training Accuracy: 83.15%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 41.15351257588898 %\n",
      "Epoch [2/60], Step [100/2146], Loss: 0.372, Training Accuracy: 88.04%\n",
      "Epoch [2/60], Step [200/2146], Loss: 0.381, Training Accuracy: 88.04%\n",
      "Epoch [2/60], Step [300/2146], Loss: 0.393, Training Accuracy: 83.70%\n",
      "Epoch [2/60], Step [400/2146], Loss: 0.426, Training Accuracy: 86.96%\n",
      "Epoch [2/60], Step [500/2146], Loss: 0.373, Training Accuracy: 85.87%\n",
      "Epoch [2/60], Step [600/2146], Loss: 0.395, Training Accuracy: 84.78%\n",
      "Epoch [2/60], Step [700/2146], Loss: 0.457, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [800/2146], Loss: 0.372, Training Accuracy: 85.33%\n",
      "Epoch [2/60], Step [900/2146], Loss: 0.362, Training Accuracy: 86.96%\n",
      "Epoch [2/60], Step [1000/2146], Loss: 0.456, Training Accuracy: 82.07%\n",
      "Epoch [2/60], Step [1100/2146], Loss: 0.300, Training Accuracy: 88.59%\n",
      "Epoch [2/60], Step [1200/2146], Loss: 0.410, Training Accuracy: 86.41%\n",
      "Epoch [2/60], Step [1300/2146], Loss: 0.283, Training Accuracy: 89.67%\n",
      "Epoch [2/60], Step [1400/2146], Loss: 0.367, Training Accuracy: 87.50%\n",
      "Epoch [2/60], Step [1500/2146], Loss: 0.391, Training Accuracy: 88.04%\n",
      "Epoch [2/60], Step [1600/2146], Loss: 0.302, Training Accuracy: 89.67%\n",
      "Epoch [2/60], Step [1700/2146], Loss: 0.388, Training Accuracy: 86.96%\n",
      "Epoch [2/60], Step [1800/2146], Loss: 0.321, Training Accuracy: 87.50%\n",
      "Epoch [2/60], Step [1900/2146], Loss: 0.323, Training Accuracy: 88.04%\n",
      "Epoch [2/60], Step [2000/2146], Loss: 0.357, Training Accuracy: 88.04%\n",
      "Epoch [2/60], Step [2100/2146], Loss: 0.334, Training Accuracy: 88.04%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 42.04611159294594 %\n",
      "Epoch [3/60], Step [100/2146], Loss: 0.351, Training Accuracy: 87.50%\n",
      "Epoch [3/60], Step [200/2146], Loss: 0.376, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [300/2146], Loss: 0.314, Training Accuracy: 90.76%\n",
      "Epoch [3/60], Step [400/2146], Loss: 0.392, Training Accuracy: 85.33%\n",
      "Epoch [3/60], Step [500/2146], Loss: 0.243, Training Accuracy: 93.48%\n",
      "Epoch [3/60], Step [600/2146], Loss: 0.322, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [700/2146], Loss: 0.291, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [800/2146], Loss: 0.329, Training Accuracy: 88.04%\n",
      "Epoch [3/60], Step [900/2146], Loss: 0.389, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [1000/2146], Loss: 0.329, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1100/2146], Loss: 0.303, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1200/2146], Loss: 0.357, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1300/2146], Loss: 0.314, Training Accuracy: 89.67%\n",
      "Epoch [3/60], Step [1400/2146], Loss: 0.366, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1500/2146], Loss: 0.279, Training Accuracy: 88.59%\n",
      "Epoch [3/60], Step [1600/2146], Loss: 0.335, Training Accuracy: 89.13%\n",
      "Epoch [3/60], Step [1700/2146], Loss: 0.314, Training Accuracy: 86.41%\n",
      "Epoch [3/60], Step [1800/2146], Loss: 0.319, Training Accuracy: 86.96%\n",
      "Epoch [3/60], Step [1900/2146], Loss: 0.338, Training Accuracy: 84.24%\n",
      "Epoch [3/60], Step [2000/2146], Loss: 0.298, Training Accuracy: 90.22%\n",
      "Epoch [3/60], Step [2100/2146], Loss: 0.360, Training Accuracy: 87.50%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 43.746988532331116 %\n",
      "Epoch [4/60], Step [100/2146], Loss: 0.360, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [200/2146], Loss: 0.228, Training Accuracy: 90.76%\n",
      "Epoch [4/60], Step [300/2146], Loss: 0.302, Training Accuracy: 88.59%\n",
      "Epoch [4/60], Step [400/2146], Loss: 0.325, Training Accuracy: 84.78%\n",
      "Epoch [4/60], Step [500/2146], Loss: 0.293, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [600/2146], Loss: 0.343, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [700/2146], Loss: 0.297, Training Accuracy: 89.67%\n",
      "Epoch [4/60], Step [800/2146], Loss: 0.324, Training Accuracy: 87.50%\n",
      "Epoch [4/60], Step [900/2146], Loss: 0.303, Training Accuracy: 90.76%\n",
      "Epoch [4/60], Step [1000/2146], Loss: 0.305, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1100/2146], Loss: 0.325, Training Accuracy: 85.87%\n",
      "Epoch [4/60], Step [1200/2146], Loss: 0.281, Training Accuracy: 90.22%\n",
      "Epoch [4/60], Step [1300/2146], Loss: 0.228, Training Accuracy: 90.76%\n",
      "Epoch [4/60], Step [1400/2146], Loss: 0.277, Training Accuracy: 89.67%\n",
      "Epoch [4/60], Step [1500/2146], Loss: 0.362, Training Accuracy: 89.13%\n",
      "Epoch [4/60], Step [1600/2146], Loss: 0.271, Training Accuracy: 90.22%\n",
      "Epoch [4/60], Step [1700/2146], Loss: 0.321, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [1800/2146], Loss: 0.344, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [1900/2146], Loss: 0.256, Training Accuracy: 88.04%\n",
      "Epoch [4/60], Step [2000/2146], Loss: 0.361, Training Accuracy: 85.33%\n",
      "Epoch [4/60], Step [2100/2146], Loss: 0.275, Training Accuracy: 90.22%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 43.645200925122865 %\n",
      "Epoch [5/60], Step [100/2146], Loss: 0.292, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [200/2146], Loss: 0.236, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [300/2146], Loss: 0.309, Training Accuracy: 89.67%\n",
      "Epoch [5/60], Step [400/2146], Loss: 0.305, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [500/2146], Loss: 0.235, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [600/2146], Loss: 0.254, Training Accuracy: 89.13%\n",
      "Epoch [5/60], Step [700/2146], Loss: 0.323, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [800/2146], Loss: 0.363, Training Accuracy: 87.50%\n",
      "Epoch [5/60], Step [900/2146], Loss: 0.194, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [1000/2146], Loss: 0.273, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [1100/2146], Loss: 0.231, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [1200/2146], Loss: 0.212, Training Accuracy: 94.02%\n",
      "Epoch [5/60], Step [1300/2146], Loss: 0.302, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [1400/2146], Loss: 0.248, Training Accuracy: 90.22%\n",
      "Epoch [5/60], Step [1500/2146], Loss: 0.227, Training Accuracy: 91.85%\n",
      "Epoch [5/60], Step [1600/2146], Loss: 0.240, Training Accuracy: 92.93%\n",
      "Epoch [5/60], Step [1700/2146], Loss: 0.244, Training Accuracy: 90.76%\n",
      "Epoch [5/60], Step [1800/2146], Loss: 0.263, Training Accuracy: 91.30%\n",
      "Epoch [5/60], Step [1900/2146], Loss: 0.287, Training Accuracy: 88.59%\n",
      "Epoch [5/60], Step [2000/2146], Loss: 0.311, Training Accuracy: 88.04%\n",
      "Epoch [5/60], Step [2100/2146], Loss: 0.191, Training Accuracy: 94.57%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 45.78755902476631 %\n",
      "Epoch [6/60], Step [100/2146], Loss: 0.310, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [200/2146], Loss: 0.317, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [300/2146], Loss: 0.217, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [400/2146], Loss: 0.305, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [500/2146], Loss: 0.316, Training Accuracy: 86.96%\n",
      "Epoch [6/60], Step [600/2146], Loss: 0.195, Training Accuracy: 93.48%\n",
      "Epoch [6/60], Step [700/2146], Loss: 0.247, Training Accuracy: 92.39%\n",
      "Epoch [6/60], Step [800/2146], Loss: 0.236, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [900/2146], Loss: 0.215, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [1000/2146], Loss: 0.296, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1100/2146], Loss: 0.384, Training Accuracy: 85.33%\n",
      "Epoch [6/60], Step [1200/2146], Loss: 0.246, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1300/2146], Loss: 0.265, Training Accuracy: 92.39%\n",
      "Epoch [6/60], Step [1400/2146], Loss: 0.245, Training Accuracy: 88.04%\n",
      "Epoch [6/60], Step [1500/2146], Loss: 0.307, Training Accuracy: 90.76%\n",
      "Epoch [6/60], Step [1600/2146], Loss: 0.268, Training Accuracy: 89.67%\n",
      "Epoch [6/60], Step [1700/2146], Loss: 0.253, Training Accuracy: 89.13%\n",
      "Epoch [6/60], Step [1800/2146], Loss: 0.219, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [1900/2146], Loss: 0.159, Training Accuracy: 96.20%\n",
      "Epoch [6/60], Step [2000/2146], Loss: 0.327, Training Accuracy: 91.85%\n",
      "Epoch [6/60], Step [2100/2146], Loss: 0.304, Training Accuracy: 88.04%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 46.63317914618869 %\n",
      "Epoch [7/60], Step [100/2146], Loss: 0.214, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [200/2146], Loss: 0.333, Training Accuracy: 86.41%\n",
      "Epoch [7/60], Step [300/2146], Loss: 0.223, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [400/2146], Loss: 0.219, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [500/2146], Loss: 0.193, Training Accuracy: 94.57%\n",
      "Epoch [7/60], Step [600/2146], Loss: 0.353, Training Accuracy: 85.87%\n",
      "Epoch [7/60], Step [700/2146], Loss: 0.295, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [800/2146], Loss: 0.328, Training Accuracy: 86.96%\n",
      "Epoch [7/60], Step [900/2146], Loss: 0.228, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [1000/2146], Loss: 0.270, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [1100/2146], Loss: 0.219, Training Accuracy: 92.93%\n",
      "Epoch [7/60], Step [1200/2146], Loss: 0.285, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [1300/2146], Loss: 0.244, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [1400/2146], Loss: 0.250, Training Accuracy: 90.22%\n",
      "Epoch [7/60], Step [1500/2146], Loss: 0.298, Training Accuracy: 89.67%\n",
      "Epoch [7/60], Step [1600/2146], Loss: 0.223, Training Accuracy: 93.48%\n",
      "Epoch [7/60], Step [1700/2146], Loss: 0.269, Training Accuracy: 90.76%\n",
      "Epoch [7/60], Step [1800/2146], Loss: 0.248, Training Accuracy: 88.04%\n",
      "Epoch [7/60], Step [1900/2146], Loss: 0.219, Training Accuracy: 91.85%\n",
      "Epoch [7/60], Step [2000/2146], Loss: 0.261, Training Accuracy: 92.39%\n",
      "Epoch [7/60], Step [2100/2146], Loss: 0.211, Training Accuracy: 93.48%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 46.10255854293148 %\n",
      "Epoch [8/60], Step [100/2146], Loss: 0.267, Training Accuracy: 88.59%\n",
      "Epoch [8/60], Step [200/2146], Loss: 0.264, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [300/2146], Loss: 0.229, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [400/2146], Loss: 0.289, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [500/2146], Loss: 0.278, Training Accuracy: 89.67%\n",
      "Epoch [8/60], Step [600/2146], Loss: 0.214, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [700/2146], Loss: 0.235, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [800/2146], Loss: 0.208, Training Accuracy: 90.76%\n",
      "Epoch [8/60], Step [900/2146], Loss: 0.221, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [1000/2146], Loss: 0.271, Training Accuracy: 92.93%\n",
      "Epoch [8/60], Step [1100/2146], Loss: 0.230, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1200/2146], Loss: 0.190, Training Accuracy: 93.48%\n",
      "Epoch [8/60], Step [1300/2146], Loss: 0.186, Training Accuracy: 92.39%\n",
      "Epoch [8/60], Step [1400/2146], Loss: 0.229, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1500/2146], Loss: 0.221, Training Accuracy: 94.02%\n",
      "Epoch [8/60], Step [1600/2146], Loss: 0.193, Training Accuracy: 91.85%\n",
      "Epoch [8/60], Step [1700/2146], Loss: 0.217, Training Accuracy: 91.30%\n",
      "Epoch [8/60], Step [1800/2146], Loss: 0.262, Training Accuracy: 87.50%\n",
      "Epoch [8/60], Step [1900/2146], Loss: 0.188, Training Accuracy: 94.57%\n",
      "Epoch [8/60], Step [2000/2146], Loss: 0.258, Training Accuracy: 89.13%\n",
      "Epoch [8/60], Step [2100/2146], Loss: 0.213, Training Accuracy: 91.85%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 44.45649031511998 %\n",
      "Epoch [9/60], Step [100/2146], Loss: 0.217, Training Accuracy: 93.48%\n",
      "Epoch [9/60], Step [200/2146], Loss: 0.359, Training Accuracy: 85.33%\n",
      "Epoch [9/60], Step [300/2146], Loss: 0.134, Training Accuracy: 95.65%\n",
      "Epoch [9/60], Step [400/2146], Loss: 0.277, Training Accuracy: 86.41%\n",
      "Epoch [9/60], Step [500/2146], Loss: 0.282, Training Accuracy: 88.59%\n",
      "Epoch [9/60], Step [600/2146], Loss: 0.297, Training Accuracy: 88.04%\n",
      "Epoch [9/60], Step [700/2146], Loss: 0.211, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [800/2146], Loss: 0.215, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [900/2146], Loss: 0.202, Training Accuracy: 94.02%\n",
      "Epoch [9/60], Step [1000/2146], Loss: 0.208, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [1100/2146], Loss: 0.210, Training Accuracy: 90.76%\n",
      "Epoch [9/60], Step [1200/2146], Loss: 0.254, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [1300/2146], Loss: 0.265, Training Accuracy: 91.30%\n",
      "Epoch [9/60], Step [1400/2146], Loss: 0.253, Training Accuracy: 91.85%\n",
      "Epoch [9/60], Step [1500/2146], Loss: 0.188, Training Accuracy: 94.57%\n",
      "Epoch [9/60], Step [1600/2146], Loss: 0.232, Training Accuracy: 89.67%\n",
      "Epoch [9/60], Step [1700/2146], Loss: 0.235, Training Accuracy: 92.39%\n",
      "Epoch [9/60], Step [1800/2146], Loss: 0.293, Training Accuracy: 87.50%\n",
      "Epoch [9/60], Step [1900/2146], Loss: 0.180, Training Accuracy: 94.57%\n",
      "Epoch [9/60], Step [2000/2146], Loss: 0.185, Training Accuracy: 93.48%\n",
      "Epoch [9/60], Step [2100/2146], Loss: 0.258, Training Accuracy: 90.22%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 44.5486412257878 %\n",
      "Epoch [10/60], Step [100/2146], Loss: 0.215, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [200/2146], Loss: 0.258, Training Accuracy: 90.22%\n",
      "Epoch [10/60], Step [300/2146], Loss: 0.278, Training Accuracy: 88.59%\n",
      "Epoch [10/60], Step [400/2146], Loss: 0.221, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [500/2146], Loss: 0.293, Training Accuracy: 88.04%\n",
      "Epoch [10/60], Step [600/2146], Loss: 0.253, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [700/2146], Loss: 0.159, Training Accuracy: 94.02%\n",
      "Epoch [10/60], Step [800/2146], Loss: 0.177, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [900/2146], Loss: 0.206, Training Accuracy: 91.30%\n",
      "Epoch [10/60], Step [1000/2146], Loss: 0.155, Training Accuracy: 96.20%\n",
      "Epoch [10/60], Step [1100/2146], Loss: 0.202, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [1200/2146], Loss: 0.176, Training Accuracy: 92.93%\n",
      "Epoch [10/60], Step [1300/2146], Loss: 0.225, Training Accuracy: 91.85%\n",
      "Epoch [10/60], Step [1400/2146], Loss: 0.287, Training Accuracy: 87.50%\n",
      "Epoch [10/60], Step [1500/2146], Loss: 0.229, Training Accuracy: 91.85%\n",
      "Epoch [10/60], Step [1600/2146], Loss: 0.211, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [1700/2146], Loss: 0.157, Training Accuracy: 95.11%\n",
      "Epoch [10/60], Step [1800/2146], Loss: 0.210, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [1900/2146], Loss: 0.184, Training Accuracy: 93.48%\n",
      "Epoch [10/60], Step [2000/2146], Loss: 0.205, Training Accuracy: 92.39%\n",
      "Epoch [10/60], Step [2100/2146], Loss: 0.270, Training Accuracy: 92.39%\n",
      "Testing epoch 10\n",
      "Epoch 10: Test Accuracy 44.575744434807746 %\n",
      "Epoch [11/60], Step [100/2146], Loss: 0.285, Training Accuracy: 90.22%\n",
      "Epoch [11/60], Step [200/2146], Loss: 0.190, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [300/2146], Loss: 0.185, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [400/2146], Loss: 0.289, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [500/2146], Loss: 0.204, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [600/2146], Loss: 0.143, Training Accuracy: 94.57%\n",
      "Epoch [11/60], Step [700/2146], Loss: 0.181, Training Accuracy: 95.11%\n",
      "Epoch [11/60], Step [800/2146], Loss: 0.153, Training Accuracy: 95.11%\n",
      "Epoch [11/60], Step [900/2146], Loss: 0.235, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [1000/2146], Loss: 0.185, Training Accuracy: 91.85%\n",
      "Epoch [11/60], Step [1100/2146], Loss: 0.136, Training Accuracy: 94.57%\n",
      "Epoch [11/60], Step [1200/2146], Loss: 0.194, Training Accuracy: 93.48%\n",
      "Epoch [11/60], Step [1300/2146], Loss: 0.239, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [1400/2146], Loss: 0.189, Training Accuracy: 94.02%\n",
      "Epoch [11/60], Step [1500/2146], Loss: 0.233, Training Accuracy: 90.76%\n",
      "Epoch [11/60], Step [1600/2146], Loss: 0.216, Training Accuracy: 91.30%\n",
      "Epoch [11/60], Step [1700/2146], Loss: 0.201, Training Accuracy: 92.93%\n",
      "Epoch [11/60], Step [1800/2146], Loss: 0.142, Training Accuracy: 95.65%\n",
      "Epoch [11/60], Step [1900/2146], Loss: 0.168, Training Accuracy: 94.57%\n",
      "Epoch [11/60], Step [2000/2146], Loss: 0.180, Training Accuracy: 92.39%\n",
      "Epoch [11/60], Step [2100/2146], Loss: 0.250, Training Accuracy: 89.67%\n",
      "Testing epoch 11\n",
      "Epoch 11: Test Accuracy 44.174616941312514 %\n",
      "Finished early after 11 epochs!\n",
      "Final Training Accuracy 92.60936747979837 %\n",
      "Final Test Accuracy 44.119808229738844 %\n",
      "By spectra accuracy: 0.6128862038136768 +/- 0.08511117652582677\n",
      "By spectra log loss: 1.6400012877902306 +/- 0.6881909697993607\n",
      "By simple sample log loss: 1.6195398940362566 +/- 1.4160871625863303\n",
      "By simple sample accuracy: 0.6165474712641223 +/- 0.11396391247385991\n"
     ]
    }
   ],
   "source": [
    "# No hyperparameter search\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "\n",
    "n_repeats = 5\n",
    "folds = 3\n",
    "\n",
    "\n",
    "outer_accuracy = []\n",
    "outer_log_loss = []\n",
    "simple_sample_accuracy = []\n",
    "simple_sample_logloss = []\n",
    "\n",
    "\n",
    "counter = 0\n",
    "while counter < n_repeats*folds:\n",
    "    sgkf = StratifiedGroupKFold(folds, shuffle = True)\n",
    "    for train_index, test_index in sgkf.split(SMART_x, SMART_y, groups=SMART_patient): #(note switching of test/train indices here as we are leaving out 2 centres)\n",
    "\n",
    "        X_train, X_test = SMART_x[train_index], SMART_x[test_index]\n",
    "        y_train, y_test = SMART_y[train_index], SMART_y[test_index]\n",
    "        group_train, group_test = SMART_uniquemapID[train_index], SMART_uniquemapID[test_index]\n",
    "\n",
    "        convnet, con_mat, test_ll, test_acc, train_ll, train_acc, test_outputs, test_predicted  = train_cnn(ConvNet, X_train, y_train, X_test, y_test, learning_rate = learning_rate, num_epochs = n_epochs, batch_size = batch_size, early_stop = 5, learning_curves = False)\n",
    "\n",
    "        outer_accuracy.append(test_acc)\n",
    "        outer_log_loss.append(test_ll)\n",
    "\n",
    "        # By simple sample accuracy\n",
    "        y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_whole(convnet, group_test, X_test, y_test, SMART_y, proportioned = False )\n",
    "        acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "        ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y) )\n",
    "        simple_sample_accuracy.append(acc)\n",
    "        simple_sample_logloss.append(ll)\n",
    "\n",
    "        counter = counter+1\n",
    "print(\"By spectra accuracy: {} +/- {}\".format(np.mean(outer_accuracy),np.std(outer_accuracy)))\n",
    "print(\"By spectra log loss: {} +/- {}\".format(np.mean(outer_log_loss),np.std(outer_log_loss)))\n",
    "print(\"By simple sample log loss: {} +/- {}\".format(np.mean(simple_sample_logloss),np.std(simple_sample_logloss)))\n",
    "print(\"By simple sample accuracy: {} +/- {}\".format(np.mean(simple_sample_accuracy),np.std(simple_sample_accuracy)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjDg-vznKX-V"
   },
   "outputs": [],
   "source": [
    "# Train model on entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "ok",
     "timestamp": 1661182899316,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "SMEoVID2KYSB",
    "outputId": "f88c7634-0583-4e15-c5ed-782e9d4fbca5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.76923076923077"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_epochs = [7,18,6,15,9,17,22,12,24,6,17,14,12]\n",
    "np.mean(early_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 672778,
     "status": "ok",
     "timestamp": 1661184055703,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "G4HAJyP8JmLL",
    "outputId": "227e1046-1e0b-4d31-abca-c755dc5e9e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/13], Step [100/5723], Loss: 1.369, Training Accuracy: 34.69%\n",
      "Epoch [1/13], Step [200/5723], Loss: 1.115, Training Accuracy: 56.12%\n",
      "Epoch [1/13], Step [300/5723], Loss: 1.154, Training Accuracy: 51.02%\n",
      "Epoch [1/13], Step [400/5723], Loss: 1.209, Training Accuracy: 58.16%\n",
      "Epoch [1/13], Step [500/5723], Loss: 1.146, Training Accuracy: 55.10%\n",
      "Epoch [1/13], Step [600/5723], Loss: 1.015, Training Accuracy: 53.06%\n",
      "Epoch [1/13], Step [700/5723], Loss: 0.852, Training Accuracy: 69.39%\n",
      "Epoch [1/13], Step [800/5723], Loss: 1.066, Training Accuracy: 59.18%\n",
      "Epoch [1/13], Step [900/5723], Loss: 1.071, Training Accuracy: 47.96%\n",
      "Epoch [1/13], Step [1000/5723], Loss: 0.939, Training Accuracy: 67.35%\n",
      "Epoch [1/13], Step [1100/5723], Loss: 0.951, Training Accuracy: 64.29%\n",
      "Epoch [1/13], Step [1200/5723], Loss: 1.063, Training Accuracy: 63.27%\n",
      "Epoch [1/13], Step [1300/5723], Loss: 1.296, Training Accuracy: 58.16%\n",
      "Epoch [1/13], Step [1400/5723], Loss: 0.996, Training Accuracy: 58.16%\n",
      "Epoch [1/13], Step [1500/5723], Loss: 0.998, Training Accuracy: 62.24%\n",
      "Epoch [1/13], Step [1600/5723], Loss: 0.932, Training Accuracy: 60.20%\n",
      "Epoch [1/13], Step [1700/5723], Loss: 0.834, Training Accuracy: 68.37%\n",
      "Epoch [1/13], Step [1800/5723], Loss: 0.958, Training Accuracy: 57.14%\n",
      "Epoch [1/13], Step [1900/5723], Loss: 1.069, Training Accuracy: 62.24%\n",
      "Epoch [1/13], Step [2000/5723], Loss: 0.922, Training Accuracy: 62.24%\n",
      "Epoch [1/13], Step [2100/5723], Loss: 0.739, Training Accuracy: 76.53%\n",
      "Epoch [1/13], Step [2200/5723], Loss: 0.814, Training Accuracy: 74.49%\n",
      "Epoch [1/13], Step [2300/5723], Loss: 0.753, Training Accuracy: 67.35%\n",
      "Epoch [1/13], Step [2400/5723], Loss: 0.771, Training Accuracy: 74.49%\n",
      "Epoch [1/13], Step [2500/5723], Loss: 0.885, Training Accuracy: 61.22%\n",
      "Epoch [1/13], Step [2600/5723], Loss: 0.801, Training Accuracy: 74.49%\n",
      "Epoch [1/13], Step [2700/5723], Loss: 0.810, Training Accuracy: 71.43%\n",
      "Epoch [1/13], Step [2800/5723], Loss: 0.740, Training Accuracy: 75.51%\n",
      "Epoch [1/13], Step [2900/5723], Loss: 1.011, Training Accuracy: 63.27%\n",
      "Epoch [1/13], Step [3000/5723], Loss: 1.099, Training Accuracy: 60.20%\n",
      "Epoch [1/13], Step [3100/5723], Loss: 0.787, Training Accuracy: 74.49%\n",
      "Epoch [1/13], Step [3200/5723], Loss: 0.798, Training Accuracy: 70.41%\n",
      "Epoch [1/13], Step [3300/5723], Loss: 0.634, Training Accuracy: 79.59%\n",
      "Epoch [1/13], Step [3400/5723], Loss: 0.899, Training Accuracy: 65.31%\n",
      "Epoch [1/13], Step [3500/5723], Loss: 0.852, Training Accuracy: 69.39%\n",
      "Epoch [1/13], Step [3600/5723], Loss: 0.888, Training Accuracy: 67.35%\n",
      "Epoch [1/13], Step [3700/5723], Loss: 0.741, Training Accuracy: 70.41%\n",
      "Epoch [1/13], Step [3800/5723], Loss: 0.884, Training Accuracy: 66.33%\n",
      "Epoch [1/13], Step [3900/5723], Loss: 0.654, Training Accuracy: 78.57%\n",
      "Epoch [1/13], Step [4000/5723], Loss: 0.676, Training Accuracy: 75.51%\n",
      "Epoch [1/13], Step [4100/5723], Loss: 0.700, Training Accuracy: 73.47%\n",
      "Epoch [1/13], Step [4200/5723], Loss: 0.707, Training Accuracy: 76.53%\n",
      "Epoch [1/13], Step [4300/5723], Loss: 0.741, Training Accuracy: 68.37%\n",
      "Epoch [1/13], Step [4400/5723], Loss: 0.764, Training Accuracy: 74.49%\n",
      "Epoch [1/13], Step [4500/5723], Loss: 0.788, Training Accuracy: 66.33%\n",
      "Epoch [1/13], Step [4600/5723], Loss: 0.929, Training Accuracy: 66.33%\n",
      "Epoch [1/13], Step [4700/5723], Loss: 0.842, Training Accuracy: 64.29%\n",
      "Epoch [1/13], Step [4800/5723], Loss: 0.686, Training Accuracy: 79.59%\n",
      "Epoch [1/13], Step [4900/5723], Loss: 0.669, Training Accuracy: 73.47%\n",
      "Epoch [1/13], Step [5000/5723], Loss: 0.862, Training Accuracy: 69.39%\n",
      "Epoch [1/13], Step [5100/5723], Loss: 0.810, Training Accuracy: 75.51%\n",
      "Epoch [1/13], Step [5200/5723], Loss: 0.884, Training Accuracy: 59.18%\n",
      "Epoch [1/13], Step [5300/5723], Loss: 0.764, Training Accuracy: 73.47%\n",
      "Epoch [1/13], Step [5400/5723], Loss: 0.782, Training Accuracy: 75.51%\n",
      "Epoch [1/13], Step [5500/5723], Loss: 0.805, Training Accuracy: 72.45%\n",
      "Epoch [1/13], Step [5600/5723], Loss: 0.491, Training Accuracy: 79.59%\n",
      "Epoch [1/13], Step [5700/5723], Loss: 0.801, Training Accuracy: 72.45%\n",
      "Epoch [2/13], Step [100/5723], Loss: 0.716, Training Accuracy: 73.47%\n",
      "Epoch [2/13], Step [200/5723], Loss: 0.890, Training Accuracy: 68.37%\n",
      "Epoch [2/13], Step [300/5723], Loss: 0.806, Training Accuracy: 75.51%\n",
      "Epoch [2/13], Step [400/5723], Loss: 0.811, Training Accuracy: 73.47%\n",
      "Epoch [2/13], Step [500/5723], Loss: 0.880, Training Accuracy: 76.53%\n",
      "Epoch [2/13], Step [600/5723], Loss: 0.792, Training Accuracy: 66.33%\n",
      "Epoch [2/13], Step [700/5723], Loss: 0.662, Training Accuracy: 75.51%\n",
      "Epoch [2/13], Step [800/5723], Loss: 0.637, Training Accuracy: 75.51%\n",
      "Epoch [2/13], Step [900/5723], Loss: 0.806, Training Accuracy: 70.41%\n",
      "Epoch [2/13], Step [1000/5723], Loss: 0.468, Training Accuracy: 84.69%\n",
      "Epoch [2/13], Step [1100/5723], Loss: 0.784, Training Accuracy: 68.37%\n",
      "Epoch [2/13], Step [1200/5723], Loss: 0.769, Training Accuracy: 73.47%\n",
      "Epoch [2/13], Step [1300/5723], Loss: 0.908, Training Accuracy: 63.27%\n",
      "Epoch [2/13], Step [1400/5723], Loss: 0.560, Training Accuracy: 78.57%\n",
      "Epoch [2/13], Step [1500/5723], Loss: 0.839, Training Accuracy: 69.39%\n",
      "Epoch [2/13], Step [1600/5723], Loss: 0.713, Training Accuracy: 68.37%\n",
      "Epoch [2/13], Step [1700/5723], Loss: 0.837, Training Accuracy: 71.43%\n",
      "Epoch [2/13], Step [1800/5723], Loss: 0.731, Training Accuracy: 81.63%\n",
      "Epoch [2/13], Step [1900/5723], Loss: 0.777, Training Accuracy: 75.51%\n",
      "Epoch [2/13], Step [2000/5723], Loss: 0.676, Training Accuracy: 79.59%\n",
      "Epoch [2/13], Step [2100/5723], Loss: 0.756, Training Accuracy: 70.41%\n",
      "Epoch [2/13], Step [2200/5723], Loss: 0.777, Training Accuracy: 63.27%\n",
      "Epoch [2/13], Step [2300/5723], Loss: 0.765, Training Accuracy: 72.45%\n",
      "Epoch [2/13], Step [2400/5723], Loss: 0.667, Training Accuracy: 74.49%\n",
      "Epoch [2/13], Step [2500/5723], Loss: 0.671, Training Accuracy: 79.59%\n",
      "Epoch [2/13], Step [2600/5723], Loss: 0.799, Training Accuracy: 78.57%\n",
      "Epoch [2/13], Step [2700/5723], Loss: 0.803, Training Accuracy: 76.53%\n",
      "Epoch [2/13], Step [2800/5723], Loss: 0.765, Training Accuracy: 74.49%\n",
      "Epoch [2/13], Step [2900/5723], Loss: 0.634, Training Accuracy: 77.55%\n",
      "Epoch [2/13], Step [3000/5723], Loss: 0.554, Training Accuracy: 76.53%\n",
      "Epoch [2/13], Step [3100/5723], Loss: 0.700, Training Accuracy: 72.45%\n",
      "Epoch [2/13], Step [3200/5723], Loss: 0.501, Training Accuracy: 74.49%\n",
      "Epoch [2/13], Step [3300/5723], Loss: 0.639, Training Accuracy: 75.51%\n",
      "Epoch [2/13], Step [3400/5723], Loss: 0.618, Training Accuracy: 76.53%\n",
      "Epoch [2/13], Step [3500/5723], Loss: 0.781, Training Accuracy: 70.41%\n",
      "Epoch [2/13], Step [3600/5723], Loss: 0.600, Training Accuracy: 77.55%\n",
      "Epoch [2/13], Step [3700/5723], Loss: 0.736, Training Accuracy: 69.39%\n",
      "Epoch [2/13], Step [3800/5723], Loss: 0.946, Training Accuracy: 68.37%\n",
      "Epoch [2/13], Step [3900/5723], Loss: 0.569, Training Accuracy: 78.57%\n",
      "Epoch [2/13], Step [4000/5723], Loss: 0.706, Training Accuracy: 73.47%\n",
      "Epoch [2/13], Step [4100/5723], Loss: 0.756, Training Accuracy: 76.53%\n",
      "Epoch [2/13], Step [4200/5723], Loss: 0.726, Training Accuracy: 73.47%\n",
      "Epoch [2/13], Step [4300/5723], Loss: 0.807, Training Accuracy: 64.29%\n",
      "Epoch [2/13], Step [4400/5723], Loss: 0.855, Training Accuracy: 72.45%\n",
      "Epoch [2/13], Step [4500/5723], Loss: 0.539, Training Accuracy: 76.53%\n",
      "Epoch [2/13], Step [4600/5723], Loss: 0.520, Training Accuracy: 78.57%\n",
      "Epoch [2/13], Step [4700/5723], Loss: 0.801, Training Accuracy: 71.43%\n",
      "Epoch [2/13], Step [4800/5723], Loss: 0.602, Training Accuracy: 80.61%\n",
      "Epoch [2/13], Step [4900/5723], Loss: 0.617, Training Accuracy: 78.57%\n",
      "Epoch [2/13], Step [5000/5723], Loss: 0.783, Training Accuracy: 70.41%\n",
      "Epoch [2/13], Step [5100/5723], Loss: 0.704, Training Accuracy: 74.49%\n",
      "Epoch [2/13], Step [5200/5723], Loss: 0.638, Training Accuracy: 73.47%\n",
      "Epoch [2/13], Step [5300/5723], Loss: 0.775, Training Accuracy: 68.37%\n",
      "Epoch [2/13], Step [5400/5723], Loss: 0.608, Training Accuracy: 84.69%\n",
      "Epoch [2/13], Step [5500/5723], Loss: 0.788, Training Accuracy: 67.35%\n",
      "Epoch [2/13], Step [5600/5723], Loss: 0.752, Training Accuracy: 75.51%\n",
      "Epoch [2/13], Step [5700/5723], Loss: 0.704, Training Accuracy: 74.49%\n",
      "Epoch [3/13], Step [100/5723], Loss: 0.604, Training Accuracy: 78.57%\n",
      "Epoch [3/13], Step [200/5723], Loss: 0.687, Training Accuracy: 72.45%\n",
      "Epoch [3/13], Step [300/5723], Loss: 0.627, Training Accuracy: 70.41%\n",
      "Epoch [3/13], Step [400/5723], Loss: 0.719, Training Accuracy: 65.31%\n",
      "Epoch [3/13], Step [500/5723], Loss: 0.615, Training Accuracy: 79.59%\n",
      "Epoch [3/13], Step [600/5723], Loss: 0.594, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [700/5723], Loss: 0.413, Training Accuracy: 85.71%\n",
      "Epoch [3/13], Step [800/5723], Loss: 0.958, Training Accuracy: 71.43%\n",
      "Epoch [3/13], Step [900/5723], Loss: 0.681, Training Accuracy: 74.49%\n",
      "Epoch [3/13], Step [1000/5723], Loss: 0.591, Training Accuracy: 79.59%\n",
      "Epoch [3/13], Step [1100/5723], Loss: 0.765, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [1200/5723], Loss: 0.598, Training Accuracy: 76.53%\n",
      "Epoch [3/13], Step [1300/5723], Loss: 0.724, Training Accuracy: 74.49%\n",
      "Epoch [3/13], Step [1400/5723], Loss: 0.813, Training Accuracy: 69.39%\n",
      "Epoch [3/13], Step [1500/5723], Loss: 0.700, Training Accuracy: 84.69%\n",
      "Epoch [3/13], Step [1600/5723], Loss: 0.600, Training Accuracy: 76.53%\n",
      "Epoch [3/13], Step [1700/5723], Loss: 0.582, Training Accuracy: 77.55%\n",
      "Epoch [3/13], Step [1800/5723], Loss: 0.727, Training Accuracy: 74.49%\n",
      "Epoch [3/13], Step [1900/5723], Loss: 0.744, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [2000/5723], Loss: 0.613, Training Accuracy: 75.51%\n",
      "Epoch [3/13], Step [2100/5723], Loss: 0.620, Training Accuracy: 81.63%\n",
      "Epoch [3/13], Step [2200/5723], Loss: 0.690, Training Accuracy: 75.51%\n",
      "Epoch [3/13], Step [2300/5723], Loss: 0.732, Training Accuracy: 67.35%\n",
      "Epoch [3/13], Step [2400/5723], Loss: 0.489, Training Accuracy: 77.55%\n",
      "Epoch [3/13], Step [2500/5723], Loss: 0.778, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [2600/5723], Loss: 0.667, Training Accuracy: 72.45%\n",
      "Epoch [3/13], Step [2700/5723], Loss: 0.646, Training Accuracy: 71.43%\n",
      "Epoch [3/13], Step [2800/5723], Loss: 0.713, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [2900/5723], Loss: 0.524, Training Accuracy: 77.55%\n",
      "Epoch [3/13], Step [3000/5723], Loss: 0.789, Training Accuracy: 77.55%\n",
      "Epoch [3/13], Step [3100/5723], Loss: 0.829, Training Accuracy: 74.49%\n",
      "Epoch [3/13], Step [3200/5723], Loss: 0.608, Training Accuracy: 79.59%\n",
      "Epoch [3/13], Step [3300/5723], Loss: 0.900, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [3400/5723], Loss: 0.779, Training Accuracy: 69.39%\n",
      "Epoch [3/13], Step [3500/5723], Loss: 0.553, Training Accuracy: 75.51%\n",
      "Epoch [3/13], Step [3600/5723], Loss: 0.678, Training Accuracy: 74.49%\n",
      "Epoch [3/13], Step [3700/5723], Loss: 0.575, Training Accuracy: 78.57%\n",
      "Epoch [3/13], Step [3800/5723], Loss: 0.605, Training Accuracy: 70.41%\n",
      "Epoch [3/13], Step [3900/5723], Loss: 0.420, Training Accuracy: 88.78%\n",
      "Epoch [3/13], Step [4000/5723], Loss: 0.824, Training Accuracy: 71.43%\n",
      "Epoch [3/13], Step [4100/5723], Loss: 0.633, Training Accuracy: 78.57%\n",
      "Epoch [3/13], Step [4200/5723], Loss: 0.496, Training Accuracy: 79.59%\n",
      "Epoch [3/13], Step [4300/5723], Loss: 0.588, Training Accuracy: 79.59%\n",
      "Epoch [3/13], Step [4400/5723], Loss: 0.799, Training Accuracy: 72.45%\n",
      "Epoch [3/13], Step [4500/5723], Loss: 0.690, Training Accuracy: 72.45%\n",
      "Epoch [3/13], Step [4600/5723], Loss: 0.577, Training Accuracy: 76.53%\n",
      "Epoch [3/13], Step [4700/5723], Loss: 0.496, Training Accuracy: 87.76%\n",
      "Epoch [3/13], Step [4800/5723], Loss: 0.594, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [4900/5723], Loss: 0.552, Training Accuracy: 82.65%\n",
      "Epoch [3/13], Step [5000/5723], Loss: 0.522, Training Accuracy: 78.57%\n",
      "Epoch [3/13], Step [5100/5723], Loss: 0.492, Training Accuracy: 78.57%\n",
      "Epoch [3/13], Step [5200/5723], Loss: 0.749, Training Accuracy: 73.47%\n",
      "Epoch [3/13], Step [5300/5723], Loss: 0.533, Training Accuracy: 87.76%\n",
      "Epoch [3/13], Step [5400/5723], Loss: 0.602, Training Accuracy: 77.55%\n",
      "Epoch [3/13], Step [5500/5723], Loss: 0.530, Training Accuracy: 83.67%\n",
      "Epoch [3/13], Step [5600/5723], Loss: 0.651, Training Accuracy: 79.59%\n",
      "Epoch [3/13], Step [5700/5723], Loss: 0.411, Training Accuracy: 84.69%\n",
      "Epoch [4/13], Step [100/5723], Loss: 0.597, Training Accuracy: 70.41%\n",
      "Epoch [4/13], Step [200/5723], Loss: 0.572, Training Accuracy: 80.61%\n",
      "Epoch [4/13], Step [300/5723], Loss: 0.560, Training Accuracy: 87.76%\n",
      "Epoch [4/13], Step [400/5723], Loss: 0.759, Training Accuracy: 74.49%\n",
      "Epoch [4/13], Step [500/5723], Loss: 0.663, Training Accuracy: 78.57%\n",
      "Epoch [4/13], Step [600/5723], Loss: 0.579, Training Accuracy: 78.57%\n",
      "Epoch [4/13], Step [700/5723], Loss: 0.615, Training Accuracy: 78.57%\n",
      "Epoch [4/13], Step [800/5723], Loss: 0.463, Training Accuracy: 80.61%\n",
      "Epoch [4/13], Step [900/5723], Loss: 0.495, Training Accuracy: 81.63%\n",
      "Epoch [4/13], Step [1000/5723], Loss: 0.629, Training Accuracy: 79.59%\n",
      "Epoch [4/13], Step [1100/5723], Loss: 0.633, Training Accuracy: 76.53%\n",
      "Epoch [4/13], Step [1200/5723], Loss: 0.548, Training Accuracy: 84.69%\n",
      "Epoch [4/13], Step [1300/5723], Loss: 0.600, Training Accuracy: 79.59%\n",
      "Epoch [4/13], Step [1400/5723], Loss: 0.461, Training Accuracy: 82.65%\n",
      "Epoch [4/13], Step [1500/5723], Loss: 0.593, Training Accuracy: 76.53%\n",
      "Epoch [4/13], Step [1600/5723], Loss: 0.748, Training Accuracy: 78.57%\n",
      "Epoch [4/13], Step [1700/5723], Loss: 0.644, Training Accuracy: 76.53%\n",
      "Epoch [4/13], Step [1800/5723], Loss: 0.659, Training Accuracy: 80.61%\n",
      "Epoch [4/13], Step [1900/5723], Loss: 0.667, Training Accuracy: 82.65%\n",
      "Epoch [4/13], Step [2000/5723], Loss: 0.816, Training Accuracy: 73.47%\n",
      "Epoch [4/13], Step [2100/5723], Loss: 0.488, Training Accuracy: 76.53%\n",
      "Epoch [4/13], Step [2200/5723], Loss: 0.612, Training Accuracy: 77.55%\n",
      "Epoch [4/13], Step [2300/5723], Loss: 0.768, Training Accuracy: 65.31%\n",
      "Epoch [4/13], Step [2400/5723], Loss: 0.531, Training Accuracy: 84.69%\n",
      "Epoch [4/13], Step [2500/5723], Loss: 0.591, Training Accuracy: 76.53%\n",
      "Epoch [4/13], Step [2600/5723], Loss: 0.577, Training Accuracy: 78.57%\n",
      "Epoch [4/13], Step [2700/5723], Loss: 0.657, Training Accuracy: 77.55%\n",
      "Epoch [4/13], Step [2800/5723], Loss: 0.588, Training Accuracy: 77.55%\n",
      "Epoch [4/13], Step [2900/5723], Loss: 0.722, Training Accuracy: 73.47%\n",
      "Epoch [4/13], Step [3000/5723], Loss: 0.698, Training Accuracy: 75.51%\n",
      "Epoch [4/13], Step [3100/5723], Loss: 0.530, Training Accuracy: 76.53%\n",
      "Epoch [4/13], Step [3200/5723], Loss: 0.459, Training Accuracy: 84.69%\n",
      "Epoch [4/13], Step [3300/5723], Loss: 0.657, Training Accuracy: 77.55%\n",
      "Epoch [4/13], Step [3400/5723], Loss: 0.703, Training Accuracy: 68.37%\n",
      "Epoch [4/13], Step [3500/5723], Loss: 0.818, Training Accuracy: 73.47%\n",
      "Epoch [4/13], Step [3600/5723], Loss: 0.371, Training Accuracy: 85.71%\n",
      "Epoch [4/13], Step [3700/5723], Loss: 0.671, Training Accuracy: 78.57%\n",
      "Epoch [4/13], Step [3800/5723], Loss: 0.551, Training Accuracy: 80.61%\n",
      "Epoch [4/13], Step [3900/5723], Loss: 0.532, Training Accuracy: 82.65%\n",
      "Epoch [4/13], Step [4000/5723], Loss: 0.527, Training Accuracy: 76.53%\n",
      "Epoch [4/13], Step [4100/5723], Loss: 0.567, Training Accuracy: 79.59%\n",
      "Epoch [4/13], Step [4200/5723], Loss: 0.686, Training Accuracy: 72.45%\n",
      "Epoch [4/13], Step [4300/5723], Loss: 0.510, Training Accuracy: 77.55%\n",
      "Epoch [4/13], Step [4400/5723], Loss: 0.583, Training Accuracy: 74.49%\n",
      "Epoch [4/13], Step [4500/5723], Loss: 0.795, Training Accuracy: 69.39%\n",
      "Epoch [4/13], Step [4600/5723], Loss: 0.576, Training Accuracy: 72.45%\n",
      "Epoch [4/13], Step [4700/5723], Loss: 0.543, Training Accuracy: 83.67%\n",
      "Epoch [4/13], Step [4800/5723], Loss: 0.619, Training Accuracy: 77.55%\n",
      "Epoch [4/13], Step [4900/5723], Loss: 0.604, Training Accuracy: 71.43%\n",
      "Epoch [4/13], Step [5000/5723], Loss: 0.530, Training Accuracy: 80.61%\n",
      "Epoch [4/13], Step [5100/5723], Loss: 0.625, Training Accuracy: 79.59%\n",
      "Epoch [4/13], Step [5200/5723], Loss: 0.571, Training Accuracy: 79.59%\n",
      "Epoch [4/13], Step [5300/5723], Loss: 0.641, Training Accuracy: 75.51%\n",
      "Epoch [4/13], Step [5400/5723], Loss: 0.617, Training Accuracy: 75.51%\n",
      "Epoch [4/13], Step [5500/5723], Loss: 0.507, Training Accuracy: 80.61%\n",
      "Epoch [4/13], Step [5600/5723], Loss: 0.460, Training Accuracy: 85.71%\n",
      "Epoch [4/13], Step [5700/5723], Loss: 0.591, Training Accuracy: 76.53%\n",
      "Epoch [5/13], Step [100/5723], Loss: 0.522, Training Accuracy: 76.53%\n",
      "Epoch [5/13], Step [200/5723], Loss: 0.518, Training Accuracy: 87.76%\n",
      "Epoch [5/13], Step [300/5723], Loss: 0.527, Training Accuracy: 80.61%\n",
      "Epoch [5/13], Step [400/5723], Loss: 0.905, Training Accuracy: 70.41%\n",
      "Epoch [5/13], Step [500/5723], Loss: 0.631, Training Accuracy: 80.61%\n",
      "Epoch [5/13], Step [600/5723], Loss: 0.414, Training Accuracy: 84.69%\n",
      "Epoch [5/13], Step [700/5723], Loss: 0.622, Training Accuracy: 78.57%\n",
      "Epoch [5/13], Step [800/5723], Loss: 0.803, Training Accuracy: 69.39%\n",
      "Epoch [5/13], Step [900/5723], Loss: 0.764, Training Accuracy: 75.51%\n",
      "Epoch [5/13], Step [1000/5723], Loss: 0.582, Training Accuracy: 79.59%\n",
      "Epoch [5/13], Step [1100/5723], Loss: 0.681, Training Accuracy: 81.63%\n",
      "Epoch [5/13], Step [1200/5723], Loss: 0.474, Training Accuracy: 80.61%\n",
      "Epoch [5/13], Step [1300/5723], Loss: 0.669, Training Accuracy: 76.53%\n",
      "Epoch [5/13], Step [1400/5723], Loss: 0.651, Training Accuracy: 81.63%\n",
      "Epoch [5/13], Step [1500/5723], Loss: 0.688, Training Accuracy: 71.43%\n",
      "Epoch [5/13], Step [1600/5723], Loss: 0.440, Training Accuracy: 82.65%\n",
      "Epoch [5/13], Step [1700/5723], Loss: 0.548, Training Accuracy: 78.57%\n",
      "Epoch [5/13], Step [1800/5723], Loss: 0.597, Training Accuracy: 73.47%\n",
      "Epoch [5/13], Step [1900/5723], Loss: 0.484, Training Accuracy: 83.67%\n",
      "Epoch [5/13], Step [2000/5723], Loss: 0.788, Training Accuracy: 77.55%\n",
      "Epoch [5/13], Step [2100/5723], Loss: 0.459, Training Accuracy: 77.55%\n",
      "Epoch [5/13], Step [2200/5723], Loss: 0.694, Training Accuracy: 72.45%\n",
      "Epoch [5/13], Step [2300/5723], Loss: 0.463, Training Accuracy: 78.57%\n",
      "Epoch [5/13], Step [2400/5723], Loss: 0.644, Training Accuracy: 78.57%\n",
      "Epoch [5/13], Step [2500/5723], Loss: 0.690, Training Accuracy: 75.51%\n",
      "Epoch [5/13], Step [2600/5723], Loss: 0.442, Training Accuracy: 85.71%\n",
      "Epoch [5/13], Step [2700/5723], Loss: 0.682, Training Accuracy: 76.53%\n",
      "Epoch [5/13], Step [2800/5723], Loss: 0.578, Training Accuracy: 79.59%\n",
      "Epoch [5/13], Step [2900/5723], Loss: 0.685, Training Accuracy: 72.45%\n",
      "Epoch [5/13], Step [3000/5723], Loss: 0.821, Training Accuracy: 75.51%\n",
      "Epoch [5/13], Step [3100/5723], Loss: 0.736, Training Accuracy: 74.49%\n",
      "Epoch [5/13], Step [3200/5723], Loss: 0.614, Training Accuracy: 70.41%\n",
      "Epoch [5/13], Step [3300/5723], Loss: 0.647, Training Accuracy: 77.55%\n",
      "Epoch [5/13], Step [3400/5723], Loss: 0.540, Training Accuracy: 81.63%\n",
      "Epoch [5/13], Step [3500/5723], Loss: 0.816, Training Accuracy: 74.49%\n",
      "Epoch [5/13], Step [3600/5723], Loss: 0.478, Training Accuracy: 83.67%\n",
      "Epoch [5/13], Step [3700/5723], Loss: 0.487, Training Accuracy: 82.65%\n",
      "Epoch [5/13], Step [3800/5723], Loss: 0.578, Training Accuracy: 77.55%\n",
      "Epoch [5/13], Step [3900/5723], Loss: 0.568, Training Accuracy: 80.61%\n",
      "Epoch [5/13], Step [4000/5723], Loss: 0.542, Training Accuracy: 74.49%\n",
      "Epoch [5/13], Step [4100/5723], Loss: 0.637, Training Accuracy: 75.51%\n",
      "Epoch [5/13], Step [4200/5723], Loss: 0.544, Training Accuracy: 77.55%\n",
      "Epoch [5/13], Step [4300/5723], Loss: 0.447, Training Accuracy: 84.69%\n",
      "Epoch [5/13], Step [4400/5723], Loss: 0.648, Training Accuracy: 75.51%\n",
      "Epoch [5/13], Step [4500/5723], Loss: 0.597, Training Accuracy: 71.43%\n",
      "Epoch [5/13], Step [4600/5723], Loss: 0.369, Training Accuracy: 84.69%\n",
      "Epoch [5/13], Step [4700/5723], Loss: 0.535, Training Accuracy: 75.51%\n",
      "Epoch [5/13], Step [4800/5723], Loss: 0.566, Training Accuracy: 77.55%\n",
      "Epoch [5/13], Step [4900/5723], Loss: 0.524, Training Accuracy: 80.61%\n",
      "Epoch [5/13], Step [5000/5723], Loss: 0.466, Training Accuracy: 77.55%\n",
      "Epoch [5/13], Step [5100/5723], Loss: 0.638, Training Accuracy: 76.53%\n",
      "Epoch [5/13], Step [5200/5723], Loss: 0.468, Training Accuracy: 83.67%\n",
      "Epoch [5/13], Step [5300/5723], Loss: 0.686, Training Accuracy: 82.65%\n",
      "Epoch [5/13], Step [5400/5723], Loss: 0.566, Training Accuracy: 76.53%\n",
      "Epoch [5/13], Step [5500/5723], Loss: 0.845, Training Accuracy: 70.41%\n",
      "Epoch [5/13], Step [5600/5723], Loss: 0.742, Training Accuracy: 70.41%\n",
      "Epoch [5/13], Step [5700/5723], Loss: 0.548, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [100/5723], Loss: 0.459, Training Accuracy: 78.57%\n",
      "Epoch [6/13], Step [200/5723], Loss: 0.567, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [300/5723], Loss: 0.451, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [400/5723], Loss: 0.554, Training Accuracy: 84.69%\n",
      "Epoch [6/13], Step [500/5723], Loss: 0.553, Training Accuracy: 77.55%\n",
      "Epoch [6/13], Step [600/5723], Loss: 0.427, Training Accuracy: 86.73%\n",
      "Epoch [6/13], Step [700/5723], Loss: 0.759, Training Accuracy: 76.53%\n",
      "Epoch [6/13], Step [800/5723], Loss: 0.606, Training Accuracy: 74.49%\n",
      "Epoch [6/13], Step [900/5723], Loss: 0.493, Training Accuracy: 85.71%\n",
      "Epoch [6/13], Step [1000/5723], Loss: 0.598, Training Accuracy: 78.57%\n",
      "Epoch [6/13], Step [1100/5723], Loss: 0.652, Training Accuracy: 77.55%\n",
      "Epoch [6/13], Step [1200/5723], Loss: 0.644, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [1300/5723], Loss: 0.638, Training Accuracy: 82.65%\n",
      "Epoch [6/13], Step [1400/5723], Loss: 0.440, Training Accuracy: 78.57%\n",
      "Epoch [6/13], Step [1500/5723], Loss: 0.557, Training Accuracy: 76.53%\n",
      "Epoch [6/13], Step [1600/5723], Loss: 0.599, Training Accuracy: 76.53%\n",
      "Epoch [6/13], Step [1700/5723], Loss: 0.674, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [1800/5723], Loss: 0.551, Training Accuracy: 75.51%\n",
      "Epoch [6/13], Step [1900/5723], Loss: 0.567, Training Accuracy: 70.41%\n",
      "Epoch [6/13], Step [2000/5723], Loss: 0.610, Training Accuracy: 76.53%\n",
      "Epoch [6/13], Step [2100/5723], Loss: 0.634, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [2200/5723], Loss: 0.561, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [2300/5723], Loss: 0.648, Training Accuracy: 74.49%\n",
      "Epoch [6/13], Step [2400/5723], Loss: 0.436, Training Accuracy: 87.76%\n",
      "Epoch [6/13], Step [2500/5723], Loss: 0.681, Training Accuracy: 73.47%\n",
      "Epoch [6/13], Step [2600/5723], Loss: 0.674, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [2700/5723], Loss: 0.897, Training Accuracy: 71.43%\n",
      "Epoch [6/13], Step [2800/5723], Loss: 0.581, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [2900/5723], Loss: 0.409, Training Accuracy: 85.71%\n",
      "Epoch [6/13], Step [3000/5723], Loss: 0.553, Training Accuracy: 78.57%\n",
      "Epoch [6/13], Step [3100/5723], Loss: 0.622, Training Accuracy: 77.55%\n",
      "Epoch [6/13], Step [3200/5723], Loss: 0.640, Training Accuracy: 75.51%\n",
      "Epoch [6/13], Step [3300/5723], Loss: 0.605, Training Accuracy: 82.65%\n",
      "Epoch [6/13], Step [3400/5723], Loss: 0.772, Training Accuracy: 72.45%\n",
      "Epoch [6/13], Step [3500/5723], Loss: 0.641, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [3600/5723], Loss: 0.630, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [3700/5723], Loss: 0.732, Training Accuracy: 76.53%\n",
      "Epoch [6/13], Step [3800/5723], Loss: 0.645, Training Accuracy: 74.49%\n",
      "Epoch [6/13], Step [3900/5723], Loss: 0.641, Training Accuracy: 72.45%\n",
      "Epoch [6/13], Step [4000/5723], Loss: 0.630, Training Accuracy: 71.43%\n",
      "Epoch [6/13], Step [4100/5723], Loss: 0.692, Training Accuracy: 71.43%\n",
      "Epoch [6/13], Step [4200/5723], Loss: 0.545, Training Accuracy: 80.61%\n",
      "Epoch [6/13], Step [4300/5723], Loss: 0.620, Training Accuracy: 77.55%\n",
      "Epoch [6/13], Step [4400/5723], Loss: 0.491, Training Accuracy: 84.69%\n",
      "Epoch [6/13], Step [4500/5723], Loss: 0.420, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [4600/5723], Loss: 0.380, Training Accuracy: 85.71%\n",
      "Epoch [6/13], Step [4700/5723], Loss: 0.785, Training Accuracy: 74.49%\n",
      "Epoch [6/13], Step [4800/5723], Loss: 0.590, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [4900/5723], Loss: 0.642, Training Accuracy: 81.63%\n",
      "Epoch [6/13], Step [5000/5723], Loss: 0.788, Training Accuracy: 82.65%\n",
      "Epoch [6/13], Step [5100/5723], Loss: 0.582, Training Accuracy: 80.61%\n",
      "Epoch [6/13], Step [5200/5723], Loss: 0.540, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [5300/5723], Loss: 0.450, Training Accuracy: 84.69%\n",
      "Epoch [6/13], Step [5400/5723], Loss: 0.660, Training Accuracy: 79.59%\n",
      "Epoch [6/13], Step [5500/5723], Loss: 0.531, Training Accuracy: 83.67%\n",
      "Epoch [6/13], Step [5600/5723], Loss: 0.495, Training Accuracy: 82.65%\n",
      "Epoch [6/13], Step [5700/5723], Loss: 0.509, Training Accuracy: 76.53%\n",
      "Epoch [7/13], Step [100/5723], Loss: 0.835, Training Accuracy: 77.55%\n",
      "Epoch [7/13], Step [200/5723], Loss: 0.604, Training Accuracy: 77.55%\n",
      "Epoch [7/13], Step [300/5723], Loss: 0.690, Training Accuracy: 71.43%\n",
      "Epoch [7/13], Step [400/5723], Loss: 0.516, Training Accuracy: 79.59%\n",
      "Epoch [7/13], Step [500/5723], Loss: 0.594, Training Accuracy: 77.55%\n",
      "Epoch [7/13], Step [600/5723], Loss: 0.607, Training Accuracy: 76.53%\n",
      "Epoch [7/13], Step [700/5723], Loss: 0.626, Training Accuracy: 75.51%\n",
      "Epoch [7/13], Step [800/5723], Loss: 0.640, Training Accuracy: 82.65%\n",
      "Epoch [7/13], Step [900/5723], Loss: 0.532, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [1000/5723], Loss: 0.616, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [1100/5723], Loss: 0.671, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [1200/5723], Loss: 0.415, Training Accuracy: 80.61%\n",
      "Epoch [7/13], Step [1300/5723], Loss: 0.718, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [1400/5723], Loss: 0.771, Training Accuracy: 73.47%\n",
      "Epoch [7/13], Step [1500/5723], Loss: 0.558, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [1600/5723], Loss: 0.478, Training Accuracy: 75.51%\n",
      "Epoch [7/13], Step [1700/5723], Loss: 0.491, Training Accuracy: 84.69%\n",
      "Epoch [7/13], Step [1800/5723], Loss: 0.606, Training Accuracy: 76.53%\n",
      "Epoch [7/13], Step [1900/5723], Loss: 0.729, Training Accuracy: 73.47%\n",
      "Epoch [7/13], Step [2000/5723], Loss: 0.759, Training Accuracy: 73.47%\n",
      "Epoch [7/13], Step [2100/5723], Loss: 0.543, Training Accuracy: 73.47%\n",
      "Epoch [7/13], Step [2200/5723], Loss: 0.578, Training Accuracy: 79.59%\n",
      "Epoch [7/13], Step [2300/5723], Loss: 0.545, Training Accuracy: 82.65%\n",
      "Epoch [7/13], Step [2400/5723], Loss: 0.390, Training Accuracy: 81.63%\n",
      "Epoch [7/13], Step [2500/5723], Loss: 0.653, Training Accuracy: 71.43%\n",
      "Epoch [7/13], Step [2600/5723], Loss: 0.654, Training Accuracy: 81.63%\n",
      "Epoch [7/13], Step [2700/5723], Loss: 0.608, Training Accuracy: 76.53%\n",
      "Epoch [7/13], Step [2800/5723], Loss: 0.628, Training Accuracy: 75.51%\n",
      "Epoch [7/13], Step [2900/5723], Loss: 0.403, Training Accuracy: 82.65%\n",
      "Epoch [7/13], Step [3000/5723], Loss: 0.527, Training Accuracy: 75.51%\n",
      "Epoch [7/13], Step [3100/5723], Loss: 0.592, Training Accuracy: 76.53%\n",
      "Epoch [7/13], Step [3200/5723], Loss: 0.425, Training Accuracy: 86.73%\n",
      "Epoch [7/13], Step [3300/5723], Loss: 0.399, Training Accuracy: 82.65%\n",
      "Epoch [7/13], Step [3400/5723], Loss: 0.674, Training Accuracy: 73.47%\n",
      "Epoch [7/13], Step [3500/5723], Loss: 0.530, Training Accuracy: 83.67%\n",
      "Epoch [7/13], Step [3600/5723], Loss: 0.556, Training Accuracy: 77.55%\n",
      "Epoch [7/13], Step [3700/5723], Loss: 0.433, Training Accuracy: 82.65%\n",
      "Epoch [7/13], Step [3800/5723], Loss: 0.824, Training Accuracy: 70.41%\n",
      "Epoch [7/13], Step [3900/5723], Loss: 0.644, Training Accuracy: 75.51%\n",
      "Epoch [7/13], Step [4000/5723], Loss: 0.537, Training Accuracy: 86.73%\n",
      "Epoch [7/13], Step [4100/5723], Loss: 0.472, Training Accuracy: 80.61%\n",
      "Epoch [7/13], Step [4200/5723], Loss: 0.542, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [4300/5723], Loss: 0.571, Training Accuracy: 83.67%\n",
      "Epoch [7/13], Step [4400/5723], Loss: 0.747, Training Accuracy: 79.59%\n",
      "Epoch [7/13], Step [4500/5723], Loss: 0.560, Training Accuracy: 84.69%\n",
      "Epoch [7/13], Step [4600/5723], Loss: 0.514, Training Accuracy: 80.61%\n",
      "Epoch [7/13], Step [4700/5723], Loss: 0.668, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [4800/5723], Loss: 0.602, Training Accuracy: 72.45%\n",
      "Epoch [7/13], Step [4900/5723], Loss: 0.451, Training Accuracy: 80.61%\n",
      "Epoch [7/13], Step [5000/5723], Loss: 0.640, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [5100/5723], Loss: 0.644, Training Accuracy: 79.59%\n",
      "Epoch [7/13], Step [5200/5723], Loss: 0.493, Training Accuracy: 81.63%\n",
      "Epoch [7/13], Step [5300/5723], Loss: 0.731, Training Accuracy: 70.41%\n",
      "Epoch [7/13], Step [5400/5723], Loss: 0.548, Training Accuracy: 80.61%\n",
      "Epoch [7/13], Step [5500/5723], Loss: 0.564, Training Accuracy: 79.59%\n",
      "Epoch [7/13], Step [5600/5723], Loss: 0.667, Training Accuracy: 78.57%\n",
      "Epoch [7/13], Step [5700/5723], Loss: 0.455, Training Accuracy: 79.59%\n",
      "Epoch [8/13], Step [100/5723], Loss: 0.565, Training Accuracy: 72.45%\n",
      "Epoch [8/13], Step [200/5723], Loss: 0.544, Training Accuracy: 82.65%\n",
      "Epoch [8/13], Step [300/5723], Loss: 0.427, Training Accuracy: 82.65%\n",
      "Epoch [8/13], Step [400/5723], Loss: 0.475, Training Accuracy: 79.59%\n",
      "Epoch [8/13], Step [500/5723], Loss: 0.440, Training Accuracy: 81.63%\n",
      "Epoch [8/13], Step [600/5723], Loss: 0.405, Training Accuracy: 84.69%\n",
      "Epoch [8/13], Step [700/5723], Loss: 0.490, Training Accuracy: 79.59%\n",
      "Epoch [8/13], Step [800/5723], Loss: 0.614, Training Accuracy: 76.53%\n",
      "Epoch [8/13], Step [900/5723], Loss: 0.442, Training Accuracy: 77.55%\n",
      "Epoch [8/13], Step [1000/5723], Loss: 0.553, Training Accuracy: 74.49%\n",
      "Epoch [8/13], Step [1100/5723], Loss: 0.384, Training Accuracy: 82.65%\n",
      "Epoch [8/13], Step [1200/5723], Loss: 0.595, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [1300/5723], Loss: 0.534, Training Accuracy: 77.55%\n",
      "Epoch [8/13], Step [1400/5723], Loss: 0.517, Training Accuracy: 85.71%\n",
      "Epoch [8/13], Step [1500/5723], Loss: 0.504, Training Accuracy: 88.78%\n",
      "Epoch [8/13], Step [1600/5723], Loss: 0.601, Training Accuracy: 79.59%\n",
      "Epoch [8/13], Step [1700/5723], Loss: 0.552, Training Accuracy: 72.45%\n",
      "Epoch [8/13], Step [1800/5723], Loss: 0.510, Training Accuracy: 83.67%\n",
      "Epoch [8/13], Step [1900/5723], Loss: 0.740, Training Accuracy: 77.55%\n",
      "Epoch [8/13], Step [2000/5723], Loss: 0.717, Training Accuracy: 76.53%\n",
      "Epoch [8/13], Step [2100/5723], Loss: 0.586, Training Accuracy: 76.53%\n",
      "Epoch [8/13], Step [2200/5723], Loss: 0.365, Training Accuracy: 82.65%\n",
      "Epoch [8/13], Step [2300/5723], Loss: 0.546, Training Accuracy: 74.49%\n",
      "Epoch [8/13], Step [2400/5723], Loss: 0.500, Training Accuracy: 80.61%\n",
      "Epoch [8/13], Step [2500/5723], Loss: 0.556, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [2600/5723], Loss: 0.474, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [2700/5723], Loss: 0.503, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [2800/5723], Loss: 0.560, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [2900/5723], Loss: 0.384, Training Accuracy: 88.78%\n",
      "Epoch [8/13], Step [3000/5723], Loss: 0.474, Training Accuracy: 83.67%\n",
      "Epoch [8/13], Step [3100/5723], Loss: 0.437, Training Accuracy: 85.71%\n",
      "Epoch [8/13], Step [3200/5723], Loss: 0.797, Training Accuracy: 75.51%\n",
      "Epoch [8/13], Step [3300/5723], Loss: 0.673, Training Accuracy: 77.55%\n",
      "Epoch [8/13], Step [3400/5723], Loss: 0.576, Training Accuracy: 80.61%\n",
      "Epoch [8/13], Step [3500/5723], Loss: 0.621, Training Accuracy: 82.65%\n",
      "Epoch [8/13], Step [3600/5723], Loss: 0.718, Training Accuracy: 77.55%\n",
      "Epoch [8/13], Step [3700/5723], Loss: 0.832, Training Accuracy: 74.49%\n",
      "Epoch [8/13], Step [3800/5723], Loss: 0.523, Training Accuracy: 71.43%\n",
      "Epoch [8/13], Step [3900/5723], Loss: 0.435, Training Accuracy: 84.69%\n",
      "Epoch [8/13], Step [4000/5723], Loss: 0.466, Training Accuracy: 83.67%\n",
      "Epoch [8/13], Step [4100/5723], Loss: 0.525, Training Accuracy: 76.53%\n",
      "Epoch [8/13], Step [4200/5723], Loss: 0.665, Training Accuracy: 71.43%\n",
      "Epoch [8/13], Step [4300/5723], Loss: 0.566, Training Accuracy: 76.53%\n",
      "Epoch [8/13], Step [4400/5723], Loss: 0.519, Training Accuracy: 82.65%\n",
      "Epoch [8/13], Step [4500/5723], Loss: 0.571, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [4600/5723], Loss: 0.430, Training Accuracy: 81.63%\n",
      "Epoch [8/13], Step [4700/5723], Loss: 0.394, Training Accuracy: 86.73%\n",
      "Epoch [8/13], Step [4800/5723], Loss: 0.799, Training Accuracy: 76.53%\n",
      "Epoch [8/13], Step [4900/5723], Loss: 0.607, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [5000/5723], Loss: 0.791, Training Accuracy: 76.53%\n",
      "Epoch [8/13], Step [5100/5723], Loss: 0.589, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [5200/5723], Loss: 0.457, Training Accuracy: 84.69%\n",
      "Epoch [8/13], Step [5300/5723], Loss: 0.558, Training Accuracy: 79.59%\n",
      "Epoch [8/13], Step [5400/5723], Loss: 0.459, Training Accuracy: 78.57%\n",
      "Epoch [8/13], Step [5500/5723], Loss: 0.525, Training Accuracy: 74.49%\n",
      "Epoch [8/13], Step [5600/5723], Loss: 0.584, Training Accuracy: 81.63%\n",
      "Epoch [8/13], Step [5700/5723], Loss: 0.468, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [100/5723], Loss: 0.680, Training Accuracy: 77.55%\n",
      "Epoch [9/13], Step [200/5723], Loss: 0.555, Training Accuracy: 85.71%\n",
      "Epoch [9/13], Step [300/5723], Loss: 0.583, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [400/5723], Loss: 0.603, Training Accuracy: 71.43%\n",
      "Epoch [9/13], Step [500/5723], Loss: 0.503, Training Accuracy: 79.59%\n",
      "Epoch [9/13], Step [600/5723], Loss: 0.696, Training Accuracy: 75.51%\n",
      "Epoch [9/13], Step [700/5723], Loss: 0.760, Training Accuracy: 72.45%\n",
      "Epoch [9/13], Step [800/5723], Loss: 0.636, Training Accuracy: 75.51%\n",
      "Epoch [9/13], Step [900/5723], Loss: 0.400, Training Accuracy: 83.67%\n",
      "Epoch [9/13], Step [1000/5723], Loss: 0.594, Training Accuracy: 75.51%\n",
      "Epoch [9/13], Step [1100/5723], Loss: 0.389, Training Accuracy: 83.67%\n",
      "Epoch [9/13], Step [1200/5723], Loss: 0.341, Training Accuracy: 86.73%\n",
      "Epoch [9/13], Step [1300/5723], Loss: 0.571, Training Accuracy: 83.67%\n",
      "Epoch [9/13], Step [1400/5723], Loss: 0.457, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [1500/5723], Loss: 0.525, Training Accuracy: 79.59%\n",
      "Epoch [9/13], Step [1600/5723], Loss: 0.496, Training Accuracy: 78.57%\n",
      "Epoch [9/13], Step [1700/5723], Loss: 0.626, Training Accuracy: 84.69%\n",
      "Epoch [9/13], Step [1800/5723], Loss: 0.499, Training Accuracy: 84.69%\n",
      "Epoch [9/13], Step [1900/5723], Loss: 0.596, Training Accuracy: 74.49%\n",
      "Epoch [9/13], Step [2000/5723], Loss: 0.480, Training Accuracy: 81.63%\n",
      "Epoch [9/13], Step [2100/5723], Loss: 0.611, Training Accuracy: 81.63%\n",
      "Epoch [9/13], Step [2200/5723], Loss: 0.497, Training Accuracy: 78.57%\n",
      "Epoch [9/13], Step [2300/5723], Loss: 0.564, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [2400/5723], Loss: 0.506, Training Accuracy: 81.63%\n",
      "Epoch [9/13], Step [2500/5723], Loss: 0.546, Training Accuracy: 78.57%\n",
      "Epoch [9/13], Step [2600/5723], Loss: 0.604, Training Accuracy: 78.57%\n",
      "Epoch [9/13], Step [2700/5723], Loss: 0.351, Training Accuracy: 87.76%\n",
      "Epoch [9/13], Step [2800/5723], Loss: 0.706, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [2900/5723], Loss: 0.456, Training Accuracy: 82.65%\n",
      "Epoch [9/13], Step [3000/5723], Loss: 0.619, Training Accuracy: 79.59%\n",
      "Epoch [9/13], Step [3100/5723], Loss: 0.619, Training Accuracy: 75.51%\n",
      "Epoch [9/13], Step [3200/5723], Loss: 0.420, Training Accuracy: 76.53%\n",
      "Epoch [9/13], Step [3300/5723], Loss: 0.566, Training Accuracy: 72.45%\n",
      "Epoch [9/13], Step [3400/5723], Loss: 0.634, Training Accuracy: 70.41%\n",
      "Epoch [9/13], Step [3500/5723], Loss: 0.351, Training Accuracy: 85.71%\n",
      "Epoch [9/13], Step [3600/5723], Loss: 0.364, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [3700/5723], Loss: 0.534, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [3800/5723], Loss: 0.646, Training Accuracy: 72.45%\n",
      "Epoch [9/13], Step [3900/5723], Loss: 0.573, Training Accuracy: 79.59%\n",
      "Epoch [9/13], Step [4000/5723], Loss: 0.624, Training Accuracy: 74.49%\n",
      "Epoch [9/13], Step [4100/5723], Loss: 0.431, Training Accuracy: 77.55%\n",
      "Epoch [9/13], Step [4200/5723], Loss: 0.538, Training Accuracy: 77.55%\n",
      "Epoch [9/13], Step [4300/5723], Loss: 0.547, Training Accuracy: 73.47%\n",
      "Epoch [9/13], Step [4400/5723], Loss: 0.537, Training Accuracy: 75.51%\n",
      "Epoch [9/13], Step [4500/5723], Loss: 0.437, Training Accuracy: 81.63%\n",
      "Epoch [9/13], Step [4600/5723], Loss: 0.511, Training Accuracy: 84.69%\n",
      "Epoch [9/13], Step [4700/5723], Loss: 0.581, Training Accuracy: 76.53%\n",
      "Epoch [9/13], Step [4800/5723], Loss: 0.358, Training Accuracy: 90.82%\n",
      "Epoch [9/13], Step [4900/5723], Loss: 0.460, Training Accuracy: 82.65%\n",
      "Epoch [9/13], Step [5000/5723], Loss: 0.513, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [5100/5723], Loss: 0.464, Training Accuracy: 80.61%\n",
      "Epoch [9/13], Step [5200/5723], Loss: 0.704, Training Accuracy: 78.57%\n",
      "Epoch [9/13], Step [5300/5723], Loss: 0.569, Training Accuracy: 84.69%\n",
      "Epoch [9/13], Step [5400/5723], Loss: 0.409, Training Accuracy: 78.57%\n",
      "Epoch [9/13], Step [5500/5723], Loss: 0.615, Training Accuracy: 74.49%\n",
      "Epoch [9/13], Step [5600/5723], Loss: 0.584, Training Accuracy: 70.41%\n",
      "Epoch [9/13], Step [5700/5723], Loss: 0.481, Training Accuracy: 82.65%\n",
      "Epoch [10/13], Step [100/5723], Loss: 0.467, Training Accuracy: 84.69%\n",
      "Epoch [10/13], Step [200/5723], Loss: 0.509, Training Accuracy: 75.51%\n",
      "Epoch [10/13], Step [300/5723], Loss: 0.789, Training Accuracy: 75.51%\n",
      "Epoch [10/13], Step [400/5723], Loss: 0.423, Training Accuracy: 84.69%\n",
      "Epoch [10/13], Step [500/5723], Loss: 0.585, Training Accuracy: 75.51%\n",
      "Epoch [10/13], Step [600/5723], Loss: 0.710, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [700/5723], Loss: 0.535, Training Accuracy: 78.57%\n",
      "Epoch [10/13], Step [800/5723], Loss: 0.459, Training Accuracy: 77.55%\n",
      "Epoch [10/13], Step [900/5723], Loss: 0.529, Training Accuracy: 81.63%\n",
      "Epoch [10/13], Step [1000/5723], Loss: 0.475, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [1100/5723], Loss: 0.479, Training Accuracy: 82.65%\n",
      "Epoch [10/13], Step [1200/5723], Loss: 0.350, Training Accuracy: 83.67%\n",
      "Epoch [10/13], Step [1300/5723], Loss: 0.669, Training Accuracy: 85.71%\n",
      "Epoch [10/13], Step [1400/5723], Loss: 0.655, Training Accuracy: 80.61%\n",
      "Epoch [10/13], Step [1500/5723], Loss: 0.446, Training Accuracy: 88.78%\n",
      "Epoch [10/13], Step [1600/5723], Loss: 0.658, Training Accuracy: 74.49%\n",
      "Epoch [10/13], Step [1700/5723], Loss: 0.454, Training Accuracy: 87.76%\n",
      "Epoch [10/13], Step [1800/5723], Loss: 0.542, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [1900/5723], Loss: 0.430, Training Accuracy: 81.63%\n",
      "Epoch [10/13], Step [2000/5723], Loss: 0.468, Training Accuracy: 81.63%\n",
      "Epoch [10/13], Step [2100/5723], Loss: 0.383, Training Accuracy: 80.61%\n",
      "Epoch [10/13], Step [2200/5723], Loss: 0.465, Training Accuracy: 84.69%\n",
      "Epoch [10/13], Step [2300/5723], Loss: 0.572, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [2400/5723], Loss: 0.486, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [2500/5723], Loss: 0.417, Training Accuracy: 78.57%\n",
      "Epoch [10/13], Step [2600/5723], Loss: 0.606, Training Accuracy: 77.55%\n",
      "Epoch [10/13], Step [2700/5723], Loss: 0.441, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [2800/5723], Loss: 0.606, Training Accuracy: 76.53%\n",
      "Epoch [10/13], Step [2900/5723], Loss: 0.603, Training Accuracy: 74.49%\n",
      "Epoch [10/13], Step [3000/5723], Loss: 0.473, Training Accuracy: 75.51%\n",
      "Epoch [10/13], Step [3100/5723], Loss: 0.492, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [3200/5723], Loss: 0.682, Training Accuracy: 75.51%\n",
      "Epoch [10/13], Step [3300/5723], Loss: 0.397, Training Accuracy: 81.63%\n",
      "Epoch [10/13], Step [3400/5723], Loss: 0.471, Training Accuracy: 80.61%\n",
      "Epoch [10/13], Step [3500/5723], Loss: 0.519, Training Accuracy: 77.55%\n",
      "Epoch [10/13], Step [3600/5723], Loss: 0.516, Training Accuracy: 86.73%\n",
      "Epoch [10/13], Step [3700/5723], Loss: 0.659, Training Accuracy: 80.61%\n",
      "Epoch [10/13], Step [3800/5723], Loss: 0.621, Training Accuracy: 76.53%\n",
      "Epoch [10/13], Step [3900/5723], Loss: 0.442, Training Accuracy: 80.61%\n",
      "Epoch [10/13], Step [4000/5723], Loss: 0.554, Training Accuracy: 79.59%\n",
      "Epoch [10/13], Step [4100/5723], Loss: 0.375, Training Accuracy: 82.65%\n",
      "Epoch [10/13], Step [4200/5723], Loss: 0.466, Training Accuracy: 82.65%\n",
      "Epoch [10/13], Step [4300/5723], Loss: 0.502, Training Accuracy: 82.65%\n",
      "Epoch [10/13], Step [4400/5723], Loss: 0.451, Training Accuracy: 75.51%\n",
      "Epoch [10/13], Step [4500/5723], Loss: 0.423, Training Accuracy: 81.63%\n",
      "Epoch [10/13], Step [4600/5723], Loss: 0.555, Training Accuracy: 78.57%\n",
      "Epoch [10/13], Step [4700/5723], Loss: 0.643, Training Accuracy: 76.53%\n",
      "Epoch [10/13], Step [4800/5723], Loss: 0.576, Training Accuracy: 77.55%\n",
      "Epoch [10/13], Step [4900/5723], Loss: 0.630, Training Accuracy: 76.53%\n",
      "Epoch [10/13], Step [5000/5723], Loss: 0.474, Training Accuracy: 82.65%\n",
      "Epoch [10/13], Step [5100/5723], Loss: 0.617, Training Accuracy: 77.55%\n",
      "Epoch [10/13], Step [5200/5723], Loss: 0.379, Training Accuracy: 86.73%\n",
      "Epoch [10/13], Step [5300/5723], Loss: 0.453, Training Accuracy: 77.55%\n",
      "Epoch [10/13], Step [5400/5723], Loss: 0.520, Training Accuracy: 86.73%\n",
      "Epoch [10/13], Step [5500/5723], Loss: 0.526, Training Accuracy: 77.55%\n",
      "Epoch [10/13], Step [5600/5723], Loss: 0.761, Training Accuracy: 75.51%\n",
      "Epoch [10/13], Step [5700/5723], Loss: 0.579, Training Accuracy: 76.53%\n",
      "Epoch [11/13], Step [100/5723], Loss: 0.802, Training Accuracy: 70.41%\n",
      "Epoch [11/13], Step [200/5723], Loss: 0.337, Training Accuracy: 86.73%\n",
      "Epoch [11/13], Step [300/5723], Loss: 0.600, Training Accuracy: 85.71%\n",
      "Epoch [11/13], Step [400/5723], Loss: 0.506, Training Accuracy: 78.57%\n",
      "Epoch [11/13], Step [500/5723], Loss: 0.559, Training Accuracy: 78.57%\n",
      "Epoch [11/13], Step [600/5723], Loss: 0.586, Training Accuracy: 80.61%\n",
      "Epoch [11/13], Step [700/5723], Loss: 0.698, Training Accuracy: 73.47%\n",
      "Epoch [11/13], Step [800/5723], Loss: 0.396, Training Accuracy: 81.63%\n",
      "Epoch [11/13], Step [900/5723], Loss: 0.484, Training Accuracy: 79.59%\n",
      "Epoch [11/13], Step [1000/5723], Loss: 0.517, Training Accuracy: 82.65%\n",
      "Epoch [11/13], Step [1100/5723], Loss: 0.653, Training Accuracy: 78.57%\n",
      "Epoch [11/13], Step [1200/5723], Loss: 0.488, Training Accuracy: 83.67%\n",
      "Epoch [11/13], Step [1300/5723], Loss: 0.425, Training Accuracy: 84.69%\n",
      "Epoch [11/13], Step [1400/5723], Loss: 0.509, Training Accuracy: 78.57%\n",
      "Epoch [11/13], Step [1500/5723], Loss: 0.517, Training Accuracy: 79.59%\n",
      "Epoch [11/13], Step [1600/5723], Loss: 0.420, Training Accuracy: 80.61%\n",
      "Epoch [11/13], Step [1700/5723], Loss: 0.446, Training Accuracy: 80.61%\n",
      "Epoch [11/13], Step [1800/5723], Loss: 0.422, Training Accuracy: 79.59%\n",
      "Epoch [11/13], Step [1900/5723], Loss: 0.394, Training Accuracy: 84.69%\n",
      "Epoch [11/13], Step [2000/5723], Loss: 0.347, Training Accuracy: 84.69%\n",
      "Epoch [11/13], Step [2100/5723], Loss: 0.591, Training Accuracy: 83.67%\n",
      "Epoch [11/13], Step [2200/5723], Loss: 0.526, Training Accuracy: 77.55%\n",
      "Epoch [11/13], Step [2300/5723], Loss: 0.809, Training Accuracy: 77.55%\n",
      "Epoch [11/13], Step [2400/5723], Loss: 0.460, Training Accuracy: 80.61%\n",
      "Epoch [11/13], Step [2500/5723], Loss: 0.650, Training Accuracy: 73.47%\n",
      "Epoch [11/13], Step [2600/5723], Loss: 0.278, Training Accuracy: 94.90%\n",
      "Epoch [11/13], Step [2700/5723], Loss: 0.484, Training Accuracy: 84.69%\n",
      "Epoch [11/13], Step [2800/5723], Loss: 0.547, Training Accuracy: 77.55%\n",
      "Epoch [11/13], Step [2900/5723], Loss: 0.577, Training Accuracy: 75.51%\n",
      "Epoch [11/13], Step [3000/5723], Loss: 0.607, Training Accuracy: 81.63%\n",
      "Epoch [11/13], Step [3100/5723], Loss: 0.346, Training Accuracy: 82.65%\n",
      "Epoch [11/13], Step [3200/5723], Loss: 0.409, Training Accuracy: 88.78%\n",
      "Epoch [11/13], Step [3300/5723], Loss: 0.430, Training Accuracy: 82.65%\n",
      "Epoch [11/13], Step [3400/5723], Loss: 0.569, Training Accuracy: 72.45%\n",
      "Epoch [11/13], Step [3500/5723], Loss: 0.502, Training Accuracy: 79.59%\n",
      "Epoch [11/13], Step [3600/5723], Loss: 0.527, Training Accuracy: 81.63%\n",
      "Epoch [11/13], Step [3700/5723], Loss: 0.497, Training Accuracy: 80.61%\n",
      "Epoch [11/13], Step [3800/5723], Loss: 0.458, Training Accuracy: 78.57%\n",
      "Epoch [11/13], Step [3900/5723], Loss: 0.465, Training Accuracy: 83.67%\n",
      "Epoch [11/13], Step [4000/5723], Loss: 0.380, Training Accuracy: 83.67%\n",
      "Epoch [11/13], Step [4100/5723], Loss: 0.337, Training Accuracy: 84.69%\n",
      "Epoch [11/13], Step [4200/5723], Loss: 0.497, Training Accuracy: 80.61%\n",
      "Epoch [11/13], Step [4300/5723], Loss: 0.502, Training Accuracy: 81.63%\n",
      "Epoch [11/13], Step [4400/5723], Loss: 0.531, Training Accuracy: 77.55%\n",
      "Epoch [11/13], Step [4500/5723], Loss: 0.650, Training Accuracy: 76.53%\n",
      "Epoch [11/13], Step [4600/5723], Loss: 0.605, Training Accuracy: 75.51%\n",
      "Epoch [11/13], Step [4700/5723], Loss: 0.591, Training Accuracy: 70.41%\n",
      "Epoch [11/13], Step [4800/5723], Loss: 0.644, Training Accuracy: 72.45%\n",
      "Epoch [11/13], Step [4900/5723], Loss: 0.481, Training Accuracy: 79.59%\n",
      "Epoch [11/13], Step [5000/5723], Loss: 0.468, Training Accuracy: 77.55%\n",
      "Epoch [11/13], Step [5100/5723], Loss: 0.499, Training Accuracy: 83.67%\n",
      "Epoch [11/13], Step [5200/5723], Loss: 0.646, Training Accuracy: 77.55%\n",
      "Epoch [11/13], Step [5300/5723], Loss: 0.454, Training Accuracy: 79.59%\n",
      "Epoch [11/13], Step [5400/5723], Loss: 0.487, Training Accuracy: 79.59%\n",
      "Epoch [11/13], Step [5500/5723], Loss: 0.558, Training Accuracy: 78.57%\n",
      "Epoch [11/13], Step [5600/5723], Loss: 0.692, Training Accuracy: 66.33%\n",
      "Epoch [11/13], Step [5700/5723], Loss: 0.410, Training Accuracy: 85.71%\n",
      "Epoch [12/13], Step [100/5723], Loss: 0.501, Training Accuracy: 82.65%\n",
      "Epoch [12/13], Step [200/5723], Loss: 1.005, Training Accuracy: 72.45%\n",
      "Epoch [12/13], Step [300/5723], Loss: 0.540, Training Accuracy: 76.53%\n",
      "Epoch [12/13], Step [400/5723], Loss: 0.386, Training Accuracy: 84.69%\n",
      "Epoch [12/13], Step [500/5723], Loss: 0.681, Training Accuracy: 79.59%\n",
      "Epoch [12/13], Step [600/5723], Loss: 0.610, Training Accuracy: 78.57%\n",
      "Epoch [12/13], Step [700/5723], Loss: 0.680, Training Accuracy: 75.51%\n",
      "Epoch [12/13], Step [800/5723], Loss: 0.512, Training Accuracy: 81.63%\n",
      "Epoch [12/13], Step [900/5723], Loss: 0.498, Training Accuracy: 79.59%\n",
      "Epoch [12/13], Step [1000/5723], Loss: 0.332, Training Accuracy: 84.69%\n",
      "Epoch [12/13], Step [1100/5723], Loss: 0.651, Training Accuracy: 76.53%\n",
      "Epoch [12/13], Step [1200/5723], Loss: 0.602, Training Accuracy: 79.59%\n",
      "Epoch [12/13], Step [1300/5723], Loss: 0.546, Training Accuracy: 77.55%\n",
      "Epoch [12/13], Step [1400/5723], Loss: 0.379, Training Accuracy: 88.78%\n",
      "Epoch [12/13], Step [1500/5723], Loss: 0.502, Training Accuracy: 79.59%\n",
      "Epoch [12/13], Step [1600/5723], Loss: 0.552, Training Accuracy: 80.61%\n",
      "Epoch [12/13], Step [1700/5723], Loss: 0.364, Training Accuracy: 85.71%\n",
      "Epoch [12/13], Step [1800/5723], Loss: 0.474, Training Accuracy: 79.59%\n",
      "Epoch [12/13], Step [1900/5723], Loss: 0.341, Training Accuracy: 84.69%\n",
      "Epoch [12/13], Step [2000/5723], Loss: 0.477, Training Accuracy: 83.67%\n",
      "Epoch [12/13], Step [2100/5723], Loss: 0.470, Training Accuracy: 81.63%\n",
      "Epoch [12/13], Step [2200/5723], Loss: 0.476, Training Accuracy: 81.63%\n",
      "Epoch [12/13], Step [2300/5723], Loss: 0.337, Training Accuracy: 83.67%\n",
      "Epoch [12/13], Step [2400/5723], Loss: 0.530, Training Accuracy: 75.51%\n",
      "Epoch [12/13], Step [2500/5723], Loss: 0.787, Training Accuracy: 72.45%\n",
      "Epoch [12/13], Step [2600/5723], Loss: 0.635, Training Accuracy: 73.47%\n",
      "Epoch [12/13], Step [2700/5723], Loss: 0.722, Training Accuracy: 73.47%\n",
      "Epoch [12/13], Step [2800/5723], Loss: 0.580, Training Accuracy: 81.63%\n",
      "Epoch [12/13], Step [2900/5723], Loss: 0.377, Training Accuracy: 84.69%\n",
      "Epoch [12/13], Step [3000/5723], Loss: 0.407, Training Accuracy: 81.63%\n",
      "Epoch [12/13], Step [3100/5723], Loss: 0.671, Training Accuracy: 70.41%\n",
      "Epoch [12/13], Step [3200/5723], Loss: 0.586, Training Accuracy: 75.51%\n",
      "Epoch [12/13], Step [3300/5723], Loss: 0.619, Training Accuracy: 83.67%\n",
      "Epoch [12/13], Step [3400/5723], Loss: 0.696, Training Accuracy: 76.53%\n",
      "Epoch [12/13], Step [3500/5723], Loss: 0.680, Training Accuracy: 75.51%\n",
      "Epoch [12/13], Step [3600/5723], Loss: 0.373, Training Accuracy: 85.71%\n",
      "Epoch [12/13], Step [3700/5723], Loss: 0.390, Training Accuracy: 87.76%\n",
      "Epoch [12/13], Step [3800/5723], Loss: 0.582, Training Accuracy: 83.67%\n",
      "Epoch [12/13], Step [3900/5723], Loss: 0.556, Training Accuracy: 88.78%\n",
      "Epoch [12/13], Step [4000/5723], Loss: 0.399, Training Accuracy: 85.71%\n",
      "Epoch [12/13], Step [4100/5723], Loss: 0.486, Training Accuracy: 78.57%\n",
      "Epoch [12/13], Step [4200/5723], Loss: 0.550, Training Accuracy: 77.55%\n",
      "Epoch [12/13], Step [4300/5723], Loss: 0.460, Training Accuracy: 82.65%\n",
      "Epoch [12/13], Step [4400/5723], Loss: 0.447, Training Accuracy: 81.63%\n",
      "Epoch [12/13], Step [4500/5723], Loss: 0.527, Training Accuracy: 80.61%\n",
      "Epoch [12/13], Step [4600/5723], Loss: 0.550, Training Accuracy: 77.55%\n",
      "Epoch [12/13], Step [4700/5723], Loss: 0.501, Training Accuracy: 80.61%\n",
      "Epoch [12/13], Step [4800/5723], Loss: 0.617, Training Accuracy: 75.51%\n",
      "Epoch [12/13], Step [4900/5723], Loss: 0.469, Training Accuracy: 80.61%\n",
      "Epoch [12/13], Step [5000/5723], Loss: 0.512, Training Accuracy: 82.65%\n",
      "Epoch [12/13], Step [5100/5723], Loss: 0.614, Training Accuracy: 80.61%\n",
      "Epoch [12/13], Step [5200/5723], Loss: 0.504, Training Accuracy: 84.69%\n",
      "Epoch [12/13], Step [5300/5723], Loss: 0.489, Training Accuracy: 77.55%\n",
      "Epoch [12/13], Step [5400/5723], Loss: 0.500, Training Accuracy: 78.57%\n",
      "Epoch [12/13], Step [5500/5723], Loss: 0.607, Training Accuracy: 78.57%\n",
      "Epoch [12/13], Step [5600/5723], Loss: 0.666, Training Accuracy: 73.47%\n",
      "Epoch [12/13], Step [5700/5723], Loss: 0.593, Training Accuracy: 77.55%\n",
      "Epoch [13/13], Step [100/5723], Loss: 0.437, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [200/5723], Loss: 0.319, Training Accuracy: 88.78%\n",
      "Epoch [13/13], Step [300/5723], Loss: 0.511, Training Accuracy: 81.63%\n",
      "Epoch [13/13], Step [400/5723], Loss: 0.329, Training Accuracy: 86.73%\n",
      "Epoch [13/13], Step [500/5723], Loss: 0.393, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [600/5723], Loss: 0.496, Training Accuracy: 76.53%\n",
      "Epoch [13/13], Step [700/5723], Loss: 0.639, Training Accuracy: 80.61%\n",
      "Epoch [13/13], Step [800/5723], Loss: 0.428, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [900/5723], Loss: 0.426, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [1000/5723], Loss: 0.581, Training Accuracy: 75.51%\n",
      "Epoch [13/13], Step [1100/5723], Loss: 0.647, Training Accuracy: 83.67%\n",
      "Epoch [13/13], Step [1200/5723], Loss: 0.587, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [1300/5723], Loss: 0.558, Training Accuracy: 81.63%\n",
      "Epoch [13/13], Step [1400/5723], Loss: 0.526, Training Accuracy: 74.49%\n",
      "Epoch [13/13], Step [1500/5723], Loss: 0.357, Training Accuracy: 84.69%\n",
      "Epoch [13/13], Step [1600/5723], Loss: 0.450, Training Accuracy: 81.63%\n",
      "Epoch [13/13], Step [1700/5723], Loss: 0.588, Training Accuracy: 77.55%\n",
      "Epoch [13/13], Step [1800/5723], Loss: 0.419, Training Accuracy: 84.69%\n",
      "Epoch [13/13], Step [1900/5723], Loss: 0.593, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [2000/5723], Loss: 0.462, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [2100/5723], Loss: 0.702, Training Accuracy: 73.47%\n",
      "Epoch [13/13], Step [2200/5723], Loss: 0.527, Training Accuracy: 80.61%\n",
      "Epoch [13/13], Step [2300/5723], Loss: 0.495, Training Accuracy: 76.53%\n",
      "Epoch [13/13], Step [2400/5723], Loss: 0.347, Training Accuracy: 81.63%\n",
      "Epoch [13/13], Step [2500/5723], Loss: 0.489, Training Accuracy: 83.67%\n",
      "Epoch [13/13], Step [2600/5723], Loss: 0.547, Training Accuracy: 80.61%\n",
      "Epoch [13/13], Step [2700/5723], Loss: 0.473, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [2800/5723], Loss: 0.495, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [2900/5723], Loss: 0.514, Training Accuracy: 74.49%\n",
      "Epoch [13/13], Step [3000/5723], Loss: 0.394, Training Accuracy: 84.69%\n",
      "Epoch [13/13], Step [3100/5723], Loss: 0.478, Training Accuracy: 84.69%\n",
      "Epoch [13/13], Step [3200/5723], Loss: 0.511, Training Accuracy: 76.53%\n",
      "Epoch [13/13], Step [3300/5723], Loss: 0.471, Training Accuracy: 83.67%\n",
      "Epoch [13/13], Step [3400/5723], Loss: 0.542, Training Accuracy: 73.47%\n",
      "Epoch [13/13], Step [3500/5723], Loss: 0.474, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [3600/5723], Loss: 0.629, Training Accuracy: 77.55%\n",
      "Epoch [13/13], Step [3700/5723], Loss: 0.499, Training Accuracy: 84.69%\n",
      "Epoch [13/13], Step [3800/5723], Loss: 0.471, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [3900/5723], Loss: 0.618, Training Accuracy: 83.67%\n",
      "Epoch [13/13], Step [4000/5723], Loss: 0.639, Training Accuracy: 74.49%\n",
      "Epoch [13/13], Step [4100/5723], Loss: 0.540, Training Accuracy: 78.57%\n",
      "Epoch [13/13], Step [4200/5723], Loss: 0.616, Training Accuracy: 77.55%\n",
      "Epoch [13/13], Step [4300/5723], Loss: 0.364, Training Accuracy: 87.76%\n",
      "Epoch [13/13], Step [4400/5723], Loss: 0.541, Training Accuracy: 81.63%\n",
      "Epoch [13/13], Step [4500/5723], Loss: 0.623, Training Accuracy: 77.55%\n",
      "Epoch [13/13], Step [4600/5723], Loss: 0.498, Training Accuracy: 81.63%\n",
      "Epoch [13/13], Step [4700/5723], Loss: 0.450, Training Accuracy: 85.71%\n",
      "Epoch [13/13], Step [4800/5723], Loss: 0.471, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [4900/5723], Loss: 0.513, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [5000/5723], Loss: 0.655, Training Accuracy: 78.57%\n",
      "Epoch [13/13], Step [5100/5723], Loss: 0.530, Training Accuracy: 79.59%\n",
      "Epoch [13/13], Step [5200/5723], Loss: 0.571, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [5300/5723], Loss: 0.544, Training Accuracy: 82.65%\n",
      "Epoch [13/13], Step [5400/5723], Loss: 0.576, Training Accuracy: 75.51%\n",
      "Epoch [13/13], Step [5500/5723], Loss: 0.643, Training Accuracy: 87.76%\n",
      "Epoch [13/13], Step [5600/5723], Loss: 0.398, Training Accuracy: 85.71%\n",
      "Epoch [13/13], Step [5700/5723], Loss: 0.483, Training Accuracy: 78.57%\n",
      " Accuracy: 0.9617486338797814\n",
      " Log Loss0.30195772864655335\n"
     ]
    }
   ],
   "source": [
    "# Train CNN with no CV\n",
    "\n",
    "# by geometric mean\n",
    "learning_rate = 0.0002\n",
    "batch_size = 98\n",
    "num_epochs = 13\n",
    "\n",
    "    # perform CNN\n",
    "\n",
    "train_loader = spectral_dataloader(SMART_x,SMART_y,batch_size = batch_size, shuffle=True) # requires dataloader functions\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "\n",
    "            # Loss and optimizer\n",
    "\n",
    "model = ConvNet() #this should be called before running the function so is not needed here\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weight = np.squeeze(get_data_weights(y_train)).to(device) # Need to send all variables used in optimisation to the GPU, including this weights\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "cuda = torch.cuda.is_available()\n",
    "model.cuda()\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (spectra, labels) in enumerate(train_loader):\n",
    "        spectra, labels = spectra.cuda(), labels.cuda()\n",
    "\n",
    "            # Run the forward pass\n",
    "        outputs = model(spectra)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "            # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.3f}, Training Accuracy: {:.2f}%'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item(), (correct / total) * 100))\n",
    "\n",
    "\n",
    "y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_whole(model, SMART_uniquemapID, SMART_x, SMART_y, SMART_y, proportioned = False )\n",
    "con_mat = confusion_matrix(y_sample_label, y_sample_clas)\n",
    "ll = metrics.log_loss(y_sample_label, y_sample_preds,  labels = np.unique(SMART_y))\n",
    "acc = metrics.accuracy_score(y_sample_label, y_sample_clas)\n",
    "\n",
    "print(\" Accuracy: {}\".format(acc))\n",
    "print(\" Log Loss{}\".format(ll))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhpzmktGJmXf"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/ML/ALLSMART_FINAL_CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7hnBi3pfoOD"
   },
   "outputs": [],
   "source": [
    "# Create ROC curves for leave two out CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2241035,
     "status": "ok",
     "timestamp": 1661593777766,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "cBjRSD0ZfodV",
    "outputId": "42b20961-afae-4bf1-b86e-d8e3adb5b0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Centre: GRH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/60], Step [100/1967], Loss: 1.346, Training Accuracy: 62.24%\n",
      "Epoch [1/60], Step [200/1967], Loss: 1.081, Training Accuracy: 58.16%\n",
      "Epoch [1/60], Step [300/1967], Loss: 0.988, Training Accuracy: 65.31%\n",
      "Epoch [1/60], Step [400/1967], Loss: 0.859, Training Accuracy: 69.39%\n",
      "Epoch [1/60], Step [500/1967], Loss: 0.982, Training Accuracy: 69.39%\n",
      "Epoch [1/60], Step [600/1967], Loss: 0.764, Training Accuracy: 75.51%\n",
      "Epoch [1/60], Step [700/1967], Loss: 0.721, Training Accuracy: 77.55%\n",
      "Epoch [1/60], Step [800/1967], Loss: 0.837, Training Accuracy: 76.53%\n",
      "Epoch [1/60], Step [900/1967], Loss: 0.984, Training Accuracy: 67.35%\n",
      "Epoch [1/60], Step [1000/1967], Loss: 0.710, Training Accuracy: 75.51%\n",
      "Epoch [1/60], Step [1100/1967], Loss: 0.849, Training Accuracy: 73.47%\n",
      "Epoch [1/60], Step [1200/1967], Loss: 0.673, Training Accuracy: 75.51%\n",
      "Epoch [1/60], Step [1300/1967], Loss: 0.998, Training Accuracy: 63.27%\n",
      "Epoch [1/60], Step [1400/1967], Loss: 0.695, Training Accuracy: 78.57%\n",
      "Epoch [1/60], Step [1500/1967], Loss: 0.826, Training Accuracy: 67.35%\n",
      "Epoch [1/60], Step [1600/1967], Loss: 0.683, Training Accuracy: 71.43%\n",
      "Epoch [1/60], Step [1700/1967], Loss: 0.950, Training Accuracy: 66.33%\n",
      "Epoch [1/60], Step [1800/1967], Loss: 0.681, Training Accuracy: 75.51%\n",
      "Epoch [1/60], Step [1900/1967], Loss: 0.919, Training Accuracy: 69.39%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 63.34853675034432 %\n",
      "Epoch [2/60], Step [100/1967], Loss: 0.592, Training Accuracy: 79.59%\n",
      "Epoch [2/60], Step [200/1967], Loss: 0.790, Training Accuracy: 71.43%\n",
      "Epoch [2/60], Step [300/1967], Loss: 0.726, Training Accuracy: 71.43%\n",
      "Epoch [2/60], Step [400/1967], Loss: 0.713, Training Accuracy: 72.45%\n",
      "Epoch [2/60], Step [500/1967], Loss: 0.657, Training Accuracy: 78.57%\n",
      "Epoch [2/60], Step [600/1967], Loss: 0.774, Training Accuracy: 69.39%\n",
      "Epoch [2/60], Step [700/1967], Loss: 0.719, Training Accuracy: 72.45%\n",
      "Epoch [2/60], Step [800/1967], Loss: 0.652, Training Accuracy: 76.53%\n",
      "Epoch [2/60], Step [900/1967], Loss: 0.789, Training Accuracy: 68.37%\n",
      "Epoch [2/60], Step [1000/1967], Loss: 0.736, Training Accuracy: 76.53%\n",
      "Epoch [2/60], Step [1100/1967], Loss: 0.777, Training Accuracy: 70.41%\n",
      "Epoch [2/60], Step [1200/1967], Loss: 0.520, Training Accuracy: 81.63%\n",
      "Epoch [2/60], Step [1300/1967], Loss: 0.607, Training Accuracy: 81.63%\n",
      "Epoch [2/60], Step [1400/1967], Loss: 0.785, Training Accuracy: 76.53%\n",
      "Epoch [2/60], Step [1500/1967], Loss: 0.482, Training Accuracy: 89.80%\n",
      "Epoch [2/60], Step [1600/1967], Loss: 0.791, Training Accuracy: 71.43%\n",
      "Epoch [2/60], Step [1700/1967], Loss: 0.809, Training Accuracy: 72.45%\n",
      "Epoch [2/60], Step [1800/1967], Loss: 0.550, Training Accuracy: 83.67%\n",
      "Epoch [2/60], Step [1900/1967], Loss: 0.616, Training Accuracy: 74.49%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 65.68528111833402 %\n",
      "Epoch [3/60], Step [100/1967], Loss: 0.672, Training Accuracy: 74.49%\n",
      "Epoch [3/60], Step [200/1967], Loss: 0.763, Training Accuracy: 75.51%\n",
      "Epoch [3/60], Step [300/1967], Loss: 0.657, Training Accuracy: 78.57%\n",
      "Epoch [3/60], Step [400/1967], Loss: 0.731, Training Accuracy: 73.47%\n",
      "Epoch [3/60], Step [500/1967], Loss: 0.769, Training Accuracy: 79.59%\n",
      "Epoch [3/60], Step [600/1967], Loss: 0.757, Training Accuracy: 79.59%\n",
      "Epoch [3/60], Step [700/1967], Loss: 0.835, Training Accuracy: 75.51%\n",
      "Epoch [3/60], Step [800/1967], Loss: 0.479, Training Accuracy: 85.71%\n",
      "Epoch [3/60], Step [900/1967], Loss: 0.712, Training Accuracy: 75.51%\n",
      "Epoch [3/60], Step [1000/1967], Loss: 0.637, Training Accuracy: 73.47%\n",
      "Epoch [3/60], Step [1100/1967], Loss: 0.476, Training Accuracy: 81.63%\n",
      "Epoch [3/60], Step [1200/1967], Loss: 0.669, Training Accuracy: 75.51%\n",
      "Epoch [3/60], Step [1300/1967], Loss: 0.510, Training Accuracy: 81.63%\n",
      "Epoch [3/60], Step [1400/1967], Loss: 0.698, Training Accuracy: 77.55%\n",
      "Epoch [3/60], Step [1500/1967], Loss: 0.557, Training Accuracy: 75.51%\n",
      "Epoch [3/60], Step [1600/1967], Loss: 0.500, Training Accuracy: 80.61%\n",
      "Epoch [3/60], Step [1700/1967], Loss: 0.610, Training Accuracy: 78.57%\n",
      "Epoch [3/60], Step [1800/1967], Loss: 0.567, Training Accuracy: 81.63%\n",
      "Epoch [3/60], Step [1900/1967], Loss: 0.643, Training Accuracy: 74.49%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 65.9493261689834 %\n",
      "Epoch [4/60], Step [100/1967], Loss: 0.677, Training Accuracy: 75.51%\n",
      "Epoch [4/60], Step [200/1967], Loss: 0.429, Training Accuracy: 83.67%\n",
      "Epoch [4/60], Step [300/1967], Loss: 0.510, Training Accuracy: 84.69%\n",
      "Epoch [4/60], Step [400/1967], Loss: 0.692, Training Accuracy: 76.53%\n",
      "Epoch [4/60], Step [500/1967], Loss: 0.603, Training Accuracy: 77.55%\n",
      "Epoch [4/60], Step [600/1967], Loss: 0.610, Training Accuracy: 77.55%\n",
      "Epoch [4/60], Step [700/1967], Loss: 0.518, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [800/1967], Loss: 0.711, Training Accuracy: 73.47%\n",
      "Epoch [4/60], Step [900/1967], Loss: 0.745, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [1000/1967], Loss: 0.551, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [1100/1967], Loss: 0.638, Training Accuracy: 81.63%\n",
      "Epoch [4/60], Step [1200/1967], Loss: 0.545, Training Accuracy: 81.63%\n",
      "Epoch [4/60], Step [1300/1967], Loss: 0.499, Training Accuracy: 85.71%\n",
      "Epoch [4/60], Step [1400/1967], Loss: 0.672, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [1500/1967], Loss: 0.709, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [1600/1967], Loss: 0.511, Training Accuracy: 83.67%\n",
      "Epoch [4/60], Step [1700/1967], Loss: 0.681, Training Accuracy: 74.49%\n",
      "Epoch [4/60], Step [1800/1967], Loss: 0.632, Training Accuracy: 75.51%\n",
      "Epoch [4/60], Step [1900/1967], Loss: 0.500, Training Accuracy: 83.67%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 65.8518033570666 %\n",
      "Epoch [5/60], Step [100/1967], Loss: 0.648, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [200/1967], Loss: 0.572, Training Accuracy: 80.61%\n",
      "Epoch [5/60], Step [300/1967], Loss: 0.574, Training Accuracy: 81.63%\n",
      "Epoch [5/60], Step [400/1967], Loss: 0.799, Training Accuracy: 66.33%\n",
      "Epoch [5/60], Step [500/1967], Loss: 0.723, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [600/1967], Loss: 0.862, Training Accuracy: 86.73%\n",
      "Epoch [5/60], Step [700/1967], Loss: 0.577, Training Accuracy: 79.59%\n",
      "Epoch [5/60], Step [800/1967], Loss: 0.445, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [900/1967], Loss: 0.649, Training Accuracy: 79.59%\n",
      "Epoch [5/60], Step [1000/1967], Loss: 0.471, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [1100/1967], Loss: 0.380, Training Accuracy: 86.73%\n",
      "Epoch [5/60], Step [1200/1967], Loss: 0.717, Training Accuracy: 81.63%\n",
      "Epoch [5/60], Step [1300/1967], Loss: 0.468, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [1400/1967], Loss: 0.474, Training Accuracy: 83.67%\n",
      "Epoch [5/60], Step [1500/1967], Loss: 0.465, Training Accuracy: 87.76%\n",
      "Epoch [5/60], Step [1600/1967], Loss: 0.510, Training Accuracy: 78.57%\n",
      "Epoch [5/60], Step [1700/1967], Loss: 0.567, Training Accuracy: 80.61%\n",
      "Epoch [5/60], Step [1800/1967], Loss: 0.652, Training Accuracy: 80.61%\n",
      "Epoch [5/60], Step [1900/1967], Loss: 0.527, Training Accuracy: 78.57%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 65.76215843246341 %\n",
      "Epoch [6/60], Step [100/1967], Loss: 0.528, Training Accuracy: 83.67%\n",
      "Epoch [6/60], Step [200/1967], Loss: 0.436, Training Accuracy: 86.73%\n",
      "Epoch [6/60], Step [300/1967], Loss: 0.340, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [400/1967], Loss: 0.459, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [500/1967], Loss: 0.352, Training Accuracy: 84.69%\n",
      "Epoch [6/60], Step [600/1967], Loss: 0.625, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [700/1967], Loss: 0.590, Training Accuracy: 77.55%\n",
      "Epoch [6/60], Step [800/1967], Loss: 0.386, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [900/1967], Loss: 0.596, Training Accuracy: 78.57%\n",
      "Epoch [6/60], Step [1000/1967], Loss: 0.516, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1100/1967], Loss: 0.372, Training Accuracy: 86.73%\n",
      "Epoch [6/60], Step [1200/1967], Loss: 0.501, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1300/1967], Loss: 0.409, Training Accuracy: 88.78%\n",
      "Epoch [6/60], Step [1400/1967], Loss: 0.467, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1500/1967], Loss: 0.579, Training Accuracy: 83.67%\n",
      "Epoch [6/60], Step [1600/1967], Loss: 0.401, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [1700/1967], Loss: 0.573, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1800/1967], Loss: 0.595, Training Accuracy: 79.59%\n",
      "Epoch [6/60], Step [1900/1967], Loss: 0.542, Training Accuracy: 78.57%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 65.52745172077508 %\n",
      "Epoch [7/60], Step [100/1967], Loss: 0.568, Training Accuracy: 75.51%\n",
      "Epoch [7/60], Step [200/1967], Loss: 0.569, Training Accuracy: 78.57%\n",
      "Epoch [7/60], Step [300/1967], Loss: 0.322, Training Accuracy: 85.71%\n",
      "Epoch [7/60], Step [400/1967], Loss: 0.454, Training Accuracy: 88.78%\n",
      "Epoch [7/60], Step [500/1967], Loss: 0.456, Training Accuracy: 87.76%\n",
      "Epoch [7/60], Step [600/1967], Loss: 0.575, Training Accuracy: 83.67%\n",
      "Epoch [7/60], Step [700/1967], Loss: 0.367, Training Accuracy: 87.76%\n",
      "Epoch [7/60], Step [800/1967], Loss: 0.419, Training Accuracy: 86.73%\n",
      "Epoch [7/60], Step [900/1967], Loss: 0.793, Training Accuracy: 77.55%\n",
      "Epoch [7/60], Step [1000/1967], Loss: 0.584, Training Accuracy: 81.63%\n",
      "Epoch [7/60], Step [1100/1967], Loss: 0.423, Training Accuracy: 87.76%\n",
      "Epoch [7/60], Step [1200/1967], Loss: 0.469, Training Accuracy: 83.67%\n",
      "Epoch [7/60], Step [1300/1967], Loss: 0.528, Training Accuracy: 80.61%\n",
      "Epoch [7/60], Step [1400/1967], Loss: 0.682, Training Accuracy: 79.59%\n",
      "Epoch [7/60], Step [1500/1967], Loss: 0.494, Training Accuracy: 86.73%\n",
      "Epoch [7/60], Step [1600/1967], Loss: 0.575, Training Accuracy: 85.71%\n",
      "Epoch [7/60], Step [1700/1967], Loss: 0.307, Training Accuracy: 88.78%\n",
      "Epoch [7/60], Step [1800/1967], Loss: 0.384, Training Accuracy: 87.76%\n",
      "Epoch [7/60], Step [1900/1967], Loss: 0.430, Training Accuracy: 81.63%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 65.77981576609737 %\n",
      "Epoch [8/60], Step [100/1967], Loss: 0.422, Training Accuracy: 86.73%\n",
      "Epoch [8/60], Step [200/1967], Loss: 0.487, Training Accuracy: 84.69%\n",
      "Epoch [8/60], Step [300/1967], Loss: 0.622, Training Accuracy: 75.51%\n",
      "Epoch [8/60], Step [400/1967], Loss: 0.719, Training Accuracy: 80.61%\n",
      "Epoch [8/60], Step [500/1967], Loss: 0.499, Training Accuracy: 82.65%\n",
      "Epoch [8/60], Step [600/1967], Loss: 0.505, Training Accuracy: 84.69%\n",
      "Epoch [8/60], Step [700/1967], Loss: 0.522, Training Accuracy: 80.61%\n",
      "Epoch [8/60], Step [800/1967], Loss: 0.551, Training Accuracy: 75.51%\n",
      "Epoch [8/60], Step [900/1967], Loss: 0.445, Training Accuracy: 85.71%\n",
      "Epoch [8/60], Step [1000/1967], Loss: 0.536, Training Accuracy: 88.78%\n",
      "Epoch [8/60], Step [1100/1967], Loss: 0.542, Training Accuracy: 81.63%\n",
      "Epoch [8/60], Step [1200/1967], Loss: 0.464, Training Accuracy: 83.67%\n",
      "Epoch [8/60], Step [1300/1967], Loss: 0.487, Training Accuracy: 82.65%\n",
      "Epoch [8/60], Step [1400/1967], Loss: 0.429, Training Accuracy: 83.67%\n",
      "Epoch [8/60], Step [1500/1967], Loss: 0.444, Training Accuracy: 84.69%\n",
      "Epoch [8/60], Step [1600/1967], Loss: 0.444, Training Accuracy: 85.71%\n",
      "Epoch [8/60], Step [1700/1967], Loss: 0.578, Training Accuracy: 79.59%\n",
      "Epoch [8/60], Step [1800/1967], Loss: 0.364, Training Accuracy: 87.76%\n",
      "Epoch [8/60], Step [1900/1967], Loss: 0.436, Training Accuracy: 86.73%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 65.50137319725415 %\n",
      "Finished early after 8 epochs!\n",
      "Final Training Accuracy 84.15620134924754 %\n",
      "Final Test Accuracy 65.49947163824741 %\n",
      "train accuracy: 0.8415620134924754, train logloss: 0.4139895385269047\n",
      "\n",
      "By sample accuracy: 0.9344262295081968, log loss 0.2840221720367829\n",
      "Test Centre: UCL\n",
      "By spectra accuracy: 0.6549341002222235, logloss: 2.0888427333951642\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.639344262295082 logloss 1.261051705792048\n",
      "\n",
      "Test Centre: UoE\n",
      "By spectra accuracy: 0.6545766769006609, logloss: 3.825712176876583\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.639344262295082 logloss 2.2889612398630255\n",
      "\n",
      "\n",
      "Training Centre: UCL\n",
      "Epoch [1/60], Step [100/1796], Loss: 1.151, Training Accuracy: 58.16%\n",
      "Epoch [1/60], Step [200/1796], Loss: 1.193, Training Accuracy: 52.04%\n",
      "Epoch [1/60], Step [300/1796], Loss: 1.111, Training Accuracy: 60.20%\n",
      "Epoch [1/60], Step [400/1796], Loss: 1.015, Training Accuracy: 50.00%\n",
      "Epoch [1/60], Step [500/1796], Loss: 0.919, Training Accuracy: 64.29%\n",
      "Epoch [1/60], Step [600/1796], Loss: 1.040, Training Accuracy: 66.33%\n",
      "Epoch [1/60], Step [700/1796], Loss: 0.971, Training Accuracy: 66.33%\n",
      "Epoch [1/60], Step [800/1796], Loss: 0.905, Training Accuracy: 69.39%\n",
      "Epoch [1/60], Step [900/1796], Loss: 0.888, Training Accuracy: 69.39%\n",
      "Epoch [1/60], Step [1000/1796], Loss: 0.884, Training Accuracy: 76.53%\n",
      "Epoch [1/60], Step [1100/1796], Loss: 0.849, Training Accuracy: 63.27%\n",
      "Epoch [1/60], Step [1200/1796], Loss: 0.977, Training Accuracy: 63.27%\n",
      "Epoch [1/60], Step [1300/1796], Loss: 0.983, Training Accuracy: 61.22%\n",
      "Epoch [1/60], Step [1400/1796], Loss: 1.019, Training Accuracy: 61.22%\n",
      "Epoch [1/60], Step [1500/1796], Loss: 0.854, Training Accuracy: 64.29%\n",
      "Epoch [1/60], Step [1600/1796], Loss: 0.960, Training Accuracy: 60.20%\n",
      "Epoch [1/60], Step [1700/1796], Loss: 0.879, Training Accuracy: 71.43%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 57.33052719099956 %\n",
      "Epoch [2/60], Step [100/1796], Loss: 0.779, Training Accuracy: 72.45%\n",
      "Epoch [2/60], Step [200/1796], Loss: 0.774, Training Accuracy: 76.53%\n",
      "Epoch [2/60], Step [300/1796], Loss: 0.789, Training Accuracy: 71.43%\n",
      "Epoch [2/60], Step [400/1796], Loss: 0.746, Training Accuracy: 75.51%\n",
      "Epoch [2/60], Step [500/1796], Loss: 0.739, Training Accuracy: 73.47%\n",
      "Epoch [2/60], Step [600/1796], Loss: 0.746, Training Accuracy: 74.49%\n",
      "Epoch [2/60], Step [700/1796], Loss: 0.680, Training Accuracy: 73.47%\n",
      "Epoch [2/60], Step [800/1796], Loss: 0.879, Training Accuracy: 66.33%\n",
      "Epoch [2/60], Step [900/1796], Loss: 0.864, Training Accuracy: 74.49%\n",
      "Epoch [2/60], Step [1000/1796], Loss: 0.671, Training Accuracy: 80.61%\n",
      "Epoch [2/60], Step [1100/1796], Loss: 0.795, Training Accuracy: 67.35%\n",
      "Epoch [2/60], Step [1200/1796], Loss: 0.659, Training Accuracy: 79.59%\n",
      "Epoch [2/60], Step [1300/1796], Loss: 0.789, Training Accuracy: 66.33%\n",
      "Epoch [2/60], Step [1400/1796], Loss: 0.787, Training Accuracy: 71.43%\n",
      "Epoch [2/60], Step [1500/1796], Loss: 0.785, Training Accuracy: 69.39%\n",
      "Epoch [2/60], Step [1600/1796], Loss: 0.759, Training Accuracy: 78.57%\n",
      "Epoch [2/60], Step [1700/1796], Loss: 0.691, Training Accuracy: 76.53%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 64.32561644191546 %\n",
      "Epoch [3/60], Step [100/1796], Loss: 0.691, Training Accuracy: 70.41%\n",
      "Epoch [3/60], Step [200/1796], Loss: 0.783, Training Accuracy: 74.49%\n",
      "Epoch [3/60], Step [300/1796], Loss: 0.568, Training Accuracy: 85.71%\n",
      "Epoch [3/60], Step [400/1796], Loss: 0.647, Training Accuracy: 83.67%\n",
      "Epoch [3/60], Step [500/1796], Loss: 0.672, Training Accuracy: 74.49%\n",
      "Epoch [3/60], Step [600/1796], Loss: 0.723, Training Accuracy: 77.55%\n",
      "Epoch [3/60], Step [700/1796], Loss: 0.877, Training Accuracy: 68.37%\n",
      "Epoch [3/60], Step [800/1796], Loss: 0.779, Training Accuracy: 75.51%\n",
      "Epoch [3/60], Step [900/1796], Loss: 0.706, Training Accuracy: 74.49%\n",
      "Epoch [3/60], Step [1000/1796], Loss: 0.626, Training Accuracy: 77.55%\n",
      "Epoch [3/60], Step [1100/1796], Loss: 0.612, Training Accuracy: 83.67%\n",
      "Epoch [3/60], Step [1200/1796], Loss: 0.867, Training Accuracy: 68.37%\n",
      "Epoch [3/60], Step [1300/1796], Loss: 0.733, Training Accuracy: 77.55%\n",
      "Epoch [3/60], Step [1400/1796], Loss: 0.951, Training Accuracy: 70.41%\n",
      "Epoch [3/60], Step [1500/1796], Loss: 0.641, Training Accuracy: 72.45%\n",
      "Epoch [3/60], Step [1600/1796], Loss: 0.906, Training Accuracy: 67.35%\n",
      "Epoch [3/60], Step [1700/1796], Loss: 0.927, Training Accuracy: 73.47%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 63.986800737911494 %\n",
      "Epoch [4/60], Step [100/1796], Loss: 0.630, Training Accuracy: 78.57%\n",
      "Epoch [4/60], Step [200/1796], Loss: 0.769, Training Accuracy: 78.57%\n",
      "Epoch [4/60], Step [300/1796], Loss: 0.545, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [400/1796], Loss: 0.675, Training Accuracy: 78.57%\n",
      "Epoch [4/60], Step [500/1796], Loss: 0.743, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [600/1796], Loss: 0.652, Training Accuracy: 77.55%\n",
      "Epoch [4/60], Step [700/1796], Loss: 0.829, Training Accuracy: 77.55%\n",
      "Epoch [4/60], Step [800/1796], Loss: 0.650, Training Accuracy: 74.49%\n",
      "Epoch [4/60], Step [900/1796], Loss: 0.629, Training Accuracy: 75.51%\n",
      "Epoch [4/60], Step [1000/1796], Loss: 0.555, Training Accuracy: 76.53%\n",
      "Epoch [4/60], Step [1100/1796], Loss: 0.611, Training Accuracy: 80.61%\n",
      "Epoch [4/60], Step [1200/1796], Loss: 0.616, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [1300/1796], Loss: 0.504, Training Accuracy: 76.53%\n",
      "Epoch [4/60], Step [1400/1796], Loss: 0.588, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [1500/1796], Loss: 0.645, Training Accuracy: 76.53%\n",
      "Epoch [4/60], Step [1600/1796], Loss: 0.633, Training Accuracy: 72.45%\n",
      "Epoch [4/60], Step [1700/1796], Loss: 0.585, Training Accuracy: 76.53%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 61.585210590589035 %\n",
      "Epoch [5/60], Step [100/1796], Loss: 0.603, Training Accuracy: 76.53%\n",
      "Epoch [5/60], Step [200/1796], Loss: 0.540, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [300/1796], Loss: 0.715, Training Accuracy: 80.61%\n",
      "Epoch [5/60], Step [400/1796], Loss: 0.511, Training Accuracy: 84.69%\n",
      "Epoch [5/60], Step [500/1796], Loss: 0.562, Training Accuracy: 73.47%\n",
      "Epoch [5/60], Step [600/1796], Loss: 0.628, Training Accuracy: 76.53%\n",
      "Epoch [5/60], Step [700/1796], Loss: 0.565, Training Accuracy: 77.55%\n",
      "Epoch [5/60], Step [800/1796], Loss: 0.700, Training Accuracy: 77.55%\n",
      "Epoch [5/60], Step [900/1796], Loss: 0.685, Training Accuracy: 78.57%\n",
      "Epoch [5/60], Step [1000/1796], Loss: 0.440, Training Accuracy: 78.57%\n",
      "Epoch [5/60], Step [1100/1796], Loss: 0.963, Training Accuracy: 69.39%\n",
      "Epoch [5/60], Step [1200/1796], Loss: 0.508, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [1300/1796], Loss: 0.665, Training Accuracy: 76.53%\n",
      "Epoch [5/60], Step [1400/1796], Loss: 0.670, Training Accuracy: 71.43%\n",
      "Epoch [5/60], Step [1500/1796], Loss: 0.858, Training Accuracy: 75.51%\n",
      "Epoch [5/60], Step [1600/1796], Loss: 0.633, Training Accuracy: 76.53%\n",
      "Epoch [5/60], Step [1700/1796], Loss: 0.466, Training Accuracy: 81.63%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 63.40426637565931 %\n",
      "Epoch [6/60], Step [100/1796], Loss: 0.548, Training Accuracy: 81.63%\n",
      "Epoch [6/60], Step [200/1796], Loss: 0.545, Training Accuracy: 76.53%\n",
      "Epoch [6/60], Step [300/1796], Loss: 0.569, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [400/1796], Loss: 0.537, Training Accuracy: 82.65%\n",
      "Epoch [6/60], Step [500/1796], Loss: 0.678, Training Accuracy: 71.43%\n",
      "Epoch [6/60], Step [600/1796], Loss: 0.731, Training Accuracy: 73.47%\n",
      "Epoch [6/60], Step [700/1796], Loss: 0.585, Training Accuracy: 76.53%\n",
      "Epoch [6/60], Step [800/1796], Loss: 0.628, Training Accuracy: 79.59%\n",
      "Epoch [6/60], Step [900/1796], Loss: 0.696, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1000/1796], Loss: 0.711, Training Accuracy: 71.43%\n",
      "Epoch [6/60], Step [1100/1796], Loss: 0.606, Training Accuracy: 75.51%\n",
      "Epoch [6/60], Step [1200/1796], Loss: 0.534, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1300/1796], Loss: 0.668, Training Accuracy: 74.49%\n",
      "Epoch [6/60], Step [1400/1796], Loss: 0.532, Training Accuracy: 77.55%\n",
      "Epoch [6/60], Step [1500/1796], Loss: 0.368, Training Accuracy: 83.67%\n",
      "Epoch [6/60], Step [1600/1796], Loss: 0.590, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1700/1796], Loss: 0.566, Training Accuracy: 84.69%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 63.502221529347565 %\n",
      "Epoch [7/60], Step [100/1796], Loss: 0.781, Training Accuracy: 73.47%\n",
      "Epoch [7/60], Step [200/1796], Loss: 0.549, Training Accuracy: 80.61%\n",
      "Epoch [7/60], Step [300/1796], Loss: 0.521, Training Accuracy: 85.71%\n",
      "Epoch [7/60], Step [400/1796], Loss: 0.546, Training Accuracy: 81.63%\n",
      "Epoch [7/60], Step [500/1796], Loss: 0.505, Training Accuracy: 86.73%\n",
      "Epoch [7/60], Step [600/1796], Loss: 0.703, Training Accuracy: 78.57%\n",
      "Epoch [7/60], Step [700/1796], Loss: 0.423, Training Accuracy: 83.67%\n",
      "Epoch [7/60], Step [800/1796], Loss: 0.740, Training Accuracy: 69.39%\n",
      "Epoch [7/60], Step [900/1796], Loss: 0.747, Training Accuracy: 75.51%\n",
      "Epoch [7/60], Step [1000/1796], Loss: 0.593, Training Accuracy: 77.55%\n",
      "Epoch [7/60], Step [1100/1796], Loss: 0.535, Training Accuracy: 82.65%\n",
      "Epoch [7/60], Step [1200/1796], Loss: 0.594, Training Accuracy: 78.57%\n",
      "Epoch [7/60], Step [1300/1796], Loss: 0.784, Training Accuracy: 64.29%\n",
      "Epoch [7/60], Step [1400/1796], Loss: 0.635, Training Accuracy: 78.57%\n",
      "Epoch [7/60], Step [1500/1796], Loss: 0.913, Training Accuracy: 72.45%\n",
      "Epoch [7/60], Step [1600/1796], Loss: 0.647, Training Accuracy: 80.61%\n",
      "Epoch [7/60], Step [1700/1796], Loss: 0.535, Training Accuracy: 75.51%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 63.061293423753476 %\n",
      "Finished early after 7 epochs!\n",
      "Final Training Accuracy 81.21671620753742 %\n",
      "Final Test Accuracy 63.05453789591291 %\n",
      "train accuracy: 0.8121671620753741, train logloss: 0.5030702350859304\n",
      "\n",
      "By sample accuracy: 1.0, log loss 0.29315464325643614\n",
      "Test Centre: GRH\n",
      "By spectra accuracy: 0.6295537104307213, logloss: 1.1306350539591408\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.6229508196721312 logloss 0.9531354736729066\n",
      "\n",
      "Test Centre: UoE\n",
      "By spectra accuracy: 0.6304261851485664, logloss: 1.06660453383127\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.6065573770491803 logloss 0.7682575873037558\n",
      "\n",
      "\n",
      "Training Centre: UoE\n",
      "Epoch [1/60], Step [100/1961], Loss: 1.160, Training Accuracy: 63.27%\n",
      "Epoch [1/60], Step [200/1961], Loss: 1.110, Training Accuracy: 65.31%\n",
      "Epoch [1/60], Step [300/1961], Loss: 0.933, Training Accuracy: 67.35%\n",
      "Epoch [1/60], Step [400/1961], Loss: 0.809, Training Accuracy: 70.41%\n",
      "Epoch [1/60], Step [500/1961], Loss: 0.872, Training Accuracy: 73.47%\n",
      "Epoch [1/60], Step [600/1961], Loss: 1.074, Training Accuracy: 63.27%\n",
      "Epoch [1/60], Step [700/1961], Loss: 0.756, Training Accuracy: 67.35%\n",
      "Epoch [1/60], Step [800/1961], Loss: 0.851, Training Accuracy: 68.37%\n",
      "Epoch [1/60], Step [900/1961], Loss: 0.741, Training Accuracy: 76.53%\n",
      "Epoch [1/60], Step [1000/1961], Loss: 0.700, Training Accuracy: 79.59%\n",
      "Epoch [1/60], Step [1100/1961], Loss: 0.658, Training Accuracy: 71.43%\n",
      "Epoch [1/60], Step [1200/1961], Loss: 0.963, Training Accuracy: 70.41%\n",
      "Epoch [1/60], Step [1300/1961], Loss: 0.799, Training Accuracy: 72.45%\n",
      "Epoch [1/60], Step [1400/1961], Loss: 0.830, Training Accuracy: 77.55%\n",
      "Epoch [1/60], Step [1500/1961], Loss: 0.848, Training Accuracy: 69.39%\n",
      "Epoch [1/60], Step [1600/1961], Loss: 0.667, Training Accuracy: 80.61%\n",
      "Epoch [1/60], Step [1700/1961], Loss: 0.759, Training Accuracy: 75.51%\n",
      "Epoch [1/60], Step [1800/1961], Loss: 0.593, Training Accuracy: 77.55%\n",
      "Epoch [1/60], Step [1900/1961], Loss: 0.756, Training Accuracy: 76.53%\n",
      "Testing epoch 1\n",
      "Epoch 1: Test Accuracy 60.08506736760442 %\n",
      "Epoch [2/60], Step [100/1961], Loss: 0.722, Training Accuracy: 73.47%\n",
      "Epoch [2/60], Step [200/1961], Loss: 0.747, Training Accuracy: 69.39%\n",
      "Epoch [2/60], Step [300/1961], Loss: 0.674, Training Accuracy: 71.43%\n",
      "Epoch [2/60], Step [400/1961], Loss: 0.527, Training Accuracy: 79.59%\n",
      "Epoch [2/60], Step [500/1961], Loss: 0.591, Training Accuracy: 79.59%\n",
      "Epoch [2/60], Step [600/1961], Loss: 0.590, Training Accuracy: 75.51%\n",
      "Epoch [2/60], Step [700/1961], Loss: 0.734, Training Accuracy: 65.31%\n",
      "Epoch [2/60], Step [800/1961], Loss: 0.693, Training Accuracy: 77.55%\n",
      "Epoch [2/60], Step [900/1961], Loss: 0.475, Training Accuracy: 82.65%\n",
      "Epoch [2/60], Step [1000/1961], Loss: 0.671, Training Accuracy: 78.57%\n",
      "Epoch [2/60], Step [1100/1961], Loss: 0.734, Training Accuracy: 76.53%\n",
      "Epoch [2/60], Step [1200/1961], Loss: 0.612, Training Accuracy: 81.63%\n",
      "Epoch [2/60], Step [1300/1961], Loss: 0.517, Training Accuracy: 77.55%\n",
      "Epoch [2/60], Step [1400/1961], Loss: 0.489, Training Accuracy: 83.67%\n",
      "Epoch [2/60], Step [1500/1961], Loss: 0.595, Training Accuracy: 79.59%\n",
      "Epoch [2/60], Step [1600/1961], Loss: 0.658, Training Accuracy: 75.51%\n",
      "Epoch [2/60], Step [1700/1961], Loss: 0.438, Training Accuracy: 85.71%\n",
      "Epoch [2/60], Step [1800/1961], Loss: 0.594, Training Accuracy: 82.65%\n",
      "Epoch [2/60], Step [1900/1961], Loss: 0.701, Training Accuracy: 83.67%\n",
      "Testing epoch 2\n",
      "Epoch 2: Test Accuracy 58.887993728451725 %\n",
      "Epoch [3/60], Step [100/1961], Loss: 0.675, Training Accuracy: 76.53%\n",
      "Epoch [3/60], Step [200/1961], Loss: 0.954, Training Accuracy: 72.45%\n",
      "Epoch [3/60], Step [300/1961], Loss: 0.481, Training Accuracy: 79.59%\n",
      "Epoch [3/60], Step [400/1961], Loss: 0.535, Training Accuracy: 81.63%\n",
      "Epoch [3/60], Step [500/1961], Loss: 0.563, Training Accuracy: 78.57%\n",
      "Epoch [3/60], Step [600/1961], Loss: 0.573, Training Accuracy: 77.55%\n",
      "Epoch [3/60], Step [700/1961], Loss: 0.540, Training Accuracy: 85.71%\n",
      "Epoch [3/60], Step [800/1961], Loss: 0.480, Training Accuracy: 79.59%\n",
      "Epoch [3/60], Step [900/1961], Loss: 0.453, Training Accuracy: 80.61%\n",
      "Epoch [3/60], Step [1000/1961], Loss: 0.398, Training Accuracy: 82.65%\n",
      "Epoch [3/60], Step [1100/1961], Loss: 0.673, Training Accuracy: 73.47%\n",
      "Epoch [3/60], Step [1200/1961], Loss: 0.600, Training Accuracy: 82.65%\n",
      "Epoch [3/60], Step [1300/1961], Loss: 0.389, Training Accuracy: 84.69%\n",
      "Epoch [3/60], Step [1400/1961], Loss: 0.666, Training Accuracy: 73.47%\n",
      "Epoch [3/60], Step [1500/1961], Loss: 0.533, Training Accuracy: 81.63%\n",
      "Epoch [3/60], Step [1600/1961], Loss: 0.557, Training Accuracy: 84.69%\n",
      "Epoch [3/60], Step [1700/1961], Loss: 0.398, Training Accuracy: 85.71%\n",
      "Epoch [3/60], Step [1800/1961], Loss: 0.531, Training Accuracy: 78.57%\n",
      "Epoch [3/60], Step [1900/1961], Loss: 0.700, Training Accuracy: 79.59%\n",
      "Testing epoch 3\n",
      "Epoch 3: Test Accuracy 61.80974314320669 %\n",
      "Epoch [4/60], Step [100/1961], Loss: 0.478, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [200/1961], Loss: 0.496, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [300/1961], Loss: 0.518, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [400/1961], Loss: 0.472, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [500/1961], Loss: 0.469, Training Accuracy: 83.67%\n",
      "Epoch [4/60], Step [600/1961], Loss: 0.460, Training Accuracy: 81.63%\n",
      "Epoch [4/60], Step [700/1961], Loss: 0.692, Training Accuracy: 77.55%\n",
      "Epoch [4/60], Step [800/1961], Loss: 0.476, Training Accuracy: 83.67%\n",
      "Epoch [4/60], Step [900/1961], Loss: 0.607, Training Accuracy: 81.63%\n",
      "Epoch [4/60], Step [1000/1961], Loss: 0.417, Training Accuracy: 83.67%\n",
      "Epoch [4/60], Step [1100/1961], Loss: 0.397, Training Accuracy: 83.67%\n",
      "Epoch [4/60], Step [1200/1961], Loss: 0.384, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [1300/1961], Loss: 0.354, Training Accuracy: 86.73%\n",
      "Epoch [4/60], Step [1400/1961], Loss: 0.635, Training Accuracy: 78.57%\n",
      "Epoch [4/60], Step [1500/1961], Loss: 0.542, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [1600/1961], Loss: 0.506, Training Accuracy: 79.59%\n",
      "Epoch [4/60], Step [1700/1961], Loss: 0.545, Training Accuracy: 82.65%\n",
      "Epoch [4/60], Step [1800/1961], Loss: 0.439, Training Accuracy: 83.67%\n",
      "Epoch [4/60], Step [1900/1961], Loss: 0.509, Training Accuracy: 75.51%\n",
      "Testing epoch 4\n",
      "Epoch 4: Test Accuracy 62.60860601819075 %\n",
      "Epoch [5/60], Step [100/1961], Loss: 0.374, Training Accuracy: 84.69%\n",
      "Epoch [5/60], Step [200/1961], Loss: 0.370, Training Accuracy: 87.76%\n",
      "Epoch [5/60], Step [300/1961], Loss: 0.460, Training Accuracy: 79.59%\n",
      "Epoch [5/60], Step [400/1961], Loss: 0.599, Training Accuracy: 73.47%\n",
      "Epoch [5/60], Step [500/1961], Loss: 0.450, Training Accuracy: 80.61%\n",
      "Epoch [5/60], Step [600/1961], Loss: 0.533, Training Accuracy: 85.71%\n",
      "Epoch [5/60], Step [700/1961], Loss: 0.316, Training Accuracy: 85.71%\n",
      "Epoch [5/60], Step [800/1961], Loss: 0.428, Training Accuracy: 82.65%\n",
      "Epoch [5/60], Step [900/1961], Loss: 0.439, Training Accuracy: 83.67%\n",
      "Epoch [5/60], Step [1000/1961], Loss: 0.311, Training Accuracy: 89.80%\n",
      "Epoch [5/60], Step [1100/1961], Loss: 0.528, Training Accuracy: 76.53%\n",
      "Epoch [5/60], Step [1200/1961], Loss: 0.336, Training Accuracy: 88.78%\n",
      "Epoch [5/60], Step [1300/1961], Loss: 0.630, Training Accuracy: 71.43%\n",
      "Epoch [5/60], Step [1400/1961], Loss: 0.371, Training Accuracy: 87.76%\n",
      "Epoch [5/60], Step [1500/1961], Loss: 0.399, Training Accuracy: 84.69%\n",
      "Epoch [5/60], Step [1600/1961], Loss: 0.479, Training Accuracy: 78.57%\n",
      "Epoch [5/60], Step [1700/1961], Loss: 0.412, Training Accuracy: 83.67%\n",
      "Epoch [5/60], Step [1800/1961], Loss: 0.592, Training Accuracy: 81.63%\n",
      "Epoch [5/60], Step [1900/1961], Loss: 0.536, Training Accuracy: 81.63%\n",
      "Testing epoch 5\n",
      "Epoch 5: Test Accuracy 59.25066933587233 %\n",
      "Epoch [6/60], Step [100/1961], Loss: 0.434, Training Accuracy: 81.63%\n",
      "Epoch [6/60], Step [200/1961], Loss: 0.632, Training Accuracy: 78.57%\n",
      "Epoch [6/60], Step [300/1961], Loss: 0.410, Training Accuracy: 79.59%\n",
      "Epoch [6/60], Step [400/1961], Loss: 0.394, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [500/1961], Loss: 0.435, Training Accuracy: 87.76%\n",
      "Epoch [6/60], Step [600/1961], Loss: 0.547, Training Accuracy: 79.59%\n",
      "Epoch [6/60], Step [700/1961], Loss: 0.481, Training Accuracy: 78.57%\n",
      "Epoch [6/60], Step [800/1961], Loss: 0.307, Training Accuracy: 87.76%\n",
      "Epoch [6/60], Step [900/1961], Loss: 0.470, Training Accuracy: 85.71%\n",
      "Epoch [6/60], Step [1000/1961], Loss: 0.432, Training Accuracy: 82.65%\n",
      "Epoch [6/60], Step [1100/1961], Loss: 0.587, Training Accuracy: 70.41%\n",
      "Epoch [6/60], Step [1200/1961], Loss: 0.498, Training Accuracy: 83.67%\n",
      "Epoch [6/60], Step [1300/1961], Loss: 0.397, Training Accuracy: 82.65%\n",
      "Epoch [6/60], Step [1400/1961], Loss: 0.332, Training Accuracy: 89.80%\n",
      "Epoch [6/60], Step [1500/1961], Loss: 0.343, Training Accuracy: 86.73%\n",
      "Epoch [6/60], Step [1600/1961], Loss: 0.510, Training Accuracy: 73.47%\n",
      "Epoch [6/60], Step [1700/1961], Loss: 0.676, Training Accuracy: 80.61%\n",
      "Epoch [6/60], Step [1800/1961], Loss: 0.434, Training Accuracy: 83.67%\n",
      "Epoch [6/60], Step [1900/1961], Loss: 0.450, Training Accuracy: 80.61%\n",
      "Testing epoch 6\n",
      "Epoch 6: Test Accuracy 60.54458305868183 %\n",
      "Epoch [7/60], Step [100/1961], Loss: 0.278, Training Accuracy: 84.69%\n",
      "Epoch [7/60], Step [200/1961], Loss: 0.570, Training Accuracy: 81.63%\n",
      "Epoch [7/60], Step [300/1961], Loss: 0.465, Training Accuracy: 83.67%\n",
      "Epoch [7/60], Step [400/1961], Loss: 0.393, Training Accuracy: 77.55%\n",
      "Epoch [7/60], Step [500/1961], Loss: 0.330, Training Accuracy: 85.71%\n",
      "Epoch [7/60], Step [600/1961], Loss: 0.453, Training Accuracy: 80.61%\n",
      "Epoch [7/60], Step [700/1961], Loss: 0.487, Training Accuracy: 83.67%\n",
      "Epoch [7/60], Step [800/1961], Loss: 0.541, Training Accuracy: 79.59%\n",
      "Epoch [7/60], Step [900/1961], Loss: 0.445, Training Accuracy: 88.78%\n",
      "Epoch [7/60], Step [1000/1961], Loss: 0.387, Training Accuracy: 83.67%\n",
      "Epoch [7/60], Step [1100/1961], Loss: 0.404, Training Accuracy: 86.73%\n",
      "Epoch [7/60], Step [1200/1961], Loss: 0.503, Training Accuracy: 74.49%\n",
      "Epoch [7/60], Step [1300/1961], Loss: 0.235, Training Accuracy: 90.82%\n",
      "Epoch [7/60], Step [1400/1961], Loss: 0.376, Training Accuracy: 86.73%\n",
      "Epoch [7/60], Step [1500/1961], Loss: 0.376, Training Accuracy: 86.73%\n",
      "Epoch [7/60], Step [1600/1961], Loss: 0.515, Training Accuracy: 81.63%\n",
      "Epoch [7/60], Step [1700/1961], Loss: 0.531, Training Accuracy: 83.67%\n",
      "Epoch [7/60], Step [1800/1961], Loss: 0.304, Training Accuracy: 88.78%\n",
      "Epoch [7/60], Step [1900/1961], Loss: 0.508, Training Accuracy: 87.76%\n",
      "Testing epoch 7\n",
      "Epoch 7: Test Accuracy 61.215139604339086 %\n",
      "Epoch [8/60], Step [100/1961], Loss: 0.610, Training Accuracy: 84.69%\n",
      "Epoch [8/60], Step [200/1961], Loss: 0.548, Training Accuracy: 81.63%\n",
      "Epoch [8/60], Step [300/1961], Loss: 0.427, Training Accuracy: 85.71%\n",
      "Epoch [8/60], Step [400/1961], Loss: 0.306, Training Accuracy: 87.76%\n",
      "Epoch [8/60], Step [500/1961], Loss: 0.422, Training Accuracy: 83.67%\n",
      "Epoch [8/60], Step [600/1961], Loss: 0.478, Training Accuracy: 82.65%\n",
      "Epoch [8/60], Step [700/1961], Loss: 0.329, Training Accuracy: 87.76%\n",
      "Epoch [8/60], Step [800/1961], Loss: 0.457, Training Accuracy: 79.59%\n",
      "Epoch [8/60], Step [900/1961], Loss: 0.438, Training Accuracy: 82.65%\n",
      "Epoch [8/60], Step [1000/1961], Loss: 0.197, Training Accuracy: 89.80%\n",
      "Epoch [8/60], Step [1100/1961], Loss: 0.465, Training Accuracy: 82.65%\n",
      "Epoch [8/60], Step [1200/1961], Loss: 0.506, Training Accuracy: 80.61%\n",
      "Epoch [8/60], Step [1300/1961], Loss: 0.281, Training Accuracy: 89.80%\n",
      "Epoch [8/60], Step [1400/1961], Loss: 0.495, Training Accuracy: 86.73%\n",
      "Epoch [8/60], Step [1500/1961], Loss: 0.351, Training Accuracy: 83.67%\n",
      "Epoch [8/60], Step [1600/1961], Loss: 0.321, Training Accuracy: 85.71%\n",
      "Epoch [8/60], Step [1700/1961], Loss: 0.442, Training Accuracy: 85.71%\n",
      "Epoch [8/60], Step [1800/1961], Loss: 0.296, Training Accuracy: 86.73%\n",
      "Epoch [8/60], Step [1900/1961], Loss: 0.481, Training Accuracy: 84.69%\n",
      "Testing epoch 8\n",
      "Epoch 8: Test Accuracy 59.43051520552069 %\n",
      "Epoch [9/60], Step [100/1961], Loss: 0.348, Training Accuracy: 86.73%\n",
      "Epoch [9/60], Step [200/1961], Loss: 0.387, Training Accuracy: 85.71%\n",
      "Epoch [9/60], Step [300/1961], Loss: 0.379, Training Accuracy: 83.67%\n",
      "Epoch [9/60], Step [400/1961], Loss: 0.237, Training Accuracy: 85.71%\n",
      "Epoch [9/60], Step [500/1961], Loss: 0.393, Training Accuracy: 83.67%\n",
      "Epoch [9/60], Step [600/1961], Loss: 0.264, Training Accuracy: 89.80%\n",
      "Epoch [9/60], Step [700/1961], Loss: 0.358, Training Accuracy: 87.76%\n",
      "Epoch [9/60], Step [800/1961], Loss: 0.452, Training Accuracy: 80.61%\n",
      "Epoch [9/60], Step [900/1961], Loss: 0.407, Training Accuracy: 78.57%\n",
      "Epoch [9/60], Step [1000/1961], Loss: 0.326, Training Accuracy: 86.73%\n",
      "Epoch [9/60], Step [1100/1961], Loss: 0.429, Training Accuracy: 83.67%\n",
      "Epoch [9/60], Step [1200/1961], Loss: 0.356, Training Accuracy: 87.76%\n",
      "Epoch [9/60], Step [1300/1961], Loss: 0.308, Training Accuracy: 86.73%\n",
      "Epoch [9/60], Step [1400/1961], Loss: 0.396, Training Accuracy: 84.69%\n",
      "Epoch [9/60], Step [1500/1961], Loss: 0.324, Training Accuracy: 90.82%\n",
      "Epoch [9/60], Step [1600/1961], Loss: 0.386, Training Accuracy: 83.67%\n",
      "Epoch [9/60], Step [1700/1961], Loss: 0.332, Training Accuracy: 85.71%\n",
      "Epoch [9/60], Step [1800/1961], Loss: 0.359, Training Accuracy: 86.73%\n",
      "Epoch [9/60], Step [1900/1961], Loss: 0.415, Training Accuracy: 87.76%\n",
      "Testing epoch 9\n",
      "Epoch 9: Test Accuracy 60.598292684911655 %\n",
      "Finished early after 9 epochs!\n",
      "Final Training Accuracy 85.22610188895248 %\n",
      "Final Test Accuracy 60.70489815515572 %\n",
      "train accuracy: 0.8522610188895249, train logloss: 0.3753821281112512\n",
      "\n",
      "By sample accuracy: 0.9180327868852459, log loss 0.26033898072798334\n",
      "Test Centre: GRH\n",
      "By spectra accuracy: 0.6267981318111053, logloss: 1.668235864231046\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.6721311475409836 logloss 0.9976541474441244\n",
      "\n",
      "Test Centre: UCL\n",
      "By spectra accuracy: 0.5838680526743545, logloss: 1.2386568791983006\n",
      "\n",
      "By SAMPLE SIMPLE CONSENSUS test accuracy: 0.819672131147541 logloss 0.777404847568519\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on one centre, test on two\n",
    "# No hyperparameter search\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "\n",
    "cnn_test_logloss = []\n",
    "cnn_test_accuracy = []\n",
    "cnn_test_logloss = []\n",
    "cnn_test_confusion_matrix = []\n",
    "labels = []\n",
    "predictions = []\n",
    "\n",
    "\n",
    "learning_rate = 0.0002\n",
    "batch_size = 98\n",
    "n_epochs = 60\n",
    "\n",
    "gkf = LeaveOneGroupOut()\n",
    "\n",
    "for test_index, train_index in gkf.split(SMART_x, SMART_y, groups=SMART_centre): #(note switching of test/train indices here as we are leaving out 2 centres)\n",
    "\n",
    "    X_train, X_test = SMART_x[train_index], SMART_x[test_index]\n",
    "    y_train, y_test = SMART_y[train_index], SMART_y[test_index]\n",
    "    centre_train = np.unique(SMART_centre[train_index])\n",
    "    centre_test = np.unique(SMART_centre[test_index])\n",
    "    group_train = SMART_centre[train_index]\n",
    "    group_test = SMART_centre[test_index]\n",
    "    centre_test_1 = np.unique(SMART_centre[test_index])[0]\n",
    "    centre_test_2 = np.unique(SMART_centre[test_index])[1]\n",
    "\n",
    "    print(\"Training Centre: {}\".format(list(Centre.keys())[list(Centre.values()).index(centre_train)]))\n",
    "\n",
    "\n",
    "    # Train CNN on the training and test data\n",
    "    convnet, con_mat, test_ll, test_acc, train_ll, train_acc, test_outputs, test_predicted  = train_cnn(ConvNet, X_train, y_train, X_test, y_test, learning_rate = learning_rate, num_epochs = n_epochs, batch_size = batch_size, early_stop = 5, weightedLoss = True, learning_curves = False)\n",
    "    print(\"train accuracy: {}, train logloss: {}\".format(train_acc,train_ll))\n",
    "    print()\n",
    "\n",
    "    y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_batch(convnet, SMART_centre, SMART_uniquemapID, centre_train, SMART_x, SMART_y, proportioned = False )\n",
    "    acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "    ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y)  )\n",
    "    print(\"By sample accuracy: {}, log loss {}\".format(acc, ll))\n",
    "\n",
    "\n",
    "\n",
    "    # Test individual centres\n",
    "\n",
    "    print(\"Test Centre: {}\".format(list(Centre.keys())[list(Centre.values()).index(centre_test_1)]))\n",
    "    ll, acc, _  = test_a_batch(convnet, SMART_x[SMART_centre == centre_test_1], SMART_y[SMART_centre == centre_test_1], batch_size = batch_size)\n",
    "\n",
    "    print(\"By spectra accuracy: {}, logloss: {}\".format(acc,ll))\n",
    "    print()\n",
    "\n",
    "    y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_batch(convnet, SMART_centre, SMART_uniquemapID, centre_test_1, SMART_x, SMART_y, proportioned = False )\n",
    "    acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "    ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y) )\n",
    "    con_mat = confusion_matrix(y_sample_label, y_sample_clas)\n",
    "    print(\"By SAMPLE SIMPLE CONSENSUS test accuracy: {} logloss {}\".format(acc, ll))\n",
    "    print()\n",
    "\n",
    "    cnn_test_confusion_matrix.append(con_mat)\n",
    "    cnn_test_accuracy.append(acc)\n",
    "    cnn_test_logloss.append(ll)\n",
    "\n",
    "    # These are for the ROC curves\n",
    "    labels.append(y_sample_label)\n",
    "    predictions.append(y_sample_preds)\n",
    "\n",
    "    print(\"Test Centre: {}\".format(list(Centre.keys())[list(Centre.values()).index(centre_test_2)]))\n",
    "\n",
    "    ll, acc, _  = test_a_batch(convnet, SMART_x[SMART_centre == centre_test_2], SMART_y[SMART_centre == centre_test_2], batch_size = batch_size)\n",
    "\n",
    "    print(\"By spectra accuracy: {}, logloss: {}\".format(acc,ll))\n",
    "    print()\n",
    "\n",
    "\n",
    "    y_sample_label, y_sample_clas, y_sample_preds = sample_accuracy_batch(convnet, SMART_centre, SMART_uniquemapID, centre_test_2, SMART_x, SMART_y, proportioned = False )\n",
    "    acc = metrics.accuracy_score(y_sample_label, y_sample_clas )\n",
    "    ll = metrics.log_loss(y_sample_label, y_sample_preds, labels = np.unique(SMART_y) )\n",
    "    con_mat = confusion_matrix(y_sample_label, y_sample_clas)\n",
    "    print(\"By SAMPLE SIMPLE CONSENSUS test accuracy: {} logloss {}\".format(acc, ll))\n",
    "    print()\n",
    "\n",
    "    cnn_test_confusion_matrix.append(con_mat)\n",
    "    cnn_test_accuracy.append(acc)\n",
    "    cnn_test_logloss.append(ll)\n",
    "\n",
    "    # These are for the ROC curves\n",
    "    labels.append(y_sample_label)\n",
    "    predictions.append(y_sample_preds)\n",
    "\n",
    "\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wREHwGI9jaZv"
   },
   "outputs": [],
   "source": [
    "def one_hot_vector(test_y):\n",
    "    \"\"\"Function to turn a vector of five labels into one hot vector form\n",
    "    Input:\n",
    "        An nx1 vector of labels\n",
    "\n",
    "    Output:\n",
    "        An nx5 vector of labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    raw_y_one_hot_clas = np.zeros([test_y.shape[0], 5])\n",
    "\n",
    "    for i in range(0, len(test_y)):\n",
    "        if test_y[i] == 0:\n",
    "            raw_y_one_hot_clas[i,0] = 1\n",
    "        elif test_y[i] == 1:\n",
    "            raw_y_one_hot_clas[i,1] = 1\n",
    "        elif test_y[i] == 2:\n",
    "            raw_y_one_hot_clas[i,2] = 1\n",
    "        elif test_y[i] == 3:\n",
    "            raw_y_one_hot_clas[i,3] = 1\n",
    "        elif test_y[i] == 4:\n",
    "            raw_y_one_hot_clas[i,4] = 1\n",
    "\n",
    "    return raw_y_one_hot_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 522,
     "status": "ok",
     "timestamp": 1661526610916,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "Gd-rjDjifooP",
    "outputId": "80c4a33b-29a3-458e-a65c-ce7c94ec9639"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "which_class = 0\n",
    "n_classes = 5\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "#plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[which_class], tpr[which_class], color='lime',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2],alpha=.1)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[which_class], tpr[which_class])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[which_class])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='darkgreen',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=1)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='palegreen', alpha=.3,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic Curve\")\n",
    "#ax.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1661526659080,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "-vYp86lStK5O",
    "outputId": "7dda4e78-b25a-4806-e789-cd39e1c06459"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "which_class = 1\n",
    "n_classes = 5\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "#plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[which_class], tpr[which_class], color='blue',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1],alpha=.1)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[which_class], tpr[which_class])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[which_class])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='darkblue',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='cornflowerblue', alpha=.3,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic Curve\")\n",
    "#ax.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1661526674669,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "-7AAdzJ8tLHG",
    "outputId": "6b2cd27d-eb26-4701-8538-fc2f041d0fc6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "which_class = 2\n",
    "n_classes = 5\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "#plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[which_class], tpr[which_class], color='yellow',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2],alpha=.1)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[which_class], tpr[which_class])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[which_class])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='gold',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='khaki', alpha=.3,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic Curve\")\n",
    "#ax.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1661526687549,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "ehiVW5DStLWO",
    "outputId": "a37fe540-8ef3-4498-d350-feeabc395b1e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "which_class = 3\n",
    "n_classes = 5\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "#plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[which_class], tpr[which_class], color='orange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2],alpha=.1)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[which_class], tpr[which_class])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[which_class])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='darkorange',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='wheat', alpha=.3,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic Curve\")\n",
    "#ax.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 428,
     "status": "ok",
     "timestamp": 1661526704151,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "QCffTY-4tLjZ",
    "outputId": "b5c91bbb-728f-4799-dbaf-11c38f649c8a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "which_class = 4\n",
    "n_classes = 5\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "#plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[which_class], tpr[which_class], color='lightcoral',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2],alpha=.1)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[which_class], tpr[which_class])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[which_class])\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='red',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='lightsalmon', alpha=.3,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic Curve\")\n",
    "#ax.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2l8JfRYtL3c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1661593778953,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "7Hyg5a0zfo0B",
    "outputId": "b679d551-742a-46ac-ec0a-8f73ce8e84fe"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "n_classes = 5\n",
    "lw = 2\n",
    "tprs = []\n",
    "aucs = []\n",
    "cls = 0\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "#plt.figure()\n",
    "#figsize=(10,5)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[cls], tpr[cls], color='green',lw=lw,alpha=.05)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[cls], tpr[cls])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[cls])\n",
    "\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='darkgreen',\n",
    "        label=r'NSQ (AUC = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "cls = 1\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[cls], tpr[cls], color='blue',lw=lw,alpha=.05)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[cls], tpr[cls])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[cls])\n",
    "\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='darkblue',\n",
    "        label=r'IM (AUC = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "cls = 2\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[cls], tpr[cls], color='gold',lw=lw,alpha=.05)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[cls], tpr[cls])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[cls])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='yellow',\n",
    "        label=r'LGD (AUC = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "cls = 3\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[cls], tpr[cls], color='orange',lw=lw,alpha=.05)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[cls], tpr[cls])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[cls])\n",
    "\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='darkorange',\n",
    "        label=r'HGD (AUC = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "cls = 4\n",
    "for i in range(0, len(labels)):\n",
    "\n",
    "    labels_one = one_hot_vector(labels[i])\n",
    "    #predictions_one = predictions[i]\n",
    "    predictions_one = np.array([np.array(x) for x in predictions[i]])\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_one[:, i], predictions_one[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(labels_one.ravel(), predictions_one.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    plt.plot(fpr[cls], tpr[cls], color='lightsalmon',lw=lw,alpha=.05)\n",
    "\n",
    "    interp_tpr = np.interp(mean_fpr, fpr[cls], tpr[cls])\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(roc_auc[cls])\n",
    "\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='red',\n",
    "        label=r'AC (AUC = %0.3f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "ax.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I472qp5qOs1Y"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    else:\n",
    "      thresh = cm.min()+(cm.max() - cm.min()) / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1661593778218,
     "user": {
      "displayName": "Nathan Blake",
      "userId": "02361771191498418780"
     },
     "user_tz": -60
    },
    "id": "SwfKEdT1syZc",
    "outputId": "27bc06bb-c6ec-439c-8914-12711b88f8ae"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plot_confusion_matrix(np.mean(cnn_test_confusion_matrix,axis=0),Raw_labels.keys(),normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRbzgggYaQ-Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOumksGpocletjdu/HtNG5m",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
